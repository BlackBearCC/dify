# -*- coding: utf-8 -*-
"""
LLMå®¢æˆ·ç«¯ - æ”¯æŒClaudeå’Œè±†åŒ…æ¨¡å‹çš„ç»Ÿä¸€æ¥å£
æ”¯æŒæ¨¡å‹ï¼š
- Claudeç³»åˆ— (Anthropic)
- è±†åŒ…ç³»åˆ— (å­—èŠ‚è·³åŠ¨)
- Kimiç³»åˆ— (æœˆä¹‹æš—é¢)
- DeepSeekç³»åˆ— (DeepSeek)
"""

import requests
import json
import time
import sys
import re
from typing import Optional, Dict, Any, List
from enum import Enum

class LLMProvider(Enum):
    """LLMæä¾›å•†æšä¸¾"""
    CLAUDE = "claude"
    DOUBAO = "doubao"
    DEEPSEEK = "deepseek"

class LLMClient:
    """ç»Ÿä¸€çš„LLMå®¢æˆ·ç«¯ï¼Œæ”¯æŒå¤šä¸ªæ¨¡å‹æä¾›å•†"""

    @staticmethod
    def _remove_think_tags(text: str) -> str:
        """ç§»é™¤æ–‡æœ¬ä¸­çš„<think>å’Œ</think>æ ‡ç­¾åŠå…¶å†…å®¹"""
        # ç§»é™¤<think>...</think>æ ‡ç­¾åŠå…¶å†…å®¹
        pattern = r'<think>.*?</think>'
        cleaned_text = re.sub(pattern, '', text, flags=re.DOTALL)
        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
        cleaned_text = re.sub(r'\n\s*\n', '\n\n', cleaned_text)
        return cleaned_text.strip()

    # è±†åŒ…å¹³å°æ¨¡å‹æ˜ å°„
    DOUBAO_MODELS = {
        # è±†åŒ…æ¨¡å‹
        "doubao-1.5-pro-32k": "ep-20250312153153-npj4s",
        "doubao-1.5-lite-32k": "ep-20250312153312-hwtd2", 
        "doubao-1.5-pro-256k": "ep-20250312153332-jfhkj",
        "doubao-1.5-pro-character": "ep-20250312153655-ntg8z",
        "doubao-1.5-thinking-pro": "ep-20250417214536-hpndh",
        "doubao-1.6": "ep-20250612123019-mb9bb",
        "doubao-1.6-thinking": "ep-20250612123438-7fj94",
        "doubao-1.6-flash": "ep-20250612122042-t6g56",
        "doubao-embedding": "ep-20250312154514-xrm58",

        # è±†åŒ…è§†è§‰æ¨¡å‹æ˜ å°„
        "doubao-vision-pro": "ep-20250704095927-j6t2g",
        

        
        # DeepSeekæ¨¡å‹
        "deepseek-V3": "ep-20250221154410-vh78x",
        "deepseek-R1": "ep-20250221154107-c4qc7"
    }
    
    def __init__(self, 
                 provider: LLMProvider = LLMProvider.CLAUDE,
                 model: str = "claude-sonnet-4-20250514",
                 api_key: str = None,
                 base_url: str = None):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        
        Args:
            provider: LLMæä¾›å•† (CLAUDE, DOUBAO, KIMI, DEEPSEEK)
            model: æ¨¡å‹åç§°
            api_key: APIå¯†é’¥
            base_url: åŸºç¡€URL
        """
        self.provider = provider
        self.model = model
        self.api_key = api_key
        self.base_url = base_url
        
        # è®¾ç½®é»˜è®¤é…ç½®
        self._setup_default_config()
        
        print(f"ğŸ¤– LLMå®¢æˆ·ç«¯åˆå§‹åŒ–: {self.provider.value} - {self.model}")
        
    def _setup_default_config(self):
        """è®¾ç½®é»˜è®¤é…ç½®"""
        if self.provider == LLMProvider.CLAUDE:
            if not self.base_url:
                self.base_url = "https://club.claudecode.site"
            self.endpoint = f"{self.base_url}/v1/messages"
            
        elif self.provider in [LLMProvider.DOUBAO, LLMProvider.DEEPSEEK]:
            if not self.base_url:
                self.base_url = "https://ark.cn-beijing.volces.com/api/v3"
            self.endpoint = f"{self.base_url}/chat/completions"
            
            # å°†å‹å¥½åç§°è½¬æ¢ä¸ºendpoint ID
            if self.model in self.DOUBAO_MODELS:
                self.endpoint_id = self.DOUBAO_MODELS[self.model]
                print(f"ğŸ“‹ æ¨¡å‹æ˜ å°„: {self.model} -> {self.endpoint_id}")
            else:
                self.endpoint_id = self.model  # å¦‚æœç›´æ¥æä¾›endpoint ID
    
    def call(self, system_prompt_or_full_prompt: str,
             user_message: Optional[str] = None,
             agent_name: str = "Assistant",
             max_tokens: int = 2000,
             temperature: float = 0.7,
             stream: bool = True,
             enable_thinking: bool = False) -> str:
        """
        è°ƒç”¨LLM APIçš„ç»Ÿä¸€æ¥å£ - æ”¯æŒåˆ†ç¦»çš„ç³»ç»Ÿæç¤ºå’Œç”¨æˆ·æ¶ˆæ¯

        Args:
            system_prompt_or_full_prompt: ç³»ç»Ÿæç¤ºè¯ï¼Œå¦‚æœuser_messageä¸ºNoneï¼Œåˆ™ä½œä¸ºå®Œæ•´æç¤º
            user_message: ç”¨æˆ·æ¶ˆæ¯ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœæä¾›åˆ™ä¸system_promptåˆ†ç¦»
            agent_name: ä»£ç†åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            stream: æ˜¯å¦æµå¼è¾“å‡º
            enable_thinking: æ˜¯å¦å¯ç”¨thinkingæ¨¡å¼ï¼ˆä»…é€‚ç”¨äºdoubaoæ¨¡å‹ï¼‰

        Returns:
            str: LLMå“åº”å†…å®¹
        """
        if not self.api_key:
            error_msg = f"âŒ [{agent_name}] æœªé…ç½®{self.provider.value} APIå¯†é’¥"
            print(error_msg, flush=True)
            return error_msg
        
        print(f"ğŸ¤– [{agent_name}] è°ƒç”¨{self.provider.value}æ¨¡å‹: {self.model}", flush=True)
        
        try:
            # è®¡ç®—æç¤ºé•¿åº¦ç”¨äºæ—¥å¿—
            if user_message is not None:
                total_length = len(system_prompt_or_full_prompt) + len(user_message)
                print(f"ğŸ“ æç¤ºé•¿åº¦: {total_length} å­—ç¬¦ (ç³»ç»Ÿæç¤º: {len(system_prompt_or_full_prompt)}, ç”¨æˆ·æ¶ˆæ¯: {len(user_message)})", flush=True)
            else:
                print(f"ğŸ“ æç¤ºé•¿åº¦: {len(system_prompt_or_full_prompt)} å­—ç¬¦", flush=True)
            
            if self.provider == LLMProvider.CLAUDE:
                response = self._call_claude_api(system_prompt_or_full_prompt, user_message, agent_name, max_tokens, stream)
            else:
                response = self._call_doubao_api(system_prompt_or_full_prompt, user_message, agent_name, max_tokens, temperature, stream, enable_thinking)
            
            print(f"âœ… [{agent_name}] å“åº”å®Œæˆï¼Œå…±{len(response)}å­—ç¬¦", flush=True)

            # å¦‚æœæ˜¯doubaoæ¨¡å‹ä¸”ç¦ç”¨äº†thinkingæ¨¡å¼ï¼Œæ¸…ç†å¯èƒ½æ®‹ç•™çš„thinkæ ‡ç­¾
            if (self.provider == LLMProvider.DOUBAO and
                hasattr(self, '_thinking_disabled') and self._thinking_disabled):
                response = self._remove_think_tags(response)

            return response
                
        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            error_msg = f"âŒ [{agent_name}] {self.provider.value}è°ƒç”¨å¤±è´¥: {e}\nè¯¦ç»†é”™è¯¯:\n{error_detail}"
            print(error_msg, flush=True)
            return error_msg
    
    def _call_claude_api(self, system_prompt_or_full_prompt: str, user_message: Optional[str], 
                         agent_name: str, max_tokens: int, stream: bool) -> str:
        """è°ƒç”¨Claude API - æ”¯æŒåˆ†ç¦»çš„ç³»ç»Ÿæç¤ºå’Œç”¨æˆ·æ¶ˆæ¯"""
        if user_message is not None:
            # åˆ†ç¦»æ¨¡å¼ï¼šç³»ç»Ÿæç¤º + ç”¨æˆ·æ¶ˆæ¯
            payload = {
                "model": self.model,
                "system": system_prompt_or_full_prompt,
                "messages": [{"role": "user", "content": user_message}],
                "max_tokens": max_tokens,
                "stream": stream
            }
        else:
            # å…¼å®¹æ¨¡å¼ï¼šå•ä¸ªå®Œæ•´æç¤º
            payload = {
                "model": self.model,
                "messages": [{"role": "user", "content": system_prompt_or_full_prompt}],
                "max_tokens": max_tokens,
                "stream": stream
            }
        
        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json"
        }
        
        return self._make_request(payload, headers, agent_name, stream)
    
    def _call_doubao_api(self, system_prompt_or_full_prompt: str, user_message: Optional[str],
                         agent_name: str, max_tokens: int, temperature: float, stream: bool, enable_thinking: bool = False) -> str:
        """è°ƒç”¨è±†åŒ…/Kimi/DeepSeek API - æ”¯æŒåˆ†ç¦»çš„ç³»ç»Ÿæç¤ºå’Œç”¨æˆ·æ¶ˆæ¯"""
        if user_message is not None:
            # åˆ†ç¦»æ¨¡å¼ï¼šç³»ç»Ÿæç¤º + ç”¨æˆ·æ¶ˆæ¯
            messages = [
                {"role": "system", "content": system_prompt_or_full_prompt},
                {"role": "user", "content": user_message}
            ]
        else:
            # å…¼å®¹æ¨¡å¼ï¼šå•ä¸ªå®Œæ•´æç¤º
            messages = [{"role": "user", "content": system_prompt_or_full_prompt}]

        payload = {
            "model": self.endpoint_id,  # ä½¿ç”¨endpoint ID
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stream": stream
        }

        # æ·»åŠ thinkingæ¨¡å¼æ§åˆ¶ï¼ˆä»…å¯¹doubaoæ¨¡å‹æœ‰æ•ˆï¼‰
        if self.provider == LLMProvider.DOUBAO:
            thinking_type = "enabled" if enable_thinking else "disabled"
            payload["extra"] = {
                "thinking": {"type": thinking_type}
            }
            # è®¾ç½®æ ‡å¿—ï¼Œç”¨äºåç»­æ¸…ç†thinkæ ‡ç­¾
            self._thinking_disabled = not enable_thinking
            if not enable_thinking:
                print(f"ğŸ§  [{agent_name}] Thinkingæ¨¡å¼å·²ç¦ç”¨ï¼Œå°†å»é™¤<think>æ ‡ç­¾", flush=True)

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        return self._make_request(payload, headers, agent_name, stream)
    
    def call_with_image(self, system_prompt: str, user_message: str,
                       image_base64: str, image_mime: str = "image/jpeg",
                       agent_name: str = "Assistant",
                       max_tokens: int = 4096,
                       temperature: float = 0.7) -> str:
        """
        è°ƒç”¨æ”¯æŒå›¾ç‰‡çš„LLM API

        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_message: ç”¨æˆ·æ¶ˆæ¯
            image_base64: Base64ç¼–ç çš„å›¾ç‰‡æ•°æ®
            image_mime: å›¾ç‰‡MIMEç±»å‹
            agent_name: ä»£ç†åç§°
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°

        Returns:
            str: LLMå“åº”å†…å®¹
        """
        if not self.api_key:
            error_msg = f"âŒ [{agent_name}] æœªé…ç½®{self.provider.value} APIå¯†é’¥"
            print(error_msg, flush=True)
            return error_msg

        if self.provider != LLMProvider.DOUBAO:
            error_msg = f"âŒ [{agent_name}] å½“å‰ä»…æ”¯æŒè±†åŒ…æ¨¡å‹çš„å›¾ç‰‡è¯†åˆ«åŠŸèƒ½"
            print(error_msg, flush=True)
            return error_msg

        print(f"ğŸ–¼ï¸ [{agent_name}] è°ƒç”¨è±†åŒ…è§†è§‰æ¨¡å‹: {self.model}", flush=True)

        try:
            return self._call_doubao_vision_api(system_prompt, user_message, image_base64,
                                              image_mime, agent_name, max_tokens, temperature)
        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            error_msg = f"âŒ [{agent_name}] è±†åŒ…è§†è§‰æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}\nè¯¦ç»†é”™è¯¯:\n{error_detail}"
            print(error_msg, flush=True)
            return error_msg

    def _call_doubao_vision_api(self, system_prompt: str, user_message: str,
                               image_base64: str, image_mime: str,
                               agent_name: str, max_tokens: int, temperature: float) -> str:
        """è°ƒç”¨è±†åŒ…è§†è§‰API"""
        messages = [
            {"role": "system", "content": system_prompt},
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": user_message},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:{image_mime};base64,{image_base64}"
                        }
                    }
                ]
            }
        ]

        payload = {
            "model": self.endpoint_id,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stream": False  # å›¾ç‰‡è¯†åˆ«æš‚ä¸æ”¯æŒæµå¼
        }

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        return self._make_request(payload, headers, agent_name, False)

    def _make_request(self, payload: dict, headers: dict, agent_name: str, stream: bool) -> str:
        """å‘é€HTTPè¯·æ±‚"""
        try:
            response = requests.post(self.endpoint, json=payload, headers=headers, timeout=60, stream=stream)

            if response.status_code != 200:
                error_msg = f"âŒ [{agent_name}] APIè¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}"
                print(error_msg, flush=True)
                return error_msg

            if stream:
                return self._handle_stream_response(response, agent_name)
            else:
                return self._handle_normal_response(response, agent_name)

        except requests.exceptions.Timeout:
            error_msg = f"âŒ [{agent_name}] è¯·æ±‚è¶…æ—¶"
            print(error_msg, flush=True)
            return error_msg
        except requests.exceptions.RequestException as e:
            error_msg = f"âŒ [{agent_name}] ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}"
            print(error_msg, flush=True)
            return error_msg
    
    def _handle_stream_response(self, response, agent_name: str) -> str:
        """å¤„ç†æµå¼å“åº”"""
        full_response = ""
        buffer = ""
        
        try:
            for chunk in response:
                if chunk:
                    buffer += chunk.decode('utf-8', errors='ignore')
                    
                    # å¤„ç†å®Œæ•´çš„è¡Œ
                    while '\n' in buffer:
                        line, buffer = buffer.split('\n', 1)
                        line = line.strip()
                        
                        if line.startswith('data: '):
                            data_text = line[6:]
                            if data_text.strip() == '[DONE]':
                                break
                            
                            try:
                                data = json.loads(data_text)
                                
                                # Claudeæ ¼å¼
                                if self.provider == LLMProvider.CLAUDE:
                                    if data.get('type') == 'content_block_delta':
                                        if 'delta' in data and data['delta'].get('type') == 'text_delta':
                                            chunk_text = data['delta']['text']
                                            print(chunk_text, end='', flush=True)
                                            sys.stdout.flush()
                                            full_response += chunk_text
                                
                                # è±†åŒ…/Kimi/DeepSeekæ ¼å¼
                                else:
                                    if 'choices' in data and len(data['choices']) > 0:
                                        choice = data['choices'][0]
                                        if 'delta' in choice and 'content' in choice['delta']:
                                            chunk_text = choice['delta']['content']
                                            if chunk_text:
                                                print(chunk_text, end='', flush=True)
                                                sys.stdout.flush()
                                                full_response += chunk_text
                                                
                            except json.JSONDecodeError:
                                continue
                            except Exception as e:
                                print(f"âš ï¸ [{agent_name}] å¤„ç†æ•°æ®é”™è¯¯: {e}", flush=True)
                                continue
            
            print()  # æ¢è¡Œ
            
            if not full_response.strip():
                error_msg = f"âŒ [{agent_name}] æœªæ”¶åˆ°æœ‰æ•ˆå“åº”å†…å®¹"
                print(error_msg, flush=True)
                return error_msg
            
            return full_response.strip()
            
        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            error_msg = f"âŒ [{agent_name}] æµå¼å“åº”å¤„ç†é”™è¯¯: {e}\nè¯¦ç»†é”™è¯¯:\n{error_detail}"
            print(error_msg, flush=True)
            return error_msg
    
    def _handle_normal_response(self, response, agent_name: str) -> str:
        """å¤„ç†æ™®é€šå“åº”"""
        try:
            data = response.json()
            
            # Claudeæ ¼å¼
            if self.provider == LLMProvider.CLAUDE:
                if 'content' in data and len(data['content']) > 0:
                    return data['content'][0]['text']
            
            # è±†åŒ…/Kimi/DeepSeekæ ¼å¼
            else:
                if 'choices' in data and len(data['choices']) > 0:
                    return data['choices'][0]['message']['content']
            
            error_msg = f"âŒ [{agent_name}] å“åº”æ ¼å¼å¼‚å¸¸: {data}"
            print(error_msg, flush=True)
            return error_msg
            
        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            error_msg = f"âŒ [{agent_name}] å“åº”è§£æé”™è¯¯: {e}\nè¯¦ç»†é”™è¯¯:\n{error_detail}"
            print(error_msg, flush=True)
            return error_msg

# ä¾¿æ·çš„å·¥å‚å‡½æ•°
def create_claude_client(api_key: str, model: str = "claude-sonnet-4-20250514", base_url: str = None) -> LLMClient:
    """åˆ›å»ºClaudeå®¢æˆ·ç«¯"""
    return LLMClient(
        provider=LLMProvider.CLAUDE,
        model=model,
        api_key=api_key,
        base_url=base_url
    )

def create_doubao_client(api_key: str, model: str = "doubao-1.6", base_url: str = None) -> LLMClient:
    """åˆ›å»ºè±†åŒ…å®¢æˆ·ç«¯"""
    return LLMClient(
        provider=LLMProvider.DOUBAO,
        model=model,
        api_key=api_key,
        base_url=base_url
    )

def create_deepseek_client(api_key: str, model: str = "deepseek-V3", base_url: str = None) -> LLMClient:
    """åˆ›å»ºDeepSeekå®¢æˆ·ç«¯"""
    return LLMClient(
        provider=LLMProvider.DEEPSEEK,
        model=model,
        api_key=api_key,
        base_url=base_url
    )