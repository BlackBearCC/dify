"""æ—¥ç¨‹ç”Ÿæˆå·¥ä½œæµ - åŸºäºGraph+Nodeçš„æ—¥ç¨‹åˆ›ä½œç³»ç»Ÿ
é›†æˆè§’è‰²åº“ã€åœ°ç‚¹åº“ã€å‰§æƒ…åº“ç­‰åŠŸèƒ½ï¼Œä¸ºä¸»è§’ç”Ÿæˆæ¯å‘¨å’Œæ¯å¤©çš„è¯¦ç»†æ—¥ç¨‹å®‰æ’
"""

import json
import asyncio
import csv
import random
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import calendar

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from core.graph import StateGraph
from core.base import BaseNode
from llm.base import LLMFactory
from core.types import LLMConfig, TaskResult, Message, MessageRole

logger = logging.getLogger(__name__)

class ScheduleWorkflow:
    """æ—¥ç¨‹ç”Ÿæˆå·¥ä½œæµç®¡ç†å™¨"""
    
    def __init__(self, llm=None):
        self.llm = llm
        self.graph = None
        self.characters_data = {}
        self.locations_data = {}
        self.stories_data = {}  # å‰§æƒ…åº“æ•°æ®
        self.protagonist_data = ""  # ä¸»è§’ç©†æ˜­çš„è¯¦ç»†äººè®¾
        self.holidays_data = {}  # èŠ‚å‡æ—¥æ•°æ®
        self.current_config = {
            'protagonist': 'ç©†æ˜­',  # å›ºå®šä¸»è§’
            'schedule_type': 'weekly',  # weekly, daily, monthly
            'start_date': '',
            'end_date': '',
            'total_days': 7,
            'selected_characters': [],
            'selected_locations': [],
            'selected_stories': [],  # é€‰æ‹©çš„å‰§æƒ…
            'time_slots_config': {
                'å¤œé—´': {'start': '23:00', 'end': '06:00'},
                'ä¸Šåˆ': {'start': '06:00', 'end': '11:00'},
                'ä¸­åˆ': {'start': '11:00', 'end': '14:00'},
                'ä¸‹åˆ': {'start': '14:00', 'end': '18:00'},
                'æ™šä¸Š': {'start': '18:00', 'end': '23:00'}
            },
            'character_distribution': 'balanced',  # balanced, random, weighted
            'story_integration': 'moderate',  # minimal, moderate, intensive
            'include_holidays': True,
            'include_lunar': True,
            'mood_variety': True,
            'location_variety': True,
            'enable_cycle_summary': False,  # æ˜¯å¦å¯ç”¨å‘¨æœŸæ€»ç»“åŠŸèƒ½ï¼Œé»˜è®¤å…³é—­
            'cycle_summary': ''  # å½“å‰å‘¨æœŸæ€»ç»“å†…å®¹
        }
        
        # é¢„å…ˆåˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„ï¼Œé˜²æ­¢æ‰§è¡Œæ—¶æ‰åˆ›å»ºå¯¼è‡´é”™è¯¯
        try:
            from database.managers import schedule_manager
            schedule_manager.ScheduleManager()  # åˆå§‹åŒ–ä¼šè‡ªåŠ¨åˆ›å»ºè¡¨ç»“æ„
            logger.info("æ•°æ®åº“è¡¨ç»“æ„åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.warning(f"é¢„åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„å¤±è´¥ï¼Œç¨åå°†é‡è¯•: {e}")
        
        # åŠ è½½å„ç§æ•°æ®
        self._load_game_data()
        self._load_protagonist_data()
        self._load_stories_data()
        self._load_holidays_data()
    
    def _load_game_data(self):
        """åŠ è½½æ¸¸æˆè§’è‰²å’Œåœ°ç‚¹æ•°æ®"""
        try:
            # åŠ è½½è§’è‰²æ•°æ®
            char_path = os.path.join(os.path.dirname(__file__), '../../config/yunhub_characters.json')
            if os.path.exists(char_path):
                with open(char_path, 'r', encoding='utf-8') as f:
                    self.characters_data = json.load(f)
                    logger.info(f"æˆåŠŸåŠ è½½è§’è‰²æ•°æ®ï¼ŒåŒ…å« {len(self.characters_data.get('è§’è‰²åˆ—è¡¨', {}))} ä¸ªè§’è‰²")
            
            # åŠ è½½åœ°ç‚¹æ•°æ®
            loc_path = os.path.join(os.path.dirname(__file__), '../../config/yunhub_locations.json')
            if os.path.exists(loc_path):
                with open(loc_path, 'r', encoding='utf-8') as f:
                    self.locations_data = json.load(f)
                    district_count = len(self.locations_data.get("districts", {}))
                    logger.info(f"æˆåŠŸåŠ è½½åœ°ç‚¹æ•°æ®ï¼ŒåŒ…å« {district_count} ä¸ªåŒºåŸŸ")
                    
        except Exception as e:
            logger.error(f"åŠ è½½æ¸¸æˆæ•°æ®å¤±è´¥: {e}")
    
    def _load_protagonist_data(self):
        """åŠ è½½ä¸»è§’ç©†æ˜­çš„è¯¦ç»†äººè®¾"""
        try:
            protagonist_path = os.path.join(os.path.dirname(__file__), '../../config/åŸºç¡€äººè®¾_ç©†æ˜­.txt')
            if os.path.exists(protagonist_path):
                with open(protagonist_path, 'r', encoding='utf-8') as f:
                    self.protagonist_data = f.read()
                    logger.info(f"æˆåŠŸåŠ è½½ä¸»è§’äººè®¾ï¼Œå†…å®¹é•¿åº¦: {len(self.protagonist_data)} å­—ç¬¦")
            else:
                logger.warning("ä¸»è§’äººè®¾æ–‡ä»¶ä¸å­˜åœ¨")
                
        except Exception as e:
            logger.error(f"åŠ è½½ä¸»è§’äººè®¾å¤±è´¥: {e}")
    
    def _load_stories_data(self):
        """åŠ è½½å·²æœ‰å‰§æƒ…æ•°æ®ä½œä¸ºå‚è€ƒ"""
        try:
            from database import story_manager
            
            # è·å–æ‰€æœ‰å‰§æƒ…ä½œä¸ºå‚è€ƒ
            all_stories = story_manager.get_stories_by_filter({}, limit=100)
            
            # æŒ‰è§’è‰²åˆ†ç»„å‰§æƒ…
            self.stories_data = {
                'all_stories': all_stories,
                'by_character': {},
                'by_location': {},
                'by_type': {}
            }
            
            for story in all_stories:
                # æŒ‰è§’è‰²åˆ†ç»„
                characters = json.loads(story.get('selected_characters', '[]'))
                for char in characters:
                    if char not in self.stories_data['by_character']:
                        self.stories_data['by_character'][char] = []
                    self.stories_data['by_character'][char].append(story)
                
                # æŒ‰åœ°ç‚¹åˆ†ç»„
                locations = json.loads(story.get('selected_locations', '[]'))
                for loc in locations:
                    if loc not in self.stories_data['by_location']:
                        self.stories_data['by_location'][loc] = []
                    self.stories_data['by_location'][loc].append(story)
                
                # æŒ‰ç±»å‹åˆ†ç»„
                story_type = story.get('story_type', 'daily_life')
                if story_type not in self.stories_data['by_type']:
                    self.stories_data['by_type'][story_type] = []
                self.stories_data['by_type'][story_type].append(story)
            
            logger.info(f"æˆåŠŸåŠ è½½å‰§æƒ…æ•°æ®ï¼ŒåŒ…å« {len(all_stories)} ä¸ªå‰§æƒ…")
            
        except Exception as e:
            logger.error(f"åŠ è½½å‰§æƒ…æ•°æ®å¤±è´¥: {e}")
            self.stories_data = {'all_stories': [], 'by_character': {}, 'by_location': {}, 'by_type': {}}
    
    def _load_holidays_data(self):
        """åŠ è½½èŠ‚å‡æ—¥æ•°æ®"""
        try:
            # ä»CSVæ–‡ä»¶åŠ è½½èŠ‚å‡æ—¥æ•°æ®
            holidays_csv_path = os.path.join(os.path.dirname(__file__), '../../config/holidays.csv')
            
            if os.path.exists(holidays_csv_path):
                with open(holidays_csv_path, 'r', encoding='utf-8') as f:
                    csv_reader = csv.DictReader(f)
                    for row in csv_reader:
                        date_str = row['date']
                        self.holidays_data[date_str] = {
                            'name': row['name'],
                            'type': row['type'],
                            'lunar': row['lunar'].lower() == 'true',
                            'description': row.get('description', '')
                        }
                
                logger.info(f"ä»CSVæ–‡ä»¶åŠ è½½èŠ‚å‡æ—¥æ•°æ®ï¼ŒåŒ…å« {len(self.holidays_data)} ä¸ªèŠ‚å‡æ—¥")

            
        except Exception as e:
            logger.error(f"åŠ è½½èŠ‚å‡æ—¥æ•°æ®å¤±è´¥: {e}")
            # ä½¿ç”¨ç©ºå­—å…¸ä½œä¸ºæœ€åçš„åå¤‡
            self.holidays_data = {}
    
    def get_protagonist_info(self) -> Dict[str, Any]:
        """è·å–ä¸»è§’ä¿¡æ¯"""
        protagonist_name = self.current_config.get('protagonist', 'ç©†æ˜­')
        return {
            'name': protagonist_name,
            'type': 'protagonist',
            'description': self.protagonist_data.split('\n')[0] if self.protagonist_data else 'ä¸»è§’ä¿¡æ¯',
            'full_profile': self.protagonist_data
        }
    
    def get_characters_list(self) -> List[Dict[str, Any]]:
        """è·å–è§’è‰²åˆ—è¡¨ï¼ˆä¸åŒ…å«ä¸»è§’ï¼‰"""
        characters = []
        char_list = self.characters_data.get("è§’è‰²åˆ—è¡¨", {})
        
        for name, info in char_list.items():
            # è·³è¿‡ä¸»è§’ï¼Œä¸»è§’å•ç‹¬å¤„ç†
            if name == 'ç©†æ˜­':
                continue
                
            characters.append({
                'name': name,
                'age': info.get('å¹´é¾„', 'æœªçŸ¥'),
                'personality': info.get('æ€§æ ¼', ''),
                'description': info.get('ç®€ä»‹', ''),
                'locations': info.get('æ´»åŠ¨åœ°ç‚¹', []),
                'plots': info.get('å¯è§¦å‘å‰§æƒ…', []),
                'backstory': info.get('èƒŒæ™¯æ•…äº‹', ''),
                'relationships': info.get('äººé™…å…³ç³»', {}),
                'habits': info.get('ç”Ÿæ´»ä¹ æƒ¯', []),
                'appearance': info.get('å¤–è²Œç‰¹å¾', ''),
                'skills': info.get('ç‰¹é•¿æŠ€èƒ½', [])
            })
        
        return characters
    
    def get_locations_list(self) -> List[Dict[str, Any]]:
        """è·å–åœ°ç‚¹åˆ—è¡¨"""
        locations = []
        districts = self.locations_data.get("districts", {})
        
        for district_name, district_info in districts.items():
            district_locations = district_info.get("locations", {})
            for loc_name, loc_info in district_locations.items():
                locations.append({
                    'name': loc_info.get('name', loc_name),
                    'type': loc_info.get('type', ''),
                    'district': district_info.get('name', district_name),
                    'description': loc_info.get('description', ''),
                    'atmosphere': loc_info.get('atmosphere', ''),
                    'keywords': loc_info.get('keywords', [])
                })
        
        return locations
    
    def get_stories_list(self) -> List[Dict[str, Any]]:
        """è·å–å‰§æƒ…åˆ—è¡¨"""
        stories = []
        for story in self.stories_data.get('all_stories', []):
            stories.append({
                'story_id': story.get('story_id', ''),
                'story_name': story.get('story_name', ''),
                'story_overview': story.get('story_overview', ''),
                'story_type': story.get('story_type', ''),
                'characters': json.loads(story.get('selected_characters', '[]')),
                'locations': json.loads(story.get('selected_locations', '[]')),
                'main_conflict': story.get('main_conflict', ''),
                'emotional_development': story.get('emotional_development', '')
            })
        
        return stories
    
    def get_holidays_in_range(self, start_date: str, end_date: str) -> Dict[str, Dict[str, Any]]:
        """è·å–æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„èŠ‚å‡æ—¥"""
        holidays = {}
        start = datetime.strptime(start_date, '%Y-%m-%d')
        end = datetime.strptime(end_date, '%Y-%m-%d')
        
        for date_str, holiday_info in self.holidays_data.items():
            holiday_date = datetime.strptime(date_str, '%Y-%m-%d')
            if start <= holiday_date <= end:
                holidays[date_str] = holiday_info
        
        return holidays
    
    def update_config(self, config_updates: Dict[str, Any]):
        """æ›´æ–°å·¥ä½œæµé…ç½®"""
        self.current_config.update(config_updates)
    
    async def prepare_cycle_summary(self, config: Dict[str, Any]) -> str:
        """å‡†å¤‡å‘¨æœŸæ€»ç»“ï¼Œè·å–å†å²æ•°æ®"""
        try:
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨å‘¨æœŸæ€»ç»“
            if not config.get('enable_cycle_summary', False):
                logger.info("å‘¨æœŸæ€»ç»“åŠŸèƒ½æœªå¯ç”¨")
                return ""
            
            # ç­‰å¾…1ç§’ï¼Œç¡®ä¿æ•°æ®åº“å†™å…¥å®Œæˆ
            import time
            time.sleep(1)
            
            # ä»æ•°æ®åº“è·å–æœ€æ–°çš„å‘¨æœŸæ€»ç»“
            from database.managers.schedule_manager import ScheduleManager
            schedule_manager = ScheduleManager()
            
            # ä¼ é€’å¼€å§‹æ—¥æœŸï¼Œç¡®ä¿è·å–æ—©äºå¼€å§‹æ—¥æœŸä¸”æ—¶é—´ä¸è¶…è¿‡ä¸‰å¤©çš„æ€»ç»“
            start_date = config.get('start_date', '')
            previous_summary = schedule_manager.get_latest_cycle_summary(before_date=start_date)
            
            if previous_summary:
                logger.info(f"è·å–åˆ°é€‚å½“çš„å†å²å‘¨æœŸæ€»ç»“ï¼Œé•¿åº¦: {len(previous_summary)} å­—ç¬¦")
                return previous_summary
            else:
                logger.info(f"æœªæ‰¾åˆ°{start_date}å‰ä¸‰å¤©å†…çš„å‘¨æœŸæ€»ç»“ï¼Œè¿™å¯èƒ½æ˜¯ç¬¬ä¸€ä¸ªå‘¨æœŸæˆ–æ—¶é—´é—´éš”è¾ƒé•¿")
                return ""
                
        except Exception as e:
            logger.error(f"å‡†å¤‡å‘¨æœŸæ€»ç»“å¤±è´¥: {e}")
            return ""
    
    async def create_schedule_graph(self) -> StateGraph:
        """åˆ›å»ºæ—¥ç¨‹ç”Ÿæˆå›¾å·¥ä½œæµ - å¤šå‘¨æœŸå¾ªç¯ç‰ˆæœ¬"""
        self.graph = StateGraph(name="schedule_generation_workflow")
        
        # åˆ›å»ºèŠ‚ç‚¹
        cycle_planning_node = CyclePlanningNode()  # å‘¨æœŸè§„åˆ’èŠ‚ç‚¹
        schedule_generate_node = ScheduleGenerateNode()  # å‘¨æœŸç”ŸæˆèŠ‚ç‚¹
        
        # æ·»åŠ èŠ‚ç‚¹åˆ°å›¾
        self.graph.add_node("cycle_planning", cycle_planning_node)
        self.graph.add_node("schedule_generate", schedule_generate_node)
        
        # å®šä¹‰æ¡ä»¶è·¯ç”±å‡½æ•°
        def should_continue_generation(state):
            """åˆ¤æ–­æ˜¯å¦ç»§ç»­ç”Ÿæˆä¸‹ä¸€ä¸ªå‘¨æœŸ"""
            current_cycle_index = state.get('current_cycle_index', 0)
            cycles = state.get('cycles', [])
            generation_complete = state.get('generation_complete', False)
            
            logger.info(f"ğŸ”„ è·¯ç”±å†³ç­–:")
            logger.info(f"  current_cycle_index: {current_cycle_index}")
            logger.info(f"  len(cycles): {len(cycles) if cycles else 0}")
            logger.info(f"  generation_complete: {generation_complete}")
            
            if generation_complete or current_cycle_index >= len(cycles):
                logger.info(f"ğŸ è·¯ç”±å†³ç­–ï¼šEND")
                return "END"  # ç»“æŸå·¥ä½œæµ
            else:
                logger.info(f"ğŸ”„ è·¯ç”±å†³ç­–ï¼šç»§ç»­ schedule_generate")
                return "schedule_generate"  # ç»§ç»­ç”Ÿæˆä¸‹ä¸€ä¸ªå‘¨æœŸ
        
        # å®šä¹‰èŠ‚ç‚¹è¿æ¥å…³ç³»
        self.graph.add_edge("cycle_planning", "schedule_generate")
        self.graph.add_conditional_edges(
            "schedule_generate",
            should_continue_generation,
            {
                "schedule_generate": "schedule_generate",  # å¾ªç¯ç”Ÿæˆ
                "END": "__end__"  # ç»“æŸ
            }
        )
        
        # è®¾ç½®å…¥å£ç‚¹
        self.graph.set_entry_point("cycle_planning")
        
        return self.graph
    
    async def execute_workflow_stream(self, config: Dict[str, Any], workflow):
        """æµå¼æ‰§è¡Œå·¥ä½œæµ - ä½¿ç”¨StateGraphè‡ªåŠ¨ç¼–æ’"""
        try:
            # å‡†å¤‡åˆå§‹è¾“å…¥
            initial_input = {
                'characters_data': self.characters_data,
                'locations_data': self.locations_data,
                'stories_data': self.stories_data,
                'protagonist_data': self.protagonist_data,
                'holidays_data': self.holidays_data,
                'config': config,
                'protagonist': config.get('protagonist', 'ç©†æ˜­'),
                'schedule_type': config.get('schedule_type', 'weekly'),
                'start_date': config.get('start_date', ''),
                'end_date': config.get('end_date', ''),
                'total_days': config.get('total_days', 7),
                'selected_characters': config.get('selected_characters', []),
                'selected_locations': config.get('selected_locations', []),
                'selected_stories': config.get('selected_stories', []),
                'time_slots_config': config.get('time_slots_config', self.current_config['time_slots_config']),
                'character_distribution': config.get('character_distribution', 'balanced'),
                'story_integration': config.get('story_integration', 'moderate'),
                'include_holidays': config.get('include_holidays', True),
                'include_lunar': config.get('include_lunar', True),
                'workflow_chat': workflow,  # ä¼ é€’UIæ›´æ–°å™¨
                'llm': self.llm  # ä¼ é€’LLMå®ä¾‹
            }
            
            # åˆ›å»ºå¹¶ç¼–è¯‘å›¾å·¥ä½œæµ
            if not self.graph:
                await self.create_schedule_graph()
            
            compiled_graph = self.graph.compile()
            
            # ä½¿ç”¨å›¾çš„æµå¼æ‰§è¡Œ - ä½¿ç”¨async foræ­£ç¡®å¤„ç†å¼‚æ­¥ç”Ÿæˆå™¨
            async for stream_event in compiled_graph.stream(initial_input):
                event_type = stream_event.get('type')
                node_name = stream_event.get('node')
                
                if event_type == 'start':
                    # å·¥ä½œæµå¼€å§‹
                    yield (
                        workflow._create_workflow_progress(),
                        "",
                        "æ—¥ç¨‹ç”Ÿæˆå·¥ä½œæµå¼€å§‹æ‰§è¡Œ...",
                        False
                    )
                
                elif event_type == 'node_start':
                    # èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ
                    node_display_name = self._get_node_display_name(node_name)
                    workflow.current_node = self._get_node_id(node_name)
                    
                    # æ›´æ–°UI - èŠ‚ç‚¹å¼€å§‹çŠ¶æ€
                    await workflow.add_node_message(
                        node_display_name,
                        "å¼€å§‹æ‰§è¡Œ...",
                        "progress"
                    )
                    
                    yield (
                        workflow._create_workflow_progress(),
                        "",
                        f"{node_display_name}å¼€å§‹æ‰§è¡Œ...",
                        False
                    )
                
                elif event_type == 'node_streaming':
                    # èŠ‚ç‚¹æµå¼æ‰§è¡Œä¸­
                    intermediate_result = stream_event.get('intermediate_result')
                    if intermediate_result and intermediate_result.state_update:
                        # è·å–å½“å‰ç”Ÿæˆçš„å†…å®¹é•¿åº¦
                        content_length = 0
                        for key in ['schedule_content', 'daily_schedules', 'schedule_result']:
                            if key in intermediate_result.state_update:
                                if isinstance(intermediate_result.state_update[key], str):
                                    content_length = len(intermediate_result.state_update[key])
                                elif isinstance(intermediate_result.state_update[key], (list, dict)):
                                    content_length = len(str(intermediate_result.state_update[key]))
                                break
                        
                        # å®æ—¶æ›´æ–°è¿›åº¦ä¿¡æ¯ - è·å–æœ€æ–°çš„è¿›åº¦HTMLï¼Œä¸story_workflowä¿æŒä¸€è‡´
                        if content_length > 0:
                            node_display_name = self._get_node_display_name(node_name)
                            await workflow.add_node_message(
                                node_display_name,
                                f"æ­£åœ¨ç”Ÿæˆæ—¥ç¨‹å†…å®¹... å½“å‰ç”Ÿæˆ{content_length}å­—ç¬¦",
                                "streaming"
                            )
                            
                            yield (
                                workflow._create_workflow_progress(),
                                "",
                                f"æ­£åœ¨ç”Ÿæˆæ—¥ç¨‹å†…å®¹... å½“å‰é•¿åº¦: {content_length} å­—ç¬¦",
                                False
                            )
                
                elif event_type == 'node_complete':
                    # èŠ‚ç‚¹æ‰§è¡Œå®Œæˆ
                    node_display_name = self._get_node_display_name(node_name)
                    node_id = self._get_node_id(node_name)
                    
                    # ä¸ºèŠ‚ç‚¹æ·»åŠ å®Œæˆæ¶ˆæ¯ï¼Œç¡®ä¿UIæ­£ç¡®æ›´æ–°
                    if node_name == 'schedule_generate':
                        result_content = "âœ… æ—¥ç¨‹ç”Ÿæˆå®Œæˆ"
                        if 'schedule_result' in stream_event.get('output', {}):
                            schedule_data = stream_event['output']['schedule_result']
                            if isinstance(schedule_data, (dict, list)):
                                result_content = f"âœ… å·²æˆåŠŸç”Ÿæˆ{config['total_days']}å¤©çš„æ—¥ç¨‹å®‰æ’"
                    else:
                        result_content = "âœ… æ‰§è¡Œå®Œæˆ"
                        
                    # æ›´æ–°èŠ‚ç‚¹æ¶ˆæ¯
                    await workflow.add_node_message(
                        node_display_name,
                        result_content,
                        "completed"
                    )
                    
                    yield (
                        workflow._create_workflow_progress(),
                        "",
                        f"{node_display_name}æ‰§è¡Œå®Œæˆ",
                        False
                    )
                
                elif event_type == 'node_error':
                    # èŠ‚ç‚¹æ‰§è¡Œé”™è¯¯
                    error_msg = stream_event.get('error', 'æœªçŸ¥é”™è¯¯')
                    node_display_name = self._get_node_display_name(node_name)
                    
                    await workflow.add_node_message(
                        node_display_name,
                        f"æ‰§è¡Œå¤±è´¥: {error_msg}",
                        "error"
                    )
                    
                    yield (
                        workflow._create_workflow_progress(),
                        "",
                        "",
                        False
                    )
                
                elif event_type == 'final':
                    # å·¥ä½œæµå®Œæˆ
                    yield (
                        workflow._create_workflow_progress(),
                        "",
                        "æ—¥ç¨‹ç”Ÿæˆå·¥ä½œæµæ‰§è¡Œå®Œæˆ",
                        False
                    )
                
                # å…¶ä»–äº‹ä»¶ç±»å‹å¯ä»¥å¿½ç•¥æˆ–è®°å½•æ—¥å¿—
                else:
                    # æŒç»­æ›´æ–°UIä»¥ä¿æŒæµç•…æ€§
                    yield (
                        workflow._create_workflow_progress(),
                        "",
                        "æ—¥ç¨‹ç”Ÿæˆå·¥ä½œæµæ‰§è¡Œä¸­...",
                        False
                    )
                
        except Exception as e:
            logger.error(f"æ—¥ç¨‹ç”Ÿæˆå·¥ä½œæµæµå¼æ‰§è¡Œå¤±è´¥: {e}")
            await workflow.add_node_message(
                "ç³»ç»Ÿ",
                f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}",
                "error"
            )
            yield (
                workflow._create_workflow_progress(),
                "",
                "",
                False
            )
    
    def _get_node_display_name(self, node_name: str) -> str:
        """è·å–èŠ‚ç‚¹æ˜¾ç¤ºåç§°"""
        name_mapping = {
            'cycle_planning': 'å‘¨æœŸè§„åˆ’',
            'schedule_generate': 'æ—¥ç¨‹ç”Ÿæˆ'
        }
        return name_mapping.get(node_name, node_name)
    
    def _get_node_id(self, node_name: str) -> str:
        """è·å–èŠ‚ç‚¹ID"""
        id_mapping = {
            'cycle_planning': 'planning',
            'schedule_generate': 'generate'
        }
        return id_mapping.get(node_name, node_name)


class CyclePlanningNode(BaseNode):
    """å‘¨æœŸè§„åˆ’èŠ‚ç‚¹ - é¢„å…ˆè§„åˆ’æ‰€æœ‰æ‰¹æ¬¡çš„å‘¨æœŸè®¡åˆ’"""
    
    def __init__(self):
        super().__init__(name="cycle_planning", stream=True)
    
    async def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå‘¨æœŸè§„åˆ’èŠ‚ç‚¹ - éæµå¼ç‰ˆæœ¬"""
        final_result = None
        async for result in self.execute_stream(input_data):
            final_result = result
        return final_result or input_data
    
    async def execute_stream(self, input_data: Dict[str, Any]):
        """æµå¼æ‰§è¡Œå‘¨æœŸè§„åˆ’èŠ‚ç‚¹"""
        print("ğŸ“‹ å¼€å§‹å‘¨æœŸè§„åˆ’...")
        
        workflow_chat = input_data.get('workflow_chat')
        llm = input_data.get('llm')
        
        # è·å–é…ç½®å‚æ•°
        start_date = input_data.get('start_date', '')
        end_date = input_data.get('end_date', '')
        total_days = input_data.get('total_days', 7)
        protagonist = input_data.get('protagonist', 'ç©†æ˜­')
        selected_characters = input_data.get('selected_characters', [])
        selected_locations = input_data.get('selected_locations', [])
        config = input_data.get('config', {})
        
        # æ›´æ–°UI
        if workflow_chat:
            await workflow_chat.add_node_message(
                "å‘¨æœŸè§„åˆ’",
                f"æ­£åœ¨ä¸º{total_days}å¤©æ—¶é—´èŒƒå›´åˆ¶å®šå‘¨æœŸè§„åˆ’...",
                "progress"
            )
        
        try:
            from datetime import datetime, timedelta
            import math
            
            # æ–°è®¾è®¡ï¼šæ”¯æŒå¤§æ‰¹æ¬¡ç”Ÿæˆï¼Œæ¯æ¬¡è§„åˆ’ç”Ÿæˆè¾ƒå°‘å‘¨æœŸä½†å¯ä»¥å¤šæ¬¡è°ƒç”¨
            min_cycle_days = 7
            max_cycle_days = 15
            cycles_per_batch = 8  # æ¯æ‰¹æ¬¡æœ€å¤šç”Ÿæˆ8ä¸ªå‘¨æœŸ
            
            # æ™ºèƒ½åˆ†é…å‘¨æœŸé•¿åº¦
            cycles = []
            remaining_days = total_days
            current_date = datetime.strptime(start_date, '%Y-%m-%d')
            
            cycle_num = 1
            cycles_generated = 0
            
            while remaining_days > 0 and cycles_generated < cycles_per_batch:
                # æ ¹æ®å‰©ä½™å¤©æ•°æ™ºèƒ½å†³å®šå‘¨æœŸé•¿åº¦
                if remaining_days <= max_cycle_days:
                    cycle_days = remaining_days
                else:
                    # ä¼˜å…ˆé€‰æ‹©è¾ƒé•¿çš„å‘¨æœŸï¼Œä½†ä¿è¯æœ€åä¸€ä¸ªå‘¨æœŸä¸ä¼šå¤ªçŸ­
                    if remaining_days <= max_cycle_days + min_cycle_days:
                        cycle_days = remaining_days // 2
                    else:
                        cycle_days = random.randint(min_cycle_days, max_cycle_days)
                
                cycle_end_date = current_date + timedelta(days=cycle_days - 1)
                
                cycles.append({
                    'cycle_number': cycle_num,
                    'start_date': current_date.strftime('%Y-%m-%d'),
                    'end_date': cycle_end_date.strftime('%Y-%m-%d'),
                    'total_days': cycle_days,
                    'status': 'planned'
                })
                
                current_date = cycle_end_date + timedelta(days=1)
                remaining_days -= cycle_days
                cycle_num += 1
                cycles_generated += 1
            
            logger.info(f"æœ¬æ‰¹æ¬¡åˆ†é…äº† {len(cycles)} ä¸ªå‘¨æœŸï¼Œå‰©ä½™ {remaining_days} å¤©")
            
            # ä¸ºä¸‹æ¬¡è§„åˆ’å‡†å¤‡ä¿¡æ¯
            next_start_date = current_date.strftime('%Y-%m-%d') if remaining_days > 0 else None
            
            if workflow_chat:
                await workflow_chat.add_node_message(
                    "å‘¨æœŸè§„åˆ’",
                    f"å·²æ™ºèƒ½åˆ†é… {len(cycles)} ä¸ªå‘¨æœŸï¼Œæ¯ä¸ªå‘¨æœŸ {min_cycle_days}-{max_cycle_days} å¤©",
                    "progress"
                )
            
            # å‡†å¤‡å†å²ä¸Šä¸‹æ–‡
            protagonist_data = input_data.get('protagonist_data', '')
            
            # æŒ‡å®šçš„é‡è¦è§’è‰²åˆ—è¡¨
            important_characters = ['ç‘Ÿç³å¨œ', 'éƒèªæ˜', 'æ—å®‰äºˆ', 'å…ƒé€¸', 'å…ƒå—', 'ç½—æ’', 'æ˜“å¥¶å¥¶', 'é‡‘å–œ']
            
            # è·å–é‡è¦è§’è‰²çš„è¯¦ç»†ä¿¡æ¯
            important_characters_info = []
            char_list = input_data.get('characters_data', {}).get("è§’è‰²åˆ—è¡¨", {})
            for char_name in important_characters:
                if char_name in char_list:
                    char_info = char_list[char_name]
                    char_desc = f"{char_name}ï¼ˆé‡è¦è§’è‰²ï¼‰ï¼š{char_info.get('ç®€ä»‹', '')}"
                    if char_info.get('æ€§æ ¼'):
                        char_desc += f"ï¼Œæ€§æ ¼{char_info.get('æ€§æ ¼')}"
                    if char_info.get('å¹´é¾„'):
                        char_desc += f"ï¼Œå¹´é¾„{char_info.get('å¹´é¾„')}"
                    if char_info.get('æ´»åŠ¨åœ°ç‚¹'):
                        char_desc += f"ï¼Œä¸»è¦æ´»åŠ¨åœ°ç‚¹ï¼š{', '.join(char_info.get('æ´»åŠ¨åœ°ç‚¹', []))}"
                    important_characters_info.append(char_desc)
                else:
                    important_characters_info.append(f"{char_name}ï¼ˆé‡è¦è§’è‰²ï¼Œå¾…é…ç½®ï¼‰")
            
            # è·å–ä¸Šä¸€æ‰¹æ¬¡æ€»ç»“ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            previous_summary = config.get('previous_batch_summary', '')
            
            # è·å–æ•´ä¸ªæ—¶é—´æ®µå†…çš„èŠ‚å‡æ—¥ä¿¡æ¯
            holidays_data = input_data.get('holidays_data', {})
            cycle_holidays = []
            if holidays_data:
                from datetime import datetime
                period_start_dt = datetime.strptime(start_date, '%Y-%m-%d')
                period_end_dt = datetime.strptime(end_date, '%Y-%m-%d')
                
                for date_str, holiday_info in holidays_data.items():
                    try:
                        holiday_dt = datetime.strptime(date_str, '%Y-%m-%d')
                        if period_start_dt <= holiday_dt <= period_end_dt:
                            cycle_holidays.append(f"{date_str}: {holiday_info['name']} ({holiday_info['type']})")
                    except:
                        continue
            
            # æ„å»ºå‘¨æœŸè§„åˆ’æç¤ºè¯ï¼šsystemçº¯æŒ‡ä»¤ï¼ŒuseråŠ¨æ€æ•°æ®
            system_planning_instructions = """
# è§„åˆ’è¦æ±‚

## ä¸»è§’ä¸­å¿ƒçš„æ•…äº‹å‘å±•
1. **æ—¶é—´è·¨åº¦æ„Ÿ**ï¼šåœ¨ç»™å®šå¤©æ•°çš„æ—¶é—´æ®µä¸­ï¼Œä¸»è§’çš„ä¸ªäººæˆé•¿å’Œå‘å±•è½¨è¿¹
2. **ä¸ªäººç›®æ ‡æ¨è¿›**ï¼šä¸»è§’çš„å­¦æœ¯ç ”ç©¶ã€å·¥ä½œé¡¹ç›®ã€ä¸ªäººæŠ€èƒ½ç­‰æ–¹é¢çš„è¿›å±•
3. **ç”Ÿæ´»èŠ‚å¥å»ºç«‹**ï¼šå·¥ä½œä¸ç”Ÿæ´»çš„å¹³è¡¡ï¼Œæ—¥å¸¸ä¹ æƒ¯çš„å…»æˆå’Œè°ƒæ•´
4. **å­£èŠ‚é€‚åº”**ï¼šæ ¹æ®å­£èŠ‚å˜åŒ–è°ƒæ•´æ´»åŠ¨å®‰æ’å’Œå¿ƒç†çŠ¶æ€
5. è§„åˆ’ä»¥ä¸ªäººè§„åˆ’ä¸ºä¸»ï¼Œä¸å¼ºåˆ¶ç»‘å®šæŸä¸ªè§’è‰²ï¼Œä¸ä¼šåœ¨å¤§å‘¨æœŸä¸ºæŸä¸ªå…¶ä»–è§’è‰²è€Œåšäº‹
6. **èŠ‚å‡æ—¥ä½“éªŒ**ï¼šåœ¨èŠ‚å‡æ—¥ä¸­çš„ä¸ªäººå®‰æ’å’Œæ–‡åŒ–ä½“éªŒ
7. ä¸å¼ºè¡Œç»‘å®šåœ°ç‚¹ï¼Œä»¥ä¸ªäººè¡Œä¸ºä¸ºä¸»ï¼Œåœ°ç‚¹åªæ˜¯è¾…åŠ©
ä¸åšå’Œæ˜Ÿç©ºï¼Œå¤©æ–‡æœ‰å…³çš„è®¡åˆ’
## æ¯ä¸ªå‘¨æœŸè§„åˆ’å†…å®¹
ä¸ºæ¯ä¸ªå‘¨æœŸåˆ¶å®šï¼š
- **å‘¨æœŸä¸»é¢˜**ï¼šè¿™ä¸ªå‘¨æœŸçš„æ ¸å¿ƒä¸»é¢˜å’Œé‡ç‚¹
- **ä¸»è¦ç›®æ ‡**ï¼šä¸»è§’åœ¨è¿™ä¸ªå‘¨æœŸæƒ³è¦è¾¾æˆçš„å…·ä½“ç›®æ ‡
- **æ ¸å¿ƒåœ°ç‚¹**ï¼šä¸»è¦æ´»åŠ¨åœºæ‰€
- **å…³é”®äº‹ä»¶**ï¼šé¢„è®¡ä¼šå‘ç”Ÿçš„é‡è¦äº‹ä»¶
- **æƒ…æ„ŸåŸºè°ƒ**ï¼šæ•´ä¸ªå‘¨æœŸçš„æƒ…æ„Ÿå‘å±•æ–¹å‘
- **è¡”æ¥è¦ç‚¹**ï¼šä¸å‰åå‘¨æœŸçš„è¿æ¥ç‚¹

# è¾“å‡ºæ ¼å¼
è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºå‘¨æœŸè§„åˆ’ï¼Œç¦æ­¢è¾“å‡ºä»»ä½•å…¶ä»–å†…å®¹ï¼š

```json
{
  "overall_plan": {
    "total_days": <int>,
    "total_cycles": <int>,
    "story_theme": "æ•´ä¸ªæ—¶é—´æ®µçš„æ•…äº‹ä¸»é¢˜",
    "major_milestones": [
      "é‡è¦èŠ‚ç‚¹1",
      "é‡è¦èŠ‚ç‚¹2"
    ]
  },
  "cycle_plans": [
    {
      "cycle_number": 1,
      "start_date": "YYYY-MM-DD",
      "end_date": "YYYY-MM-DD", 
      "total_days": 7,
      "cycle_theme": "å‘¨æœŸä¸»é¢˜",
      "cycle_plan": "ç¬¬ä¸‰äººç§°ï¼Œä»¥ç©†æ˜­ä¸ºä¸»ä½“çš„è¯¦ç»†å‘¨æœŸè®¡åˆ’æè¿°ï¼Œ200-300å­—ï¼ŒåŒ…å«è¿™ä¸ªå‘¨æœŸçš„æ•´ä½“å®‰æ’ã€é‡ç‚¹ç›®æ ‡ã€ä¸»è¦æ´»åŠ¨ç­‰,æ³¨æ„æ˜¯è®¡åˆ’è€Œä¸æ˜¯çº²è¦æ‰€ä»¥ä¸èƒ½æœ‰é¢„çŸ¥èƒ½åŠ›ï¼Œåªæ˜¯è®¡åˆ’ä¸æ˜¯å®é™…å‘ç”Ÿçš„äº‹æƒ…ï¼Œä»¥ç©†æ˜­è®¡åˆ’xxxå¼€å§‹",
      "main_objectives": [
        "ç›®æ ‡1",
        "ç›®æ ‡2"
      ],
      "focus_characters": ["æ— ", "æ— "],
      "secondary_characters": ["æ— ", "æ— "],
      "core_locations": ["åœ°ç‚¹1", "åœ°ç‚¹2"],
      "key_events": [
        "äº‹ä»¶1",
        "äº‹ä»¶2"
      ],
      "emotional_tone": "æƒ…æ„ŸåŸºè°ƒæè¿°",
      "connection_points": {
        "from_previous": "ä¸å‰å‘¨æœŸçš„è¡”æ¥",
        "to_next": "ä¸åå‘¨æœŸçš„è¡”æ¥"
      }
    },
    // ... å…¶ä»–å‘¨æœŸ
  ]
}
```

# é‡è¦è¦æ±‚
1. **è¿è´¯æ€§**ï¼šç¡®ä¿å„å‘¨æœŸä¹‹é—´æœ‰è‡ªç„¶çš„è¿‡æ¸¡å’Œå‘å±•
2. **å¹³è¡¡æ€§**ï¼šè§’è‰²å’Œåœ°ç‚¹çš„åˆ†é…è¦ç›¸å¯¹å‡è¡¡
3. **ç°å®æ€§**ï¼šè§„åˆ’è¦ç¬¦åˆä¸»è§’çš„èº«ä»½å’Œäº‘æ¢å¸‚çš„è®¾å®š
4. **å‘å±•æ€§**ï¼šæ¯ä¸ªå‘¨æœŸéƒ½è¦æœ‰æ˜ç¡®çš„è¿›å±•ï¼Œé¿å…é‡å¤
5. **å®Œæ•´æ€§**ï¼šä¸ºæ‰€æœ‰å‘¨æœŸéƒ½åˆ¶å®šè¯¦ç»†è§„åˆ’

è¯·å¼€å§‹åˆ¶å®šè¿™ä¸ªå…¨é¢è€Œè¯¦ç»†çš„å‘¨æœŸè§„åˆ’ï¼Œç¦æ­¢è¾“å‡ºä»»ä½•å…¶ä»–å†…å®¹ã€‚
"""

            user_planning_dynamic = f"""ä½ æ˜¯ä¸€åä¸“ä¸šçš„é•¿æœŸè§„åˆ’å¸ˆï¼Œéœ€è¦ä¸ºä¸»è§’{protagonist}åˆ¶å®šä»{start_date}åˆ°{end_date}ï¼ˆå…±{total_days}å¤©ï¼‰çš„æ•´ä½“å‘¨æœŸè§„åˆ’ã€‚

# ä¸»è§’ä¿¡æ¯
{protagonist_data}

{f"# å†å²èƒŒæ™¯ä¿¡æ¯{chr(10)}{previous_summary}{chr(10)}" if previous_summary else ''}

# æ´»åŠ¨åœ°ç‚¹
{', '.join(selected_locations)}

# å‘¨æœŸåˆ†é…
å·²åˆ†é…ä¸º{len(cycles)}ä¸ªå‘¨æœŸï¼š
{json.dumps(cycles, ensure_ascii=False, indent=2)}

# èŠ‚å‡æ—¥ä¿¡æ¯
å½“å‰å‘¨æœŸå†…çš„èŠ‚å‡æ—¥ï¼š
{chr(10).join(cycle_holidays) if cycle_holidays else 'æ— ç‰¹æ®ŠèŠ‚å‡æ—¥'}"""
            
            # è°ƒç”¨LLMç”Ÿæˆå‘¨æœŸè§„åˆ’
            logger.info(f"å‘¨æœŸè§„åˆ’: å¼€å§‹LLMè°ƒç”¨ï¼ŒuseråŠ¨æ€é•¿åº¦: {len(user_planning_dynamic)}")
            
            if llm:
                # æ„å»ºæ¶ˆæ¯ï¼ˆsystem çº¯æŒ‡ä»¤ + user åŠ¨æ€èµ„æ–™ï¼‰
                from core.types import Message, MessageRole
                messages = [
                    Message(role=MessageRole.SYSTEM, content=system_planning_instructions),
                    Message(role=MessageRole.USER, content=user_planning_dynamic)
                ]
                
                # æµå¼è°ƒç”¨LLMï¼ˆè±†åŒ…è‡ªå¸¦æ‰“å°ï¼‰
                final_content = ""
                
                async for chunk_data in llm.stream_generate(
                    messages, 
                    mode="think",
                    return_dict=True
                ):
                    content_part = chunk_data.get("content", "")
                    final_content += content_part
                
                logger.info(f"å‘¨æœŸè§„åˆ’ç”Ÿæˆå®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(final_content)}")
            else:
                raise Exception("LLMæœªåˆå§‹åŒ–")
            
            # è§£æJSONç»“æœ
            cycle_planning_data = None
            try:
                from parsers.json_parser import JSONParser
                parser = JSONParser()
                
                json_content = self._extract_json_from_content(final_content)
                logger.info(f"ğŸ” æå–çš„JSONå†…å®¹é•¿åº¦: {len(json_content)}")
                logger.info(f"ğŸ“ JSONå†…å®¹å‰200å­—ç¬¦: {json_content[:200]}...")
                
                parsed_result = parser.parse(json_content)
                
                # ğŸ” è°ƒè¯•ï¼šæ‰“å°è§£æç»“æœçš„ç»“æ„
                logger.info(f"ğŸ“Š è§£æç»“æœç±»å‹: {type(parsed_result)}")
                if isinstance(parsed_result, dict):
                    logger.info(f"ğŸ”‘ è§£æç»“æœé¡¶çº§é”®: {list(parsed_result.keys())}")
                    logger.info(f"ğŸ“ è§£æç»“æœéƒ¨åˆ†å†…å®¹: {str(parsed_result)[:500]}...")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰cycle_planså­—æ®µ
                if parsed_result and 'cycle_plans' in parsed_result:
                    cycle_planning_data = parsed_result
                    logger.info(f"âœ… æ–¹å¼1ï¼šç›´æ¥æ‰¾åˆ°cycle_plansï¼ŒåŒ…å« {len(cycle_planning_data['cycle_plans'])} ä¸ªå‘¨æœŸ")
                elif parsed_result and isinstance(parsed_result, dict):
                    # æ£€æŸ¥æ˜¯å¦åªæœ‰ä¸€ä¸ªé¡¶çº§é”®åŒ…å«æ‰€æœ‰æ•°æ®
                    if len(parsed_result) == 1:
                        root_key = list(parsed_result.keys())[0] 
                        root_data = parsed_result[root_key]
                        if isinstance(root_data, dict) and 'cycle_plans' in root_data:
                            cycle_planning_data = root_data
                            logger.info(f"âœ… æ–¹å¼2ï¼šä»æ ¹é”® '{root_key}' ä¸­æ‰¾åˆ°cycle_plansï¼ŒåŒ…å« {len(cycle_planning_data['cycle_plans'])} ä¸ªå‘¨æœŸ")
                        else:
                            # å°è¯•ç”¨æ ‡å‡†jsonè§£æ
                            try:
                                complete_parsed = json.loads(json_content)
                                if 'cycle_plans' in complete_parsed:
                                    cycle_planning_data = complete_parsed
                                    logger.info(f"âœ… æ–¹å¼3ï¼šjson.loadsè§£ææˆåŠŸï¼ŒåŒ…å« {len(cycle_planning_data['cycle_plans'])} ä¸ªå‘¨æœŸ")
                                else:
                                    raise Exception(f"æ‰€æœ‰è§£ææ–¹å¼éƒ½æ— æ³•æ‰¾åˆ°cycle_planså­—æ®µï¼Œé¡¶çº§é”®: {list(complete_parsed.keys())}")
                            except Exception as json_error:
                                raise Exception(f"æ‰€æœ‰JSONè§£ææ–¹å¼éƒ½å¤±è´¥: {json_error}")
                    else:
                        raise Exception(f"è§£æç»“æœæœ‰å¤šä¸ªé¡¶çº§é”®ä½†æ— cycle_plans: {list(parsed_result.keys())}")
                else:
                    raise Exception("è§£æç»“æœä¸ºç©ºæˆ–ä¸æ˜¯å­—å…¸ç±»å‹")
                
                if cycle_planning_data and 'cycle_plans' in cycle_planning_data:
                    if workflow_chat:
                        await workflow_chat.add_node_message(
                            "å‘¨æœŸè§„åˆ’",
                            f"âœ… æˆåŠŸç”Ÿæˆ {len(cycle_planning_data['cycle_plans'])} ä¸ªå‘¨æœŸçš„è¯¦ç»†è§„åˆ’",
                            "success"
                        )
                else:
                    raise Exception("æœ€ç»ˆæœªèƒ½è·å–æœ‰æ•ˆçš„cycle_planning_data")
                    
            except Exception as parse_error:
                logger.error(f"å‘¨æœŸè§„åˆ’JSONè§£æå¤±è´¥: {parse_error}")
                # ä½¿ç”¨åŸå§‹åˆ†é…çš„å‘¨æœŸä½œä¸ºåå¤‡æ–¹æ¡ˆ
                cycle_planning_data = {
                    'overall_plan': {
                        'total_days': total_days,
                        'total_cycles': len(cycles),
                        'story_theme': f"{protagonist}çš„{total_days}å¤©ç”Ÿæ´»è§„åˆ’",
                        'character_arcs': {},
                        'major_milestones': []
                    },
                    'cycle_plans': cycles
                }
                
                # ä¸ºåå¤‡æ–¹æ¡ˆä¹Ÿç”ŸæˆåŸºç¡€çš„å‘¨æœŸè®¡åˆ’æè¿°
                for cycle_plan in cycle_planning_data['cycle_plans']:
                    cycle_number = cycle_plan.get('cycle_number', 1)
                    total_days = cycle_plan.get('total_days', 0)
                    cycle_plan['cycle_plan'] = f"å‘¨æœŸ{cycle_number}ï¼š{protagonist}å°†åœ¨{total_days}å¤©å†…é‡ç‚¹å…³æ³¨ä¸ªäººå‘å±•å’Œæ—¥å¸¸ç”Ÿæ´»çš„å¹³è¡¡ï¼Œé€šè¿‡è§„å¾‹çš„å·¥ä½œå­¦ä¹ å’Œé€‚åº¦çš„ç¤¾äº¤æ´»åŠ¨ï¼Œé€æ­¥æ¨è¿›å„é¡¹ç›®æ ‡çš„å®ç°ï¼Œä¿æŒèº«å¿ƒå¥åº·å’Œç§¯æçš„ç”Ÿæ´»çŠ¶æ€ã€‚"
                
                if workflow_chat:
                    await workflow_chat.add_node_message(
                        "å‘¨æœŸè§„åˆ’",
                        f"âš ï¸ JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€å‘¨æœŸåˆ†é…ï¼ˆ{len(cycles)}ä¸ªå‘¨æœŸï¼‰",
                        "warning"
                    )
            
            # è¾“å‡ºæœ€ç»ˆç»“æœ
            output_data = input_data.copy()
            output_data['cycle_planning_result'] = cycle_planning_data
            output_data['cycles'] = cycle_planning_data['cycle_plans']
            output_data['current_cycle_index'] = 0  # å½“å‰å¤„ç†çš„å‘¨æœŸç´¢å¼•
            
            logger.info(f"âœ… å‘¨æœŸè§„åˆ’å®Œæˆï¼Œç”Ÿæˆäº† {len(cycle_planning_data['cycle_plans'])} ä¸ªå‘¨æœŸ")
            yield output_data
            
        except Exception as e:
            logger.error(f"å‘¨æœŸè§„åˆ’å¤±è´¥: {e}")
            if workflow_chat:
                await workflow_chat.add_node_message(
                    "å‘¨æœŸè§„åˆ’",
                    f"âŒ è§„åˆ’å¤±è´¥: {str(e)}",
                    "error"
                )
            raise Exception(f"å‘¨æœŸè§„åˆ’å¤±è´¥: {str(e)}")
    

    
    def _extract_json_from_content(self, content: str) -> str:
        """ä»ç”Ÿæˆå†…å®¹ä¸­æå–JSONéƒ¨åˆ† - å¢å¼ºç‰ˆJSONæå–"""
        import re
        import json
        
        logger.info(f"ğŸ” å¼€å§‹æå–JSONï¼ŒåŸå§‹å†…å®¹é•¿åº¦: {len(content)}")
        
        # æ–¹æ³•1: ä¼˜å…ˆæŸ¥æ‰¾```json...```ä»£ç å—
        json_pattern = r'```json\s*(.*?)\s*```'
        matches = re.findall(json_pattern, content, re.DOTALL | re.IGNORECASE)
        
        for match in matches:
            extracted_json = match.strip()
            if self._is_valid_json(extracted_json):
                logger.info(f"âœ… ä»```json```ä»£ç å—æå–æœ‰æ•ˆJSONï¼Œé•¿åº¦: {len(extracted_json)}")
                return extracted_json
        
        # æ–¹æ³•2: æŸ¥æ‰¾```...```ä»£ç å—ï¼ˆä¸ä¸€å®šæ ‡æ³¨jsonï¼‰
        code_pattern = r'```[a-zA-Z]*\s*(.*?)\s*```'
        code_matches = re.findall(code_pattern, content, re.DOTALL | re.IGNORECASE)
        
        for match in code_matches:
            extracted = match.strip()
            if extracted.startswith('{') and self._is_valid_json(extracted):
                logger.info(f"âœ… ä»ä»£ç å—æå–æœ‰æ•ˆJSONï¼Œé•¿åº¦: {len(extracted)}")
                return extracted
        
        # æ–¹æ³•3: ä½¿ç”¨æ‹¬å·åŒ¹é…è®¡æ•°æå–å®Œæ•´JSON
        def extract_complete_json(text):
            start_pos = text.find('{')
            if start_pos == -1:
                return None
            
            brace_count = 0
            in_string = False
            escape_next = False
            
            for i, char in enumerate(text[start_pos:], start_pos):
                if escape_next:
                    escape_next = False
                    continue
                    
                if char == '\\' and in_string:
                    escape_next = True
                    continue
                    
                if char == '"' and not escape_next:
                    in_string = not in_string
                    continue
                    
                if not in_string:
                    if char == '{':
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            return text[start_pos:i+1]
            
            return None
        
        complete_json = extract_complete_json(content)
        if complete_json and self._is_valid_json(complete_json):
            logger.info(f"âœ… ä½¿ç”¨æ‹¬å·åŒ¹é…æå–æœ‰æ•ˆJSONï¼Œé•¿åº¦: {len(complete_json)}")
            return complete_json.strip()
        
        # æ–¹æ³•4: å¤šé‡æ­£åˆ™åŒ¹é…åéªŒè¯
        json_patterns = [
            r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}',  # ç®€å•åµŒå¥—
            r'\{.*?\}',  # è´ªå©ªåŒ¹é…
            r'\{.*\}'    # æœ€è´ªå©ªåŒ¹é…
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, content, re.DOTALL)
            if matches:
                # æŒ‰é•¿åº¦æ’åºï¼Œä¼˜å…ˆå°è¯•æœ€é•¿çš„åŒ¹é…
                sorted_matches = sorted(matches, key=len, reverse=True)
                for match in sorted_matches:
                    if self._is_valid_json(match):
                        logger.info(f"âœ… æ­£åˆ™æ¨¡å¼åŒ¹é…åˆ°æœ‰æ•ˆJSONï¼Œé•¿åº¦: {len(match)}")
                        return match.strip()
        
        logger.warning("âŒ æ‰€æœ‰æ–¹æ³•éƒ½æœªèƒ½æå–æœ‰æ•ˆJSONï¼Œè¿”å›åŸå†…å®¹")
        return content.strip()
    
    def _is_valid_json(self, json_str: str) -> bool:
        """éªŒè¯JSONå­—ç¬¦ä¸²æ˜¯å¦æœ‰æ•ˆ"""
        try:
            json.loads(json_str)
            return True
        except (json.JSONDecodeError, ValueError):
            return False


class ScheduleGenerateNode(BaseNode):
    """æ—¥ç¨‹ç”ŸæˆèŠ‚ç‚¹ - åˆ†æ‰¹æ¸è¿›å¼ç”Ÿæˆï¼Œä¸€æ¬¡ç”Ÿæˆ3å¤©æ—¥ç¨‹"""
    
    def __init__(self):
        super().__init__(name="schedule_generate", stream=True)
    
    async def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæ—¥ç¨‹ç”ŸæˆèŠ‚ç‚¹ - éæµå¼ç‰ˆæœ¬"""
        # ä½¿ç”¨æµå¼æ‰§è¡Œå¹¶è¿”å›æœ€ç»ˆç»“æœ
        final_result = None
        async for result in self.execute_stream(input_data):
            final_result = result
        return final_result or input_data
    
    async def execute_stream(self, input_data: Dict[str, Any]):
        """æµå¼æ‰§è¡Œæ—¥ç¨‹ç”ŸæˆèŠ‚ç‚¹ - å¤šå‘¨æœŸæ‰¹æ¬¡ç”Ÿæˆ"""
        print("ğŸ“… å¼€å§‹æ‰¹æ¬¡æ—¥ç¨‹ç”Ÿæˆ...")
        
        try:
            workflow_chat = input_data.get('workflow_chat')
            llm = input_data.get('llm')
            
            # è·å–å½“å‰æ‰§è¡ŒçŠ¶æ€
            current_cycle_index = input_data.get('current_cycle_index', 0)
            cycles = input_data.get('cycles', [])
            
            logger.info(f"ğŸ” æ—¥ç¨‹ç”ŸæˆèŠ‚ç‚¹çŠ¶æ€æ£€æŸ¥:")
            logger.info(f"  current_cycle_index: {current_cycle_index}")
            logger.info(f"  cycles æ•°é‡: {len(cycles) if cycles else 0}")
            logger.info(f"  cycles ç±»å‹: {type(cycles)}")
            print(f"ğŸ” DEBUG: current_cycle_index={current_cycle_index}, cyclesæ•°é‡={len(cycles) if cycles else 0}")
            
            if not cycles:
                logger.error("âŒ ç¼ºå°‘å‘¨æœŸè§„åˆ’æ•°æ®ï¼Œè¯·å…ˆæ‰§è¡Œå‘¨æœŸè§„åˆ’èŠ‚ç‚¹")
                print("âŒ DEBUG: ç¼ºå°‘å‘¨æœŸè§„åˆ’æ•°æ®")
                raise Exception("ç¼ºå°‘å‘¨æœŸè§„åˆ’æ•°æ®ï¼Œè¯·å…ˆæ‰§è¡Œå‘¨æœŸè§„åˆ’èŠ‚ç‚¹")
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å‘¨æœŸéƒ½å·²å®Œæˆ
            if current_cycle_index >= len(cycles):
                logger.info(f"âœ… æ‰€æœ‰ {len(cycles)} ä¸ªå‘¨æœŸå·²å®Œæˆ")
                print(f"âœ… DEBUG: æ‰€æœ‰ {len(cycles)} ä¸ªå‘¨æœŸå·²å®Œæˆ")
                if workflow_chat:
                    await workflow_chat.add_node_message(
                        "æ—¥ç¨‹ç”Ÿæˆ",
                        f"âœ… æ‰€æœ‰ {len(cycles)} ä¸ªå‘¨æœŸçš„æ—¥ç¨‹ç”Ÿæˆå·²å®Œæˆï¼",
                        "success"
                    )
                
                output_data = input_data.copy()
                output_data['generation_complete'] = True
                yield output_data
                return
                
            print(f"ğŸ” DEBUG: å‡†å¤‡å¤„ç†å‘¨æœŸ {current_cycle_index + 1}/{len(cycles)}")
                
            # è·å–å½“å‰è¦å¤„ç†çš„å‘¨æœŸ
            current_cycle = cycles[current_cycle_index]
            logger.info(f"ğŸ”„ å¼€å§‹å¤„ç†ç¬¬ {current_cycle_index + 1}/{len(cycles)} ä¸ªå‘¨æœŸ")
            logger.info(f"ğŸ” å½“å‰å‘¨æœŸè¯¦ç»†ä¿¡æ¯: {current_cycle}")
            print(f"ğŸ” DEBUG: å½“å‰å‘¨æœŸä¿¡æ¯: {current_cycle}")
            
            cycle_start_date = current_cycle['start_date']
            cycle_end_date = current_cycle['end_date']
            cycle_total_days = current_cycle['total_days']
            
            logger.info(f"ğŸ“… å‘¨æœŸæ—¥æœŸèŒƒå›´: {cycle_start_date} - {cycle_end_date}, å…±{cycle_total_days}å¤©")
            print(f"ğŸ“… DEBUG: æ—¥æœŸèŒƒå›´: {cycle_start_date} - {cycle_end_date}, {cycle_total_days}å¤©")
            
        except Exception as e:
            logger.error(f"âŒ æ—¥ç¨‹ç”ŸæˆèŠ‚ç‚¹åˆå§‹åŒ–å¤±è´¥: {e}")
            print(f"âŒ DEBUG: åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise e
        
        # è·å–é…ç½®å‚æ•°
        protagonist = input_data.get('protagonist', 'ç©†æ˜­')
        selected_characters = input_data.get('selected_characters', [])
        selected_locations = input_data.get('selected_locations', [])
        holidays_data = input_data.get('holidays_data', {})
        include_holidays = input_data.get('include_holidays', True)
        
        # æ›´æ–°UI - å¼€å§‹çŠ¶æ€
        if workflow_chat:
            await workflow_chat.add_node_message(
                "æ—¥ç¨‹ç”Ÿæˆ",
                f"æ­£åœ¨ç”Ÿæˆç¬¬ {current_cycle_index + 1}/{len(cycles)} ä¸ªå‘¨æœŸçš„æ—¥ç¨‹ ({cycle_start_date} - {cycle_end_date}, {cycle_total_days}å¤©)...",
                "progress"
            )
                    
        # è·å–å½“å‰å‘¨æœŸçš„è§„åˆ’ä¿¡æ¯
        current_cycle_plan = current_cycle.get('cycle_theme', '')
        current_cycle_objectives = current_cycle.get('main_objectives', [])
        focus_characters = current_cycle.get('focus_characters', [])
        secondary_characters = current_cycle.get('secondary_characters', [])
        core_locations = current_cycle.get('core_locations', [])
        key_events = current_cycle.get('key_events', [])
        emotional_tone = current_cycle.get('emotional_tone', '')
        
        # è·å–æœ€è¿‘4ä¸ªæ‰¹æ¬¡çš„summaryä½œä¸ºå†å²è®°å½•
        recent_batch_summaries = await self._get_recent_batch_summaries(4, cycle_start_date)
        batch_history_context = ""
        logger.info(f"ğŸ” å°è¯•è·å–å†å²æ‰¹æ¬¡æ€»ç»“ï¼Œæ—¥æœŸç•Œé™: {cycle_start_date}")
        logger.info(f"ğŸ“‹ è·å–åˆ° {len(recent_batch_summaries)} ä¸ªå†å²æ‰¹æ¬¡æ€»ç»“:")
        for i, summary in enumerate(recent_batch_summaries):
            logger.info(f"  ğŸ“ æ€»ç»“ {i+1}: {summary[:150]}...")
        if recent_batch_summaries:
            batch_history_context = f"## æœ€è¿‘æ‰¹æ¬¡å†å²è®°å½•{chr(10)}{chr(10).join(recent_batch_summaries)}{chr(10)}"
            logger.info(f"âœ… å†å²è®°å½•ä¸Šä¸‹æ–‡å·²æ„å»ºï¼Œé•¿åº¦: {len(batch_history_context)} å­—ç¬¦")
        
        # ğŸ” è°ƒè¯•ï¼šç¡®è®¤ä»£ç ç»§ç»­æ‰§è¡Œ
        print("ğŸ” DEBUG: å†å²è®°å½•è·å–å®Œæˆï¼Œç»§ç»­æ‰§è¡Œ...")
        logger.info("ğŸ” å†å²è®°å½•è·å–å®Œæˆï¼Œç»§ç»­æ‰§è¡Œ...")
        
        # åˆ†æ‰¹ç”Ÿæˆï¼šå°†å‘¨æœŸåˆ†æˆ3å¤©ä¸€æ‰¹
        batch_size = 3  # æ¯æ¬¡ç”Ÿæˆ3å¤©
        cycle_daily_schedules = []  # å­˜å‚¨æ•´ä¸ªå‘¨æœŸçš„æ—¥ç¨‹
        current_batch_start = 0
        
        print(f"ğŸ” DEBUG: å‡†å¤‡åˆ†æ‰¹ç”Ÿæˆï¼Œbatch_size={batch_size}")
        logger.info(f"ğŸ” å‡†å¤‡åˆ†æ‰¹ç”Ÿæˆï¼Œbatch_size={batch_size}")

        # å‡†å¤‡å½“å‰å‘¨æœŸçš„æ‰€æœ‰æ—¥æœŸä¿¡æ¯
        cycle_dates_info = []
        print(f"ğŸ” DEBUG: cycle_dates_info åˆå§‹åŒ–å®Œæˆï¼Œå‡†å¤‡è¿›å…¥tryå—")
        logger.info(f"ğŸ” cycle_dates_info åˆå§‹åŒ–å®Œæˆï¼Œå‡†å¤‡è¿›å…¥tryå—")
        
        try:
            print("ğŸ” DEBUG: è¿›å…¥æ—¥æœŸè§£ætryå—")
            logger.info("ğŸ” è¿›å…¥æ—¥æœŸè§£ætryå—")
            
            from datetime import datetime, timedelta
            
            print(f"ğŸ” DEBUG: å‡†å¤‡è§£ææ—¥æœŸ - cycle_start_date={cycle_start_date}, cycle_end_date={cycle_end_date}")
            logger.info(f"ğŸ” å‡†å¤‡è§£ææ—¥æœŸ - cycle_start_date={cycle_start_date}, cycle_end_date={cycle_end_date}")
            
            # è§£æå‘¨æœŸæ—¥æœŸèŒƒå›´
            cycle_start = datetime.strptime(cycle_start_date, '%Y-%m-%d')
            cycle_end = datetime.strptime(cycle_end_date, '%Y-%m-%d')
            
            logger.info(f"ğŸ“… è§£æçš„æ—¥æœŸèŒƒå›´: {cycle_start} - {cycle_end}")
            print(f"ğŸ“… DEBUG: è§£æçš„æ—¥æœŸèŒƒå›´æˆåŠŸ: {cycle_start} - {cycle_end}")
            
            # è·å–å‘¨æœŸå†…çš„æ‰€æœ‰æ—¥æœŸä¿¡æ¯
            current_date = cycle_start
            day_number = 1
            print(f"ğŸ” DEBUG: å¼€å§‹ç”Ÿæˆæ—¥æœŸï¼Œä» {cycle_start} åˆ° {cycle_end}")
            while current_date <= cycle_end:
                date_str = current_date.strftime('%Y-%m-%d')
                weekday = current_date.weekday()
                weekday_name = ['å‘¨ä¸€', 'å‘¨äºŒ', 'å‘¨ä¸‰', 'å‘¨å››', 'å‘¨äº”', 'å‘¨å…­', 'å‘¨æ—¥'][weekday]
            
                # æ£€æŸ¥æ˜¯å¦èŠ‚å‡æ—¥
                is_holiday = False
                holiday_name = ""
                if include_holidays and date_str in holidays_data:
                    is_holiday = True
                    holiday_name = holidays_data[date_str]['name']
            
                # æ·»åŠ æ—¥æœŸä¿¡æ¯
                cycle_dates_info.append({
                    'date': date_str,
                    'weekday': weekday,
                    'weekday_name': weekday_name,
                    'is_holiday': is_holiday,
                    'holiday_name': holiday_name,
                    'day_number': day_number,  # å‘¨æœŸå†…çš„å¤©æ•°
                    'cycle_day_number': day_number
                })
                
                print(f"  ğŸ“… DEBUG: æ·»åŠ æ—¥æœŸ {date_str} ({weekday_name})")
                
                current_date += timedelta(days=1)
                day_number += 1
                
            logger.info(f"ğŸ“Š ç”Ÿæˆäº† {len(cycle_dates_info)} å¤©çš„æ—¥æœŸä¿¡æ¯")
            if cycle_dates_info:
                logger.info(f"ğŸ“… ç¬¬ä¸€å¤©: {cycle_dates_info[0]}")
                logger.info(f"ğŸ“… æœ€åä¸€å¤©: {cycle_dates_info[-1]}")
                
        except Exception as e:
            print(f"âŒ DEBUG: å‘¨æœŸæ—¥æœŸå¤„ç†å¤±è´¥ - å¼‚å¸¸ç±»å‹: {type(e).__name__}, é”™è¯¯ä¿¡æ¯: {str(e)}")
            logger.error(f"å‘¨æœŸæ—¥æœŸå¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            logger.error(traceback.format_exc())
            if workflow_chat:
                await workflow_chat.add_node_message(
                    "æ—¥ç¨‹ç”Ÿæˆ",
                    f"å‘¨æœŸæ—¥æœŸå¤„ç†å¤±è´¥: {str(e)}",
                    "error"
                )
            raise Exception(f"å‘¨æœŸæ—¥æœŸå¤„ç†å¤±è´¥: {str(e)}")
            
        # ğŸ” å…³é”®è°ƒè¯•ï¼šæ£€æŸ¥whileå¾ªç¯æ¡ä»¶
        print(f"ğŸ” DEBUG: æ—¥æœŸè§£æå®Œæˆï¼Œå‡†å¤‡æ£€æŸ¥whileå¾ªç¯æ¡ä»¶")
        logger.info(f"ğŸ” DEBUG: æ—¥æœŸè§£æå®Œæˆï¼Œå‡†å¤‡æ£€æŸ¥whileå¾ªç¯æ¡ä»¶")
        
        logger.info(f"ğŸ” whileå¾ªç¯æ¡ä»¶æ£€æŸ¥:")
        logger.info(f"  current_batch_start: {current_batch_start}")
        logger.info(f"  len(cycle_dates_info): {len(cycle_dates_info)}")
        logger.info(f"  å¾ªç¯æ¡ä»¶ current_batch_start < len(cycle_dates_info): {current_batch_start < len(cycle_dates_info)}")
        
        print(f"ğŸ” DEBUG: æ£€æŸ¥cycle_dates_infoé•¿åº¦")
        print(f"  len(cycle_dates_info) = {len(cycle_dates_info)}")
        print(f"  current_batch_start = {current_batch_start}")
        
        if len(cycle_dates_info) == 0:
            print("âŒ DEBUG: cycle_dates_info ä¸ºç©ºï¼")
            logger.error("âŒ cycle_dates_info ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œæ‰¹æ¬¡ç”Ÿæˆ")
            raise Exception("cycle_dates_info ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œæ‰¹æ¬¡ç”Ÿæˆ")
        
        print(f"ğŸ” DEBUG: å‡†å¤‡è¿›å…¥whileå¾ªç¯")
        logger.info(f"ğŸ” å‡†å¤‡è¿›å…¥whileå¾ªç¯")
            
        # åˆ†æ‰¹ç”Ÿæˆå½“å‰å‘¨æœŸçš„æ—¥ç¨‹
        batch_count = 0
        while current_batch_start < len(cycle_dates_info):
            batch_count += 1
            print(f"ğŸ”„ DEBUG: è¿›å…¥whileå¾ªç¯ï¼Œæ‰¹æ¬¡ {batch_count}")
            logger.info(f"ğŸ”„ å¼€å§‹ç¬¬ {batch_count} ä¸ªæ‰¹æ¬¡ï¼Œcurrent_batch_start = {current_batch_start}")
            
            print(f"ğŸ” DEBUG: æ­¥éª¤1 - ç¡®å®šæ‰¹æ¬¡æ—¥æœŸèŒƒå›´")
            # ç¡®å®šå½“å‰æ‰¹æ¬¡çš„æ—¥æœŸèŒƒå›´
            batch_end = min(current_batch_start + batch_size, len(cycle_dates_info))
            batch_dates = cycle_dates_info[current_batch_start:batch_end]
            batch_days_count = len(batch_dates)
            
            batch_start_date = batch_dates[0]['date']
            batch_end_date = batch_dates[-1]['date']
            
            print(f"ğŸ” DEBUG: æ­¥éª¤2 - æ—¥æœŸèŒƒå›´ç¡®å®šå®Œæˆ: {batch_start_date} - {batch_end_date}")
            logger.info(f"ğŸ“… æ‰¹æ¬¡ {batch_count} æ—¥æœŸèŒƒå›´: {batch_start_date} - {batch_end_date}, {batch_days_count}å¤©")
            
            print(f"ğŸ” DEBUG: æ­¥éª¤3 - å‡†å¤‡æ›´æ–°UI")
            if workflow_chat:
                await workflow_chat.add_node_message(
                    "æ—¥ç¨‹ç”Ÿæˆ",
                    f"æ­£åœ¨ç”Ÿæˆç¬¬ {current_batch_start//batch_size + 1} æ‰¹æ¬¡ï¼š{batch_start_date} - {batch_end_date} ({batch_days_count}å¤©)",
                    "progress"
                )
            print(f"ğŸ” DEBUG: æ­¥éª¤4 - UIæ›´æ–°å®Œæˆ")
        
            print(f"ğŸ” DEBUG: æ­¥éª¤5 - å¼€å§‹æ”¶é›†è§’è‰²ä¿¡æ¯")
            # æ”¶é›†æ‰€æœ‰å¯ç”¨è§’è‰²ä¿¡æ¯ï¼ˆå®Œæ•´ä¿¡æ¯ï¼Œä¸çœç•¥ï¼‰
            char_list = input_data.get('characters_data', {}).get("è§’è‰²åˆ—è¡¨", {})
            
            print(f"ğŸ” DEBUG: æ­¥éª¤6 - è§’è‰²åˆ—è¡¨è·å–å®Œæˆï¼ŒåŒ…å« {len(char_list)} ä¸ªè§’è‰²")
            
            # æ”¶é›†æ‰¹æ¬¡è§’è‰²ä¿¡æ¯
            all_batch_characters = set()
            
            # ä¼˜å…ˆä½¿ç”¨å‘¨æœŸæ¨èçš„é‡ç‚¹è§’è‰²
            if focus_characters:
                all_batch_characters.update(focus_characters)
            if secondary_characters:
                all_batch_characters.update(secondary_characters)
            
            # è¡¥å……å…¶ä»–é€‰ä¸­çš„è§’è‰²
            all_batch_characters.update(selected_characters)
            
            # ç¡®ä¿æœ‰è¶³å¤Ÿçš„è§’è‰²ï¼ˆæœ€å°‘5ä¸ªï¼‰
            all_available_chars = [name for name in char_list.keys() if name != protagonist]
            while len(all_batch_characters) < min(5, len(all_available_chars)):
                for char_name in all_available_chars:
                    if char_name not in all_batch_characters:
                        all_batch_characters.add(char_name)
                        break
            
            all_batch_characters = list(all_batch_characters)
            logger.info(f"ğŸ“‹ æ‰¹æ¬¡è§’è‰²æ± : {len(all_batch_characters)} ä¸ªè§’è‰²")
            
            # è·å–è§’è‰²è¯¦ç»†ä¿¡æ¯
            all_characters_info = []
            for char_name in all_batch_characters:
                if char_name in char_list:
                    char_info = char_list[char_name]
                    char_desc = f"{char_name}ï¼š{char_info.get('ç®€ä»‹', '')}"
                    if char_info.get('æ€§æ ¼'):
                        char_desc += f"ï¼Œæ€§æ ¼{char_info.get('æ€§æ ¼')}"
                    if char_info.get('å¹´é¾„'):
                        char_desc += f"ï¼Œå¹´é¾„{char_info.get('å¹´é¾„')}"
                    if char_info.get('æ´»åŠ¨åœ°ç‚¹'):
                        char_desc += f"ï¼Œä¸»è¦æ´»åŠ¨åœ°ç‚¹ï¼š{', '.join(char_info.get('æ´»åŠ¨åœ°ç‚¹', []))}"
                    if char_info.get('å¯è§¦å‘å‰§æƒ…'):
                        char_desc += f"ï¼Œå¯è§¦å‘å‰§æƒ…ï¼š{', '.join(char_info.get('å¯è§¦å‘å‰§æƒ…', [])[:2])}"
                    all_characters_info.append(char_desc)
            
            # è·å–ä¸»è§’ä¿¡æ¯
            protagonist_data = input_data.get('protagonist_data', '')
            print(f"ğŸ” DEBUG: è·å–ä¸»è§’ä¿¡æ¯å®Œæˆï¼Œé•¿åº¦: {len(protagonist_data)}")
            
            # æ„å»ºæ‰¹æ¬¡ç”Ÿæˆæç¤ºè¯
            print(f"ğŸ” DEBUG: å¼€å§‹æ„å»ºæç¤ºè¯...")
            print(f"  - batch_start_date: {batch_start_date}")
            print(f"  - batch_end_date: {batch_end_date}")
            print(f"  - batch_days_count: {batch_days_count}")
            print(f"  - protagonist: {protagonist}")
            print(f"  - len(all_characters_info): {len(all_characters_info)}")
            print(f"  - len(selected_locations): {len(selected_locations)}")
            print(f"  - len(batch_dates): {len(batch_dates)}")
            
            try:
                # æ„å»ºæ‰¹æ¬¡ç”Ÿæˆæç¤ºè¯ï¼šsystemçº¯æŒ‡ä»¤ï¼ŒuseråŠ¨æ€æ•°æ®
                system_generation_instructions = """# æ ¸å¿ƒç”Ÿæˆè¦æ±‚
ä¸åšå’Œæ˜Ÿç©ºï¼Œå¤©æ–‡æœ‰å…³çš„è®¡åˆ’
## åˆ†æ‰¹ç”Ÿæˆè¿è´¯æ€§
1. **æ‰¹æ¬¡è¡”æ¥**ï¼šè™½ç„¶åªç”ŸæˆæŒ‡å®šå¤©æ•°ï¼Œä½†è¦ä¸å‰åæ‰¹æ¬¡è‡ªç„¶è¡”æ¥
2. **å‘¨æœŸç›®æ ‡æ¨è¿›**ï¼šåœ¨è¿™å‡ å¤©ä¸­æ¨è¿›å½“å‰å‘¨æœŸçš„ç›®æ ‡å’Œä¸»é¢˜

## äº‘æ¢å¸‚çœŸå®ç”Ÿæ´»æ„Ÿ
1. **æ—¥å¸¸éšæœºäº‹ä»¶**ï¼šå¶é‡ç†Ÿäººã€å‘ç°æ–°åº—é“ºã€å°æ„å¤–ã€å¤©æ°”å˜åŒ–ç­‰ç”Ÿæ´»åŒ–å…ƒç´ 
2. **åŸå¸‚ç”Ÿæ´»ç»†èŠ‚**ï¼šè¡—è¾¹å°åº—ã€å’–å•¡é¦†ã€å…¬å›­æ•£æ­¥ã€èœå¸‚åœºã€å…¬äº¤åœ°é“ã€ç¤¾åŒºæ´»åŠ¨ç­‰
3. **å­£èŠ‚èŠ‚æ—¥æ°›å›´**ï¼šæ ¹æ®å­£èŠ‚å’ŒèŠ‚å‡æ—¥å®‰æ’åº”æ™¯çš„æ´»åŠ¨å’Œæ°›å›´
4. **ç”Ÿæ´»åŒ–äº’åŠ¨**ï¼šè´­ç‰©ã€ç”¨é¤ã€ä¼‘é—²å¨±ä¹ã€è¿åŠ¨å¥èº«ã€è¯»ä¹¦å­¦ä¹ ç­‰æ—¥å¸¸æ´»åŠ¨
5. **é¿å…è®¾å®š**ï¼šä¸¥ç¦æ¶‰åŠå¤©æ–‡ã€æ˜Ÿç©ºã€å®‡å®™ç­‰ä¸»é¢˜ï¼Œé‡ç‚¹çªå‡ºéƒ½å¸‚ç”Ÿæ´»çš„çƒŸç«æ°”
6. ä¸å‡ºç°å’Œå…¶ä»–è§’è‰²å¼ºç›¸å…³çš„å®‰æ’ï¼Œä»¥ç‹¬ç«‹ä¸ªäººè®¡åˆ’ä¸ºä¸»

## æ•…äº‹æ€§è¦æ±‚
1. **æƒ…æ„Ÿæ¨è¿›**ï¼šæ¯ä¸ªè§’è‰²çš„å‡ºç°éƒ½åº”è¯¥æœ‰å…³ç³»å‘å±•ï¼Œæ¨è¿›å‘¨æœŸä¸»é¢˜
2. **ç»†èŠ‚ä¸°å¯Œåº¦**ï¼šæ¯ä¸ªæ—¶é—´æ®µçš„æè¿°åŒ…å«å…·ä½“çš„å¯¹è¯ç‰‡æ®µã€å†…å¿ƒæ´»åŠ¨ã€ç¯å¢ƒæå†™
3. **äº‹ä»¶è¿è´¯æ€§**ï¼šå½“å‰æ‰¹æ¬¡å†…çš„äº‹ä»¶è¦ç›¸äº’å‘¼åº”ï¼Œå½¢æˆå®Œæ•´çš„æ•…äº‹ç‰‡æ®µ
4. **ç”Ÿæ´»çœŸå®æ„Ÿ**ï¼šåŒ…å«å·¥ä½œå‹åŠ›ã€æƒ…ç»ªæ³¢åŠ¨ã€å°ç¡®å¹¸ã€æ„å¤–æƒŠå–œç­‰çœŸå®å…ƒç´ 

## è®¡åˆ’ä¸æ€»ç»“çš„åŒºåˆ«
- **æ¯æ—¥è®¡åˆ’(daily_plan)**ï¼šä¸»è§’å¯¹è¿™ä¸€å¤©çš„é¢„æœŸå’Œå®‰æ’ï¼ŒåŸºäºä»–ç°æœ‰çš„ä¿¡æ¯å’Œç»éªŒ
- **æ¯æ—¥æ€»ç»“(daily_summary)**ï¼šä¸€å¤©ç»“æŸåå¯¹å®é™…å‘ç”Ÿäº‹ä»¶çš„å›é¡¾ï¼Œå¯èƒ½ä¸è®¡åˆ’æœ‰å‡ºå…¥ï¼ŒåŒ…å«æ„å¤–å’ŒæƒŠå–œ
- **æ‰¹æ¬¡æ€»ç»“(batch_summary)**ï¼šå‡ å¤©ç»“æŸåçš„é˜¶æ®µæ€§æ€»ç»“ï¼Œå…³æ³¨è¿™å‡ å¤©çš„é‡è¦å‘å±•

## æ—¶é—´æ®µå†…å®¹è¦æ±‚
1. **å¤œé—´(23:00-06:00)**ï¼šä¼‘æ¯ã€æ¢¦å¢ƒã€æ·±å¤œæ€è€ƒï¼Œå¶å°”æœ‰ç‰¹æ®Šæƒ…å†µ
2. **ä¸Šåˆ(06:00-11:00)**ï¼šå·¥ä½œã€ç ”ç©¶ã€é‡è¦ä¼šè®®ï¼Œç²¾ç¥çŠ¶æ€æœ€ä½³çš„æ—¶æ®µ
3. **ä¸­åˆ(11:00-14:00)**ï¼šç”¨é¤ã€è½»æ¾ç¤¾äº¤ã€çŸ­æš‚ä¼‘æ¯
4. **ä¸‹åˆ(14:00-18:00)**ï¼šç»§ç»­å·¥ä½œã€å®åœ°è€ƒå¯Ÿã€å­¦æœ¯æ´»åŠ¨
5. **æ™šä¸Š(18:00-23:00)**ï¼šç¤¾äº¤æ´»åŠ¨ã€å¨±ä¹ã€ä¸ªäººæ—¶é—´ã€æ·±åº¦äº¤æµ

## ç‹¬ç«‹æ•…äº‹è¦æ±‚
1. **æ—¶é—´æ®µæ•…äº‹ç‹¬ç«‹æ€§**ï¼šæ¯ä¸ªæ—¶é—´æ®µçš„æ•…äº‹å†…å®¹å¿…é¡»æ˜¯ç‹¬ç«‹å®Œæ•´çš„ï¼Œèƒ½å¤Ÿå•ç‹¬é˜…è¯»ç†è§£
2. **å‰å› åæœæ¸…æ™°**ï¼šå³ä½¿æ˜¯ç‹¬ç«‹çš„æ—¶é—´æ®µæ•…äº‹ï¼Œä¹Ÿè¦æè¿°æ¸…æ¥šäº‹ä»¶çš„å‰å› åæœ
3. **æƒ…å¢ƒå®Œæ•´æ€§**ï¼šåŒ…å«æ˜ç¡®çš„åœºæ™¯ã€äººç‰©ã€å¯¹è¯å’Œæƒ…æ„Ÿæè¿°ï¼Œä¿è¯å†…å®¹çš„å®Œæ•´æ€§
4. **ç‹¬ç«‹å™äº‹**ï¼šæ¯ä¸ªæ—¶é—´æ®µå†…å®¹å¯èƒ½è¢«å•ç‹¬æå–ä½¿ç”¨ï¼Œå› æ­¤å¿…é¡»æ˜¯è‡ªåŒ…å«çš„å®Œæ•´æ•…äº‹
5. **ä¸Šä¸‹æ–‡è¿è´¯**ï¼šè™½ç„¶æ˜¯ç‹¬ç«‹çš„ï¼Œä½†å„æ—¶é—´æ®µä¹‹é—´åº”è¯¥æœ‰è¿è´¯çš„å…³ç³»ï¼Œå½¢æˆæ—¥å¸¸ç”Ÿæ´»çš„å®Œæ•´ç”»é¢

# é‡è¦æé†’
1. **åˆ†æ‰¹ç”Ÿæˆè¦æ±‚**ï¼š
   - åªç”ŸæˆæŒ‡å®šå¤©æ•°çš„æ—¥ç¨‹ï¼Œä¸è¦ç”Ÿæˆæ•´ä¸ªå‘¨æœŸ
   - è¦ä½“ç°å‘¨æœŸè§„åˆ’çš„ä¸»é¢˜å’Œç›®æ ‡ï¼Œä½†é‡ç‚¹æ˜¯å½“å‰æ‰¹æ¬¡
   - è¦ä¸ºåç»­æ‰¹æ¬¡ç•™ä¸‹è‡ªç„¶çš„è¡”æ¥ç‚¹

2. **æ•°æ®å®Œæ•´æ€§è¦æ±‚**ï¼š
   - daily_planï¼šæ¯å¤©éƒ½è¦æœ‰å…·ä½“çš„æ—©æ™¨è®¡åˆ’
   - daily_involved_charactersï¼šå¿…é¡»åˆ—å‡ºå½“å¤©æ‰€æœ‰å‡ºç°çš„æœ‰é…ç½®çš„è§’è‰²åç§°
   - æ¯å¤©å¿…é¡»æœ‰5ä¸ªå®Œæ•´çš„æ—¶é—´æ®µï¼ˆå¤œé—´ã€ä¸Šåˆã€ä¸­åˆã€ä¸‹åˆã€æ™šä¸Šï¼‰
   - involved_charactersï¼šæ¯ä¸ªæ—¶é—´æ®µéƒ½è¦æ˜ç¡®åˆ—å‡ºæ¶‰åŠçš„è§’è‰²åç§°åˆ—è¡¨
   - batch_summaryï¼šå¿…é¡»åŒ…å«è¿™å‡ å¤©çš„é˜¶æ®µæ€§æ€»ç»“

3. **æ—¥ç¨‹å†…å®¹è¦æ±‚**ï¼š
   - æ¯ä¸ªæ—¶é—´æ®µçš„schedule_contentå¿…é¡»ç®€æ´æ˜ç¡®ï¼Œé‡ç‚¹è®°å½•å®é™…æ´»åŠ¨
   - å„æ—¶é—´æ®µå†…å®¹ç‹¬ç«‹å®Œæ•´ï¼Œæ˜ç¡®è®°å½•æ—¶é—´åœ°ç‚¹äººå‘˜æ´»åŠ¨ç›®çš„
   - å†…å®¹çœŸå®å…·ä½“ï¼Œé¿å…è™šæ„æƒ…èŠ‚å’Œä¸å¿…è¦çš„æè¿°
   - å¯åŒ…å«æ—¥å¸¸ç”Ÿæ´»çš„çœŸå®å…ƒç´ ï¼šå·¥ä½œå®‰æ’ã€ç¤¾äº¤æ´»åŠ¨ã€ç”Ÿæ´»çäº‹ã€å·¥ä½œå‹åŠ›ã€å°ç¡®å¹¸ã€æ„å¤–æƒŠå–œç­‰çœŸå®å…ƒç´ 
   - ç¦æ­¢æœ‰ä»»ä½•ç”·å¥³æ‹çˆ±å…ƒç´ 
   - ä¸¥ç¦æ¶‰åŠå¤©æ–‡ã€æ˜Ÿç©ºã€å®‡å®™ç­‰ä¸»é¢˜ï¼Œé‡ç‚¹ä½“ç°æ™®é€šéƒ½å¸‚ç”Ÿæ´»
   - ä½“ç°äº‘æ¢å¸‚çš„ç”Ÿæ´»èŠ‚å¥ï¼šå·¥ä½œã€ç”¨é¤ã€ç¤¾äº¤ã€ä¼‘é—²ã€èŠ‚æ—¥æ´»åŠ¨ç­‰æ—¥å¸¸å®‰æ’

4. **è§’è‰²å¤„ç†è¦æ±‚**ï¼š
   - é‡ç‚¹è§’è‰²è¦å¤šå®‰æ’ï¼Œä½“ç°å‘¨æœŸä¸»é¢˜
   - å…¶ä»–è§’è‰²æ ¹æ®ç”Ÿæ´»é€»è¾‘è‡ªç„¶å‡ºç°
   - å¯ä»¥åˆ›é€ ä¸´æ—¶è§’è‰²ï¼ˆå¦‚åº—ä¸»ã€è·¯äººã€å°åŠ¨ç‰©ï¼‰å¢åŠ çœŸå®æ„Ÿ
   - involved_charactersä¸­åªéœ€åˆ—å‡ºè§’è‰²åç§°ï¼Œä¸éœ€è¦æè¿°

5. **æŠ€æœ¯è¦æ±‚**ï¼š
   - ç¡®ä¿JSONæ ¼å¼å®Œå…¨æ­£ç¡®ï¼Œå¯ä»¥è¢«ç¨‹åºè§£æ
   - æ¯ä¸ªå­—æ®µéƒ½è¦å¡«å†™å®Œæ•´ï¼Œä¸èƒ½ä¸ºç©º
   - å…³æ³¨batch_summaryå­—æ®µï¼Œå®ƒæ˜¯æœ¬æ‰¹æ¬¡çš„é‡è¦æ€»ç»“
   - è¾“å‡ºçš„å†…å®¹ä¸­ç¦æ­¢åŒ…å«""å’Œ\ï¼Œäººç‰©å¯¹è¯ç›´æ¥ç”¨:è¡”æ¥å³å¯

ç¦æ­¢è¾“å…¥ä»»ä½•å…¶ä»–å†…å®¹ã€‚

# è¾“å‡ºæ ¼å¼
è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºæ‰¹æ¬¡æ—¥ç¨‹å®‰æ’ï¼Œå¿…é¡»é™„åŠ markdownæ ‡è¯†ï¼Œç¦æ­¢è¾“å‡ºä»»ä½•å…¶ä»–å†…å®¹ï¼š

```json
{
  "batch_info": {
    "æ‰¹æ¬¡å¤©æ•°": <int>,
    "æ‰¹æ¬¡å¼€å§‹æ—¥æœŸ": "YYYY-MM-DD",
    "æ‰¹æ¬¡ç»“æŸæ—¥æœŸ": "YYYY-MM-DD",
    "æ‰€å±å‘¨æœŸ": <int>,
    "å‘¨æœŸä¸»é¢˜": "string",
    "æ‰¹æ¬¡ç‰¹ç‚¹": "æè¿°è¿™å‡ å¤©çš„ä¸»è¦ç‰¹è‰²å’Œæ•…äº‹å‘å±•",
    "é‡ç‚¹è§’è‰²": ["string"],
    "ä¸»è¦åœ°ç‚¹": ["string"]
  },

  "daily_schedules": [
    {
      "date": "YYYY-MM-DD",
      "day_number": 1,
      "weekday_name": "å‘¨å‡ ",
      "is_holiday": true/false,
      "holiday_name": "èŠ‚æ—¥åç§°ï¼ˆå¦‚æœæ˜¯èŠ‚å‡æ—¥ï¼‰",
      "weather": "å¤©æ°”æƒ…å†µ",
      "daily_plan": "ç©†æ˜­çš„è®¡åˆ’å®‰æ’ï¼Œç¬¬ä¸‰äººç§°ä»¥ä»–ä¸ºä¸»ä½“æè¿°å½“å¤©çš„å…·ä½“æ‰“ç®—ï¼Œ200-300å­—ï¼ŒåŒ…å«ï¼šä¸»è¦ç›®æ ‡ã€å…·ä½“å®‰æ’ã€æœŸæœ›æ”¶è·",
      "daily_involved_characters": ["æ— ", "æ— ", "è§’è‰²å3"],
      "time_slots": [
        {
          "slot_name": "å¤œé—´",
          "location": "å…·ä½“åœ°ç‚¹",
          "schedule_content": "å…·ä½“çš„æ—¥ç¨‹å®‰æ’è®°å½•ï¼šæ—¶é—´+å…·ä½“åœ°ç‚¹+å…·ä½“æ´»åŠ¨+ç›®çš„ï¼Œæ¶‰åŠçš„å®ä½“çš„ç»†èŠ‚ã€‚ç®€æ´æ˜ç¡®ï¼Œ80-200å­—ã€‚",
          "involved_characters": ["æ— ", "æ— "]
        },
        {
          "slot_name": "ä¸Šåˆ",
          "location": "å…·ä½“åœ°ç‚¹",
          "schedule_content": "å…·ä½“çš„æ—¥ç¨‹å®‰æ’è®°å½•ï¼šæ—©æ™¨éœ€è¦ç¬¦åˆä¸»è§’é¥®é£Ÿä¹ æƒ¯çš„é¥®é£Ÿç»†èŠ‚ï¼Œæ—¶é—´+åœ°ç‚¹+å…·ä½“æ´»åŠ¨+ç›®çš„ï¼Œæ¶‰åŠçš„å®ä½“çš„ç»†èŠ‚ã€‚ç®€æ´æ˜ç¡®ï¼Œ80-200å­—ã€‚",
          "involved_characters": ["æ— ", "æ— "]
        },
        {
          "slot_name": "ä¸­åˆ",
          "location": "å…·ä½“åœ°ç‚¹",
          "schedule_content": "å…·ä½“çš„æ—¥ç¨‹å®‰æ’è®°å½•ï¼šç¬¦åˆä¸»è§’é¥®é£Ÿä¹ æƒ¯çš„é¥®é£Ÿç»†èŠ‚ï¼Œæ—¶é—´+åœ°ç‚¹+å…·ä½“æ´»åŠ¨+ç›®çš„ï¼Œæ¶‰åŠçš„å®ä½“çš„ç»†èŠ‚ã€‚ç®€æ´æ˜ç¡®ï¼Œ80-200å­—ã€‚",
          "involved_characters": ["æ— ", "æ— "]
        },
        {
          "slot_name": "ä¸‹åˆ",
          "location": "å…·ä½“åœ°ç‚¹",
          "schedule_content": "å…·ä½“çš„æ—¥ç¨‹å®‰æ’è®°å½•ï¼šæ—¶é—´+å…·ä½“åœ°ç‚¹+å‚ä¸äººå‘˜+ç›®çš„ï¼Œæ¶‰åŠçš„å®ä½“çš„ç»†èŠ‚ã€‚ç®€æ´æ˜ç¡®ï¼Œ80-200å­—ã€‚",
          "involved_characters": ["æ— ", "æ— "]
        },
        {
          "slot_name": "æ™šä¸Š",
          "location": "å…·ä½“åœ°ç‚¹",
          "schedule_content": "å…·ä½“çš„æ—¥ç¨‹å®‰æ’è®°å½•ï¼šç¬¦åˆä¸»è§’é¥®é£Ÿä¹ æƒ¯çš„é¥®é£Ÿç»†èŠ‚ï¼Œæ—¶é—´+å…·ä½“åœ°ç‚¹+å…·ä½“æ´»åŠ¨+ç›®çš„ï¼Œæ¶‰åŠçš„å®ä½“çš„ç»†èŠ‚ã€‚ç®€æ´æ˜ç¡®ï¼Œ80-200å­—ã€‚",
          "involved_characters": ["æ— ", "æ— "]
        }
      ],
      "daily_summary": "ç¬¬ä¸‰äººç§°ï¼Œä»¥è§’è‰²åä¸ºä¸»ä½“ï¼Œä¸€å¤©ç»“æŸæ—¶çš„ç®€è¦æ€»ç»“ï¼Œ200-300å­—ï¼Œé‡ç‚¹å…³æ³¨ï¼šé‡è¦äº‹ä»¶ã€äººç‰©äº’åŠ¨ã€å¿ƒæƒ…å˜åŒ–ã€å‘ç°æ€è€ƒ"
    }
  ],
  "batch_summary": "è¿™å‡ å¤©çš„é‡è¦å‘å±•æ€»ç»“ï¼ŒåŒ…å«ç›®æ ‡æ¨è¿›å’Œå…³ç³»å˜åŒ–ï¼Œ150-200å­—"
}
```"""

                user_generation_dynamic = f"""ä½ æ˜¯ä¸€åä¸“ä¸šçš„æ—¥ç¨‹è§„åˆ’å¸ˆå’Œæ•…äº‹ç¼–å‰§ï¼Œéœ€è¦ä¸ºä¸»è§’{protagonist}ç”Ÿæˆ{batch_start_date}åˆ°{batch_end_date}çš„è¯¦ç»†æ—¥ç¨‹å®‰æ’ï¼ˆå…±{batch_days_count}å¤©ï¼‰ã€‚

è¿™æ˜¯ä¸€ä¸ªåˆ†æ‰¹æ¸è¿›å¼ç”Ÿæˆä»»åŠ¡ï¼Œå½“å‰ç”Ÿæˆçš„æ˜¯ä¸€ä¸ªæ›´å¤§å‘¨æœŸä¸­çš„ä¸€éƒ¨åˆ†ã€‚

# ä¸»è§’ä¿¡æ¯
{protagonist_data}

{batch_history_context if batch_history_context else ''}

# å½“å‰å‘¨æœŸè§„åˆ’èƒŒæ™¯
## å‘¨æœŸä¿¡æ¯
- å‘¨æœŸæ—¥æœŸï¼š{cycle_start_date} è‡³ {cycle_end_date}ï¼ˆç¬¬{current_cycle_index + 1}ä¸ªå‘¨æœŸï¼Œå…±{len(cycles)}ä¸ªå‘¨æœŸï¼‰
- å‘¨æœŸä¸»é¢˜ï¼š{current_cycle_plan}
- æƒ…æ„ŸåŸºè°ƒï¼š{emotional_tone}

## å‘¨æœŸç›®æ ‡
{chr(10).join([f"- {obj}" for obj in current_cycle_objectives])}

## æ ¸å¿ƒåœ°ç‚¹ï¼ˆæœ¬å‘¨æœŸï¼‰
{chr(10).join([f"- {loc}" for loc in core_locations])}

## å…³é”®äº‹ä»¶ï¼ˆæœ¬å‘¨æœŸé¢„æœŸï¼‰
{chr(10).join([f"- {event}" for event in key_events])}

# å½“å‰æ‰¹æ¬¡ä»»åŠ¡
- æ‰¹æ¬¡æ—¥æœŸï¼š{batch_start_date} è‡³ {batch_end_date}
- æ‰¹æ¬¡å¤©æ•°ï¼š{batch_days_count}å¤©
- è¿™æ˜¯å½“å‰å‘¨æœŸçš„ç¬¬ {current_batch_start//batch_size + 1} ä¸ªæ‰¹æ¬¡
- æ¯å¤©åˆ’åˆ†ä¸º5ä¸ªæ—¶é—´æ®µï¼šå¤œé—´(23:00-06:00)ã€ä¸Šåˆ(06:00-11:00)ã€ä¸­åˆ(11:00-14:00)ã€ä¸‹åˆ(14:00-18:00)ã€æ™šä¸Š(18:00-23:00)

# å¯ç”¨åœ°ç‚¹
{', '.join(selected_locations)}

# æ‰¹æ¬¡æ—¥æœŸä¿¡æ¯
{json.dumps(batch_dates, ensure_ascii=False, indent=2)}"""
                
                print(f"ğŸ” DEBUG: æç¤ºè¯æ„å»ºå®Œæˆï¼Œuseré•¿åº¦: {len(user_generation_dynamic)}")
                logger.info(f"ğŸš€ å³å°†è°ƒç”¨LLMç”Ÿæˆæ‰¹æ¬¡ {batch_count}")
                logger.info(f"ğŸ“ useråŠ¨æ€é•¿åº¦: {len(user_generation_dynamic)} å­—ç¬¦")
                logger.info(f"ğŸ¤– LLM å¯¹è±¡: {llm}")
                
                if not llm:
                    raise Exception("LLMå¯¹è±¡æœªåˆå§‹åŒ–")
                
                # è°ƒç”¨LLMç”Ÿæˆæ‰¹æ¬¡æ—¥ç¨‹ï¼ˆsystem çº¯æŒ‡ä»¤ + user åŠ¨æ€èµ„æ–™ï¼‰
                from core.types import Message, MessageRole
                messages = [
                    Message(role=MessageRole.SYSTEM, content=system_generation_instructions),
                    Message(role=MessageRole.USER, content=user_generation_dynamic)
                ]
                
                print(f"ğŸš€ DEBUG: å¼€å§‹è°ƒç”¨LLMæµå¼ç”Ÿæˆ...")
                
                final_content = ""
                chunk_count = 0
                async for chunk_data in llm.stream_generate(
                    messages, 
                    mode="think",
                    return_dict=True
                ):
                    chunk_count += 1
                    content_part = chunk_data.get("content", "")
                    final_content += content_part
                    
                    # æ¯100ä¸ªchunkæ›´æ–°ä¸€æ¬¡è¿›åº¦
                    if chunk_count % 100 == 0:
                        print(f"ğŸ”„ DEBUG: å·²æ¥æ”¶ {chunk_count} ä¸ªchunkï¼Œå½“å‰å†…å®¹é•¿åº¦: {len(final_content)}")
                
                print(f"âœ… DEBUG: LLMç”Ÿæˆå®Œæˆï¼Œæ€»chunkæ•°: {chunk_count}ï¼Œæœ€ç»ˆå†…å®¹é•¿åº¦: {len(final_content)}")
                logger.info(f"ğŸ“ æ‰¹æ¬¡ {batch_count} ç”Ÿæˆå®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(final_content)} å­—ç¬¦")
                
                # ä¿å­˜åŸå§‹å›å¤åˆ°TXTæ–‡ä»¶
                await self._save_raw_response_to_txt(final_content, current_cycle_index + 1, batch_count, batch_start_date, batch_end_date)
                
            except Exception as prompt_error:
                logger.error(f"âŒ æç¤ºè¯æ„å»ºæˆ–LLMè°ƒç”¨å¤±è´¥: {prompt_error}")
                print(f"âŒ DEBUG: æç¤ºè¯æ„å»ºæˆ–LLMè°ƒç”¨å¤±è´¥: {prompt_error}")
                import traceback
                traceback.print_exc()
                
                # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ‰¹æ¬¡ï¼Œä¸è¦å› ä¸ºä¸€ä¸ªæ‰¹æ¬¡å¤±è´¥å°±åœæ­¢
                current_batch_start += batch_size
                continue
            
            # è§£æJSONç»“æœ
            batch_data = None
            try:
                from parsers.json_parser import JSONParser
                parser = JSONParser()
                
                json_content = self._extract_json_from_content(final_content)
                logger.info(f"ğŸ” æå–çš„JSONå†…å®¹é•¿åº¦: {len(json_content)}")
                
                parsed_result = parser.parse(json_content)
                
                if parsed_result and 'daily_schedules' in parsed_result:
                    batch_data = parsed_result
                    logger.info(f"âœ… æˆåŠŸè§£ææ‰¹æ¬¡JSONï¼ŒåŒ…å« {len(batch_data['daily_schedules'])} å¤©æ—¥ç¨‹")
                else:
                    raise Exception("è§£æç»“æœç¼ºå°‘daily_scheduleså­—æ®µ")
                    
            except Exception as parse_error:
                logger.error(f"âŒ æ‰¹æ¬¡JSONè§£æå¤±è´¥: {parse_error}")
                
                # åˆ›å»ºåŸºç¡€çš„æ‰¹æ¬¡æ•°æ®ä½œä¸ºåå¤‡
                batch_data = {
                    'batch_summary': f"æ‰¹æ¬¡{batch_count}ï¼š{batch_start_date}è‡³{batch_end_date}çš„æ—¥ç¨‹ï¼ˆè§£æå¤±è´¥ï¼‰",
                    'daily_schedules': []
                }
                
                
                if workflow_chat:
                    await workflow_chat.add_node_message(
                        "æ—¥ç¨‹ç”Ÿæˆ",
                        f"âš ï¸ æ‰¹æ¬¡ {batch_count} JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€ç»“æ„ç»§ç»­",
                        "warning"
                    )
            
            # ä¿å­˜æ‰¹æ¬¡JSONæ•°æ®åˆ°TXT
            await self._save_batch_json_to_txt(batch_data, current_cycle_index + 1, batch_count, batch_start_date, batch_end_date)
            
            # å¢é‡ä¿å­˜åˆ°CSV
            batch_daily_schedules = batch_data.get('daily_schedules', [])
            await self._save_batch_to_csv_incrementally(batch_daily_schedules, batch_data, current_cycle_index + 1, batch_count, current_cycle)
            
            # å°†æ‰¹æ¬¡æ—¥ç¨‹æ·»åŠ åˆ°å‘¨æœŸæ—¥ç¨‹ä¸­
            cycle_daily_schedules.extend(batch_daily_schedules)
            
            # æ›´æ–°UIè¿›åº¦
            if workflow_chat:
                await workflow_chat.add_node_message(
                    "æ—¥ç¨‹ç”Ÿæˆ", 
                    f"âœ… æ‰¹æ¬¡ {batch_count} å®Œæˆï¼š{batch_start_date} - {batch_end_date} ({len(batch_daily_schedules)}å¤©)",
                    "success"
                )
            
            # æ›´æ–°æ‰¹æ¬¡è¿›åº¦
            current_batch_start += batch_size
        
        # å½“å‰å‘¨æœŸæ‰€æœ‰æ‰¹æ¬¡ç”Ÿæˆå®Œæˆï¼Œæ„å»ºå‘¨æœŸç»“æœ
        logger.info(f"ğŸ“‹ å‘¨æœŸ {current_cycle_index + 1} æ‰€æœ‰æ‰¹æ¬¡å®Œæˆï¼Œå‡†å¤‡æ„å»ºå‘¨æœŸç»“æœ")
        
        # ç”Ÿæˆå‘¨æœŸæ€»ç»“
        cycle_summary = ""
        if cycle_daily_schedules:
            try:
                current_cycle_info = {
                    'cycle_number': current_cycle_index + 1,
                    'cycle_theme': current_cycle_plan,
                    'main_objectives': current_cycle_objectives,
                    'focus_characters': focus_characters
                }
                cycle_summary = await self._generate_cycle_summary(current_cycle_info, cycle_daily_schedules, llm, workflow_chat)
            except Exception as summary_error:
                logger.error(f"ç”Ÿæˆå‘¨æœŸæ€»ç»“å¤±è´¥: {summary_error}")
                cycle_summary = f"å‘¨æœŸ{current_cycle_index + 1}å®Œæˆï¼Œå…±{len(cycle_daily_schedules)}å¤©ï¼Œä¸»é¢˜ï¼š{current_cycle_plan}ã€‚"
        
        # æ„å»ºå‘¨æœŸæ•°æ®
        schedule_data = {
            'cycle_info': {
                'cycle_number': current_cycle_index + 1,
                'start_date': cycle_start_date,
                'end_date': cycle_end_date,
                'total_days': cycle_total_days,
                'cycle_theme': current_cycle_plan,
                'cycle_plan': current_cycle.get('cycle_plan', f"å‘¨æœŸ{current_cycle_index + 1}ä¸»é¢˜ï¼š{current_cycle_plan}"),  # æ·»åŠ è¯¦ç»†å‘¨æœŸè®¡åˆ’
                'focus_characters': focus_characters,
                'core_locations': core_locations
            },
            'daily_schedules': cycle_daily_schedules,
            'cycle_summary': cycle_summary
        }
        
        # ç«‹å³ä¿å­˜å‘¨æœŸåˆ°CSV
        await self._save_cycle_to_csv_immediately(schedule_data, current_cycle_index + 1)
        
        # æ›´æ–°UI
        if workflow_chat:
            await workflow_chat.add_node_message(
                "æ—¥ç¨‹ç”Ÿæˆ",
                f"âœ… å‘¨æœŸ {current_cycle_index + 1} å®Œæˆï¼šå…±ç”Ÿæˆ {len(cycle_daily_schedules)} å¤©æ—¥ç¨‹",
                "success"
            )
        
        # æ›´æ–°è¾“å‡ºæ•°æ®
        output_data = input_data.copy()
        output_data['schedule_result'] = schedule_data
        output_data['daily_schedules'] = cycle_daily_schedules
        output_data['current_cycle_index'] = current_cycle_index + 1  # æŒ‡å‘ä¸‹ä¸€ä¸ªå‘¨æœŸ
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å‘¨æœŸéƒ½å®Œæˆäº†
        if current_cycle_index + 1 >= len(cycles):
            output_data['generation_complete'] = True
            logger.info(f"âœ… æ‰€æœ‰ {len(cycles)} ä¸ªå‘¨æœŸç”Ÿæˆå®Œæˆ")
            print(f"âœ… DEBUG: æ‰€æœ‰å‘¨æœŸå®Œæˆæ ‡è®°å·²è®¾ç½®")
        else:
            logger.info(f"âœ… å‘¨æœŸ {current_cycle_index + 1} å®Œæˆï¼Œå‡†å¤‡ä¸‹ä¸€ä¸ªå‘¨æœŸ")
            print(f"âœ… DEBUG: å½“å‰å‘¨æœŸå®Œæˆï¼Œå‡†å¤‡ä¸‹ä¸€ä¸ªå‘¨æœŸ")
        
        print(f"âœ… å‘¨æœŸ {current_cycle_index + 1} æ—¥ç¨‹ç”Ÿæˆå®Œæˆ")
        print(f"ğŸ” DEBUG: å‡†å¤‡yieldè¾“å‡ºæ•°æ®")
        yield output_data
        print(f"ğŸ” DEBUG: yieldå®Œæˆ")
    
    async def _save_batch_to_csv_incrementally(self, batch_daily_schedules: List[Dict], batch_data: Dict, cycle_number: int, batch_number: int, current_cycle: Dict):
        """æ¯3å¤©æ‰¹æ¬¡å®Œæˆåå¢é‡ä¿å­˜åˆ°CSVï¼ˆä¸»è¦è¾“å‡ºæ–‡ä»¶ï¼‰"""
        try:
            from pathlib import Path
            import csv
            import os
            from datetime import datetime
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = Path("workspace/batch_schedule_output")
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # ä½¿ç”¨å›ºå®šæ–‡ä»¶åè¿›è¡Œå¢é‡ä¿å­˜ï¼ˆä¸»è¦è¾“å‡ºï¼‰
            csv_file_path = output_dir / "batch_schedules_raw.csv"
            
            # å®šä¹‰CSVåˆ—å¤´ï¼ˆä¸batch_schedule_generator.pyä¿æŒä¸€è‡´ï¼‰
            csv_headers = [
                "æ—¥æœŸ", "æ˜ŸæœŸ", "èŠ‚æ—¥ä¿¡æ¯", "å­£èŠ‚", "å¤©æ°”", "ä¸»é¢˜", 
                "å‘¨æœŸè®¡åˆ’", "æ‰¹æ¬¡æ€»ç»“", "æ¯æ—¥è®¡åˆ’", "æ¯æ—¥æ€»ç»“", "æ¶‰åŠè§’è‰²", "è§’è‰²ç®€ä»‹",
                "ä¸Šåˆ", "ä¸­åˆ", "ä¸‹åˆ", "æ™šä¸Š", "å¤œé—´"
            ]
            
            # è·å–å‘¨æœŸå’Œæ‰¹æ¬¡ä¿¡æ¯
            cycle_theme = current_cycle.get('cycle_theme', '')
            cycle_plan = current_cycle.get('cycle_plan', f"å‘¨æœŸ{cycle_number}ä¸»é¢˜ï¼š{cycle_theme}")  # è¯¦ç»†çš„å‘¨æœŸè®¡åˆ’
            batch_summary = batch_data.get('batch_summary', '')  # ä½¿ç”¨LLMç”Ÿæˆçš„æ‰¹æ¬¡æ€»ç»“
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå†³å®šæ˜¯è¿½åŠ è¿˜æ˜¯åˆ›å»ºæ–°æ–‡ä»¶
            file_exists = csv_file_path.exists()
            write_mode = 'a' if file_exists else 'w'
            
            logger.info(f"ğŸ”„ {'è¿½åŠ ' if file_exists else 'åˆ›å»º'}æ‰¹æ¬¡CSV: å‘¨æœŸ{cycle_number}, æ‰¹æ¬¡{batch_number}, åŒ…å«{len(batch_daily_schedules)}å¤©")
            if batch_summary:
                logger.info(f"ğŸ“ æ‰¹æ¬¡æ€»ç»“: {batch_summary[:100]}...")
            
            # å†™å…¥CSVæ–‡ä»¶ï¼ˆå¢é‡ä¿å­˜ï¼‰
            with open(csv_file_path, write_mode, encoding='utf-8', newline='') as csvfile:
                writer = csv.writer(csvfile)
                
                # åªåœ¨æ–‡ä»¶ä¸å­˜åœ¨æ—¶å†™å…¥è¡¨å¤´
                if not file_exists:
                    writer.writerow(csv_headers)
                
                # éå†æ¯å¤©çš„æ—¥ç¨‹æ•°æ®
                for day_index, day_data in enumerate(batch_daily_schedules):
                    date = day_data.get('date', '')
                    weekday = day_data.get('weekday_name', '')
                    weather = day_data.get('weather', '')
                    is_holiday = day_data.get('is_holiday', False)
                    holiday_name = day_data.get('holiday_name', '')
                    
                    # èŠ‚æ—¥ä¿¡æ¯å¤„ç†
                    holiday_info = holiday_name if is_holiday and holiday_name else "æ— "
                    
                    # æ ¹æ®æ—¥æœŸç¡®å®šå­£èŠ‚
                    season = self._get_season_from_date(date)
                    
                    daily_plan = day_data.get('daily_plan', '')
                    daily_summary = day_data.get('daily_summary', '')
                    
                    # æå–æ¯æ—¥æ¶‰åŠè§’è‰²ä¿¡æ¯
                    daily_involved_characters = day_data.get('daily_involved_characters', [])
                    daily_characters_info = ''
                    
                    # ä»æ—¶é—´æ®µä¸­è‡ªåŠ¨æå–è§’è‰²ä¿¡æ¯ï¼ˆå¦‚æœdaily_involved_charactersä¸ºç©ºï¼‰
                    if not daily_involved_characters:
                        time_slot_chars = set()
                        for slot in day_data.get('time_slots', []):
                            involved_chars = slot.get('involved_characters', [])
                            time_slot_chars.update(involved_chars)
                        daily_involved_characters = list(time_slot_chars)
                    
                    # ç”Ÿæˆè§’è‰²ç®€ä»‹ä¿¡æ¯ï¼ˆç®€åŒ–ç‰ˆï¼Œå› ä¸ºå·¥ä½œæµå†…éƒ¨æ²¡æœ‰å®Œæ•´è§’è‰²æ•°æ®ï¼‰
                    if daily_involved_characters:
                        daily_characters_info = 'ï¼›'.join([f"{char}-è§’è‰²ç®€ä»‹" for char in daily_involved_characters])
                    
                    # åˆå§‹åŒ–æ—¶é—´æ®µæ•°æ®
                    time_slots_data = {
                        'ä¸Šåˆ': '',
                        'ä¸­åˆ': '', 
                        'ä¸‹åˆ': '',
                        'æ™šä¸Š': '',
                        'å¤œé—´': ''
                    }
                    
                    # æå–æ—¶é—´æ®µæ•°æ®
                    time_slots = day_data.get('time_slots', [])
                    for slot in time_slots:
                        slot_name = slot.get('slot_name', '')
                        if slot_name in time_slots_data:
                            time_slots_data[slot_name] = slot.get('schedule_content', '')
                    
                    # æ‰¹æ¬¡æ€»ç»“ï¼šåªåœ¨ç¬¬ä¸€å¤©æ˜¾ç¤ºæ‰¹æ¬¡æ€»ç»“ï¼Œå…¶ä»–å¤©ä¸ºç©º
                    day_batch_summary = ""
                    if day_index == 0:  # ç¬¬ä¸€å¤©æ˜¾ç¤ºæ‰¹æ¬¡æ€»ç»“
                        day_batch_summary = batch_summary
                    
                    # æ„å»ºCSVè¡Œæ•°æ®
                    row_data = [
                        date,                          # æ—¥æœŸ
                        weekday,                       # æ˜ŸæœŸ
                        holiday_info,                  # èŠ‚æ—¥ä¿¡æ¯
                        season,                        # å­£èŠ‚
                        weather,                       # å¤©æ°”
                        cycle_theme,                   # ä¸»é¢˜
                        cycle_plan,                    # å‘¨æœŸè®¡åˆ’
                        day_batch_summary,             # æ‰¹æ¬¡æ€»ç»“
                        daily_plan,                    # æ¯æ—¥è®¡åˆ’
                        daily_summary,                 # æ¯æ—¥æ€»ç»“
                        ', '.join(daily_involved_characters),  # æ¶‰åŠè§’è‰²
                        daily_characters_info,         # è§’è‰²ç®€ä»‹
                        time_slots_data['ä¸Šåˆ'],        # ä¸Šåˆ
                        time_slots_data['ä¸­åˆ'],        # ä¸­åˆ
                        time_slots_data['ä¸‹åˆ'],        # ä¸‹åˆ
                        time_slots_data['æ™šä¸Š'],        # æ™šä¸Š
                        time_slots_data['å¤œé—´']         # å¤œé—´
                    ]
                    
                    writer.writerow(row_data)
            
            logger.info(f"âœ… æ‰¹æ¬¡CSV{'è¿½åŠ ' if file_exists else 'ä¿å­˜'}æˆåŠŸ: {csv_file_path}")
            logger.info(f"ğŸ“Š æœ¬æ¬¡æ·»åŠ : {len(batch_daily_schedules)}å¤©æ—¥ç¨‹æ•°æ®")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ‰¹æ¬¡CSVå¤±è´¥: å‘¨æœŸ{cycle_number}, æ‰¹æ¬¡{batch_number}, é”™è¯¯: {e}")
            import traceback
            logger.error(traceback.format_exc())
    
    async def _save_batch_json_to_txt(self, batch_data: Dict, cycle_number: int, batch_number: int, start_date: str, end_date: str):
        """ä¿å­˜æ¯3å¤©æ‰¹æ¬¡çš„JSONæ•°æ®åˆ°TXTæ–‡ä»¶ï¼ˆæ–¹ä¾¿é”™è¯¯æ—¶æ‰‹åŠ¨è§£æï¼‰"""
        try:
            from pathlib import Path
            from datetime import datetime
            import json
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = Path("workspace/batch_schedule_output_raw")
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # ä½¿ç”¨å›ºå®šæ–‡ä»¶åè¿›è¡Œå¢é‡ä¿å­˜
            txt_file_path = output_dir / "batch_json_data.txt"
            
            # æ„å»ºæ ¼å¼åŒ–çš„JSONå†…å®¹
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            separator = "=" * 80
            
            # ç¾åŒ–JSONæ ¼å¼
            formatted_json = json.dumps(batch_data, ensure_ascii=False, indent=2)
            
            formatted_content = f"""
{separator}
æ‰¹æ¬¡JSONæ•°æ®: å‘¨æœŸ{cycle_number}-æ‰¹æ¬¡{batch_number} | æ—¥æœŸèŒƒå›´: {start_date} è‡³ {end_date}
ä¿å­˜æ—¶é—´: {timestamp}
æ•°æ®å®Œæ•´æ€§: {len(batch_data.get('daily_schedules', []))}å¤©æ—¥ç¨‹, æ‰¹æ¬¡æ€»ç»“{len(batch_data.get('batch_summary', ''))}å­—ç¬¦
{separator}

{formatted_json}

{separator}
æ‰¹æ¬¡JSONç»“æŸ: å‘¨æœŸ{cycle_number}-æ‰¹æ¬¡{batch_number}
{separator}

"""
            
            # å¢é‡è¿½åŠ åˆ°æ–‡ä»¶
            with open(txt_file_path, 'a', encoding='utf-8') as f:
                f.write(formatted_content)
            
            logger.info(f"âœ… æ‰¹æ¬¡JSONæ•°æ®å·²ä¿å­˜åˆ°TXT: å‘¨æœŸ{cycle_number}-æ‰¹æ¬¡{batch_number}")
            logger.info(f"ğŸ“„ æ–‡ä»¶è·¯å¾„: {txt_file_path}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ‰¹æ¬¡JSONæ•°æ®åˆ°TXTå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
        
    async def _save_cycle_to_csv_immediately(self, schedule_data: Dict[str, Any], cycle_number: int):
        """å‘¨æœŸå®Œæˆåç«‹å³ä¿å­˜åˆ°CSVï¼ˆå¢é‡æ›´æ–°ï¼‰"""
        try:
            from pathlib import Path
            import csv
            import os
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = Path("workspace/batch_schedule_output")
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # ä½¿ç”¨å›ºå®šCSVæ–‡ä»¶åï¼Œä¾¿äºå¢é‡æ›´æ–°
            csv_file_path = output_dir / "batch_schedules.csv"
            
            # å®šä¹‰CSVåˆ—å¤´
            csv_headers = [
                "æ—¥æœŸ", "æ˜ŸæœŸ", "èŠ‚æ—¥ä¿¡æ¯", "å­£èŠ‚", "å¤©æ°”", "ä¸»é¢˜", 
                "å‘¨æœŸè®¡åˆ’", "æ‰¹æ¬¡æ€»ç»“", "æ¯æ—¥è®¡åˆ’", "æ¯æ—¥æ€»ç»“", "æ¶‰åŠè§’è‰²", "è§’è‰²ç®€ä»‹",
                "ä¸Šåˆ", "ä¸­åˆ", "ä¸‹åˆ", "æ™šä¸Š", "å¤œé—´"
            ]
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå†³å®šæ˜¯è¿½åŠ è¿˜æ˜¯åˆ›å»ºæ–°æ–‡ä»¶
            file_exists = csv_file_path.exists()
            write_mode = 'a' if file_exists else 'w'
            
            # è·å–å‘¨æœŸä¿¡æ¯
            cycle_info = schedule_data.get('cycle_info', {})
            cycle_theme = cycle_info.get('cycle_theme', '')
            cycle_plan = cycle_info.get('cycle_plan', f"å‘¨æœŸè®¡åˆ’ï¼š{cycle_theme}")  # ä½¿ç”¨è¯¦ç»†çš„å‘¨æœŸè®¡åˆ’
            daily_schedules = schedule_data.get('daily_schedules', [])
            
            # å†™å…¥CSVæ–‡ä»¶
            with open(csv_file_path, write_mode, encoding='utf-8', newline='') as csvfile:
                writer = csv.writer(csvfile)
                
                # åªåœ¨æ–‡ä»¶ä¸å­˜åœ¨æ—¶å†™å…¥è¡¨å¤´
                if not file_exists:
                    writer.writerow(csv_headers)
                
                # å¤„ç†æ‰¹æ¬¡æ€»ç»“ï¼šä»schedule_dataä¸­è·å–ï¼Œè€Œä¸æ˜¯è‡ªåŠ¨ç”Ÿæˆ
                cycle_summary = schedule_data.get('cycle_summary', '')
                
                # éå†æ¯å¤©çš„æ—¥ç¨‹æ•°æ®
                for day_index, day_data in enumerate(daily_schedules):
                    date = day_data.get('date', '')
                    weekday = day_data.get('weekday_name', '')
                    weather = day_data.get('weather', '')
                    is_holiday = day_data.get('is_holiday', False)
                    holiday_name = day_data.get('holiday_name', '')
                    
                    # èŠ‚æ—¥ä¿¡æ¯å¤„ç†
                    holiday_info = holiday_name if is_holiday and holiday_name else "æ— "
                    
                    # æ ¹æ®æ—¥æœŸç¡®å®šå­£èŠ‚
                    season = self._get_season_from_date(date)
                    
                    daily_plan = day_data.get('daily_plan', '')
                    daily_summary = day_data.get('daily_summary', '')  # æ¯æ—¥æ€»ç»“
                    
                    # æå–æ¯æ—¥æ¶‰åŠè§’è‰²ä¿¡æ¯
                    daily_involved_characters = day_data.get('daily_involved_characters', [])
                    daily_characters_info = day_data.get('daily_characters_info', '')
                    
                    # å¦‚æœæ²¡æœ‰æä¾›å­—ç¬¦ä¸²æ ¼å¼çš„è§’è‰²ä¿¡æ¯ï¼Œåˆ™è‡ªåŠ¨ç”Ÿæˆ
                    if not daily_characters_info and daily_involved_characters:
                        # ä»è§’è‰²æ•°æ®ä¸­è·å–ç®€ä»‹ï¼ˆè¿™é‡Œéœ€è¦ä¼ å…¥è§’è‰²æ•°æ®ï¼‰
                        char_infos = []
                        for char_name in daily_involved_characters:
                            char_infos.append(f"{char_name}-ç®€ä»‹å¾…è¡¥å……")  # ç®€åŒ–å¤„ç†
                        daily_characters_info = 'ï¼›'.join(char_infos)
                    
                    # åˆå§‹åŒ–æ—¶é—´æ®µæ•°æ®
                    time_slots_data = {
                        'ä¸Šåˆ': '',
                        'ä¸­åˆ': '', 
                        'ä¸‹åˆ': '',
                        'æ™šä¸Š': '',
                        'å¤œé—´': ''
                    }
                    
                    # æå–æ—¶é—´æ®µæ•°æ®
                    time_slots = day_data.get('time_slots', [])
                    for slot in time_slots:
                        slot_name = slot.get('slot_name', '')
                        if slot_name in time_slots_data:
                            time_slots_data[slot_name] = slot.get('schedule_content', '')
                    
                    # æ‰¹æ¬¡æ€»ç»“ï¼šåªåœ¨å‘¨æœŸçš„ç¬¬ä¸€å¤©æ˜¾ç¤ºå‘¨æœŸæ€»ç»“ï¼Œå…¶ä»–å¤©ä¸ºç©º
                    day_cycle_summary = ""
                    if day_index == 0:  # å‘¨æœŸçš„ç¬¬ä¸€å¤©æ˜¾ç¤ºå‘¨æœŸæ€»ç»“
                        day_cycle_summary = cycle_summary
                    
                    # æ„å»ºCSVè¡Œæ•°æ®
                    row_data = [
                        date,                          # æ—¥æœŸ
                        weekday,                       # æ˜ŸæœŸ
                        holiday_info,                  # èŠ‚æ—¥ä¿¡æ¯
                        season,                        # å­£èŠ‚
                        weather,                       # å¤©æ°”
                        cycle_theme,                   # ä¸»é¢˜
                        cycle_plan,                    # å‘¨æœŸè®¡åˆ’
                        day_cycle_summary,             # æ‰¹æ¬¡æ€»ç»“
                        daily_plan,                    # æ¯æ—¥è®¡åˆ’
                        daily_summary,                 # æ¯æ—¥æ€»ç»“
                        ', '.join(daily_involved_characters),  # æ¶‰åŠè§’è‰²
                        daily_characters_info,         # è§’è‰²ç®€ä»‹
                        time_slots_data['ä¸Šåˆ'],        # ä¸Šåˆ
                        time_slots_data['ä¸­åˆ'],        # ä¸­åˆ
                        time_slots_data['ä¸‹åˆ'],        # ä¸‹åˆ
                        time_slots_data['æ™šä¸Š'],        # æ™šä¸Š
                        time_slots_data['å¤œé—´']         # å¤œé—´
                    ]
                    
                    writer.writerow(row_data)
            
            logger.info(f"å‘¨æœŸ {cycle_number} CSVæ•°æ®å·²{'è¿½åŠ åˆ°' if file_exists else 'ä¿å­˜ä¸ºæ–°'}æ–‡ä»¶: {csv_file_path}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜å‘¨æœŸ {cycle_number} CSVæ–‡ä»¶å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
    
    async def _save_raw_response_to_txt(self, raw_content: str, cycle_number: int, batch_number: int, start_date: str, end_date: str):
        """å¢é‡ä¿å­˜LLMåŸå§‹å›å¤åˆ°TXTæ–‡ä»¶ï¼Œä¿ç•™æ ¼å¼ä¾¿äºåæœŸè§£æ"""
        try:
            from pathlib import Path
            from datetime import datetime
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = Path("workspace/batch_schedule_output")
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # ä½¿ç”¨å›ºå®šæ–‡ä»¶åè¿›è¡Œå¢é‡ä¿å­˜
            txt_file_path = output_dir / "raw_llm_responses.txt"
            
            # æ„å»ºæ ¼å¼åŒ–çš„å›å¤å†…å®¹
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            separator = "=" * 80
            
            formatted_content = f"""
{separator}
æ‰¹æ¬¡ä¿¡æ¯: å‘¨æœŸ{cycle_number}-æ‰¹æ¬¡{batch_number} | æ—¥æœŸèŒƒå›´: {start_date} è‡³ {end_date}
ä¿å­˜æ—¶é—´: {timestamp}
{separator}

{raw_content}

{separator}
æ‰¹æ¬¡ç»“æŸ: å‘¨æœŸ{cycle_number}-æ‰¹æ¬¡{batch_number}
{separator}

"""
            
            # å¢é‡è¿½åŠ åˆ°æ–‡ä»¶
            with open(txt_file_path, 'a', encoding='utf-8') as f:
                f.write(formatted_content)
            
            logger.info(f"âœ… åŸå§‹å›å¤å·²ä¿å­˜åˆ°TXT: å‘¨æœŸ{cycle_number}-æ‰¹æ¬¡{batch_number}, å†…å®¹é•¿åº¦: {len(raw_content)} å­—ç¬¦")
            
        except Exception as e:
            logger.error(f"ä¿å­˜åŸå§‹å›å¤åˆ°TXTå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
    
    def _get_season_from_date(self, date_str: str) -> str:
        """æ ¹æ®æ—¥æœŸç¡®å®šå­£èŠ‚"""
        try:
            from datetime import datetime
            date = datetime.strptime(date_str, '%Y-%m-%d')
            month = date.month
            
            if month in [12, 1, 2]:
                return 'å†¬å­£'
            elif month in [3, 4, 5]:
                return 'æ˜¥å­£'
            elif month in [6, 7, 8]:
                return 'å¤å­£'
            elif month in [9, 10, 11]:
                return 'ç§‹å­£'
            else:
                return 'æœªçŸ¥'
        except:
            return 'æœªçŸ¥'
    
    async def _get_recent_batch_summaries(self, count: int, before_date: str) -> List[str]:
        """è·å–æœ€è¿‘4ä¸ªæ‰¹æ¬¡çš„summaryä½œä¸ºå†å²è®°å½• - è·¨å‘¨æœŸè·¨æ‰¹æ¬¡è®°å¿†"""
        try:
            import csv
            import os
            from pathlib import Path
            from datetime import datetime
             
            print(f"ğŸ” DEBUG: å¼€å§‹è·å–å†å²æ‰¹æ¬¡æ€»ç»“ï¼Œbefore_date={before_date}")
            
            # ä»CSVæ–‡ä»¶è¯»å–æœ€è¿‘çš„æ‰¹æ¬¡æ€»ç»“
            csv_file_path = Path("workspace/batch_schedule_output/batch_schedules.csv")
            print(f"ğŸ” DEBUG: æŸ¥æ‰¾CSVæ–‡ä»¶: {csv_file_path}")
            logger.info(f"ğŸ” æŸ¥æ‰¾CSVæ–‡ä»¶: {csv_file_path}")
            
            if not csv_file_path.exists():
                print("âŒ DEBUG: CSVæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºå†å²è®°å½•")
                logger.info("âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºå†å²è®°å½•")
                return []
            
            print(f"âœ… DEBUG: CSVæ–‡ä»¶å­˜åœ¨ï¼Œæ–‡ä»¶å¤§å°: {csv_file_path.stat().st_size} å­—èŠ‚")
            logger.info(f"âœ… CSVæ–‡ä»¶å­˜åœ¨ï¼Œæ–‡ä»¶å¤§å°: {csv_file_path.stat().st_size} å­—èŠ‚")
            
            # è§£æbefore_dateä¸ºdatetimeå¯¹è±¡
            try:
                before_dt = datetime.strptime(before_date, '%Y-%m-%d')
                print(f"ğŸ” DEBUG: è§£æbefore_dateæˆåŠŸ: {before_dt}")
            except Exception as date_error:
                logger.error(f"æ—¥æœŸè§£æå¤±è´¥: {date_error}")
                print(f"âŒ DEBUG: æ—¥æœŸè§£æå¤±è´¥: {date_error}")
                return []
            
            # è¯»å–CSVæ–‡ä»¶å¹¶æ”¶é›†æ‰¹æ¬¡æ€»ç»“
            batch_summaries = []
            unique_summaries = set()  # é¿å…é‡å¤
            
            with open(csv_file_path, 'r', encoding='utf-8') as f:
                csv_reader = csv.DictReader(f)
                
                for row in csv_reader:
                    try:
                        # è·å–è¡Œæ•°æ®
                        row_date_str = row.get('æ—¥æœŸ', '').strip()
                        batch_summary = row.get('æ‰¹æ¬¡æ€»ç»“', '').strip()
                        
                        # è·³è¿‡ç©ºçš„æ—¥æœŸæˆ–æ€»ç»“
                        if not row_date_str or not batch_summary:
                            continue
                        
                        # è§£æè¡Œæ—¥æœŸ
                        row_date = datetime.strptime(row_date_str, '%Y-%m-%d')
                        
                        # åªè€ƒè™‘before_dateä¹‹å‰çš„è®°å½•
                        if row_date >= before_dt:
                            continue
                        
                        # é¿å…é‡å¤çš„æ€»ç»“ï¼ˆåŒä¸€ä¸ªæ‰¹æ¬¡ä¼šæœ‰å¤šè¡Œç›¸åŒçš„æ€»ç»“ï¼‰
                        if batch_summary in unique_summaries:
                            continue
                        
                        unique_summaries.add(batch_summary)
                        batch_summaries.append({
                            'date': row_date,
                            'summary': batch_summary
                        })
                        
                    except Exception as row_error:
                        # è·³è¿‡æœ‰é—®é¢˜çš„è¡Œ
                        continue
            
            # æŒ‰æ—¥æœŸé™åºæ’åºï¼Œè·å–æœ€è¿‘çš„countä¸ªæ€»ç»“
            batch_summaries.sort(key=lambda x: x['date'], reverse=True)
            recent_summaries = batch_summaries[:count]
            
            # æå–æ€»ç»“æ–‡æœ¬
            summary_texts = [item['summary'] for item in recent_summaries]
            
            print(f"âœ… DEBUG: æˆåŠŸè·å– {len(summary_texts)} ä¸ªå†å²æ‰¹æ¬¡æ€»ç»“")
            logger.info(f"âœ… æˆåŠŸè·å– {len(summary_texts)} ä¸ªå†å²æ‰¹æ¬¡æ€»ç»“")
            
            # æ‰“å°æ€»ç»“é¢„è§ˆ
            for i, summary in enumerate(summary_texts):
                preview = summary[:100] + "..." if len(summary) > 100 else summary
                print(f"  ğŸ“ æ€»ç»“ {i+1}: {preview}")
                logger.info(f"  ğŸ“ æ€»ç»“ {i+1}: {preview}")
            
            return summary_texts
            
        except Exception as e:
            print(f"âŒ DEBUG: è·å–å†å²æ‰¹æ¬¡æ€»ç»“å¤±è´¥: {e}")
            logger.error(f"è·å–å†å²æ‰¹æ¬¡æ€»ç»“å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    async def _generate_cycle_summary(self, cycle_info: Dict, daily_schedules: List[Dict], llm, workflow_chat) -> str:
        """ç”Ÿæˆå‘¨æœŸæ€»ç»“"""
        try:
            if workflow_chat:
                await workflow_chat.add_node_message(
                    "æ—¥ç¨‹ç”Ÿæˆ",
                    "æ­£åœ¨ç”Ÿæˆå‘¨æœŸæ€»ç»“...",
                    "progress"
                )
            
            # æå–å‘¨æœŸå…³é”®ä¿¡æ¯
            cycle_theme = cycle_info.get('cycle_theme', '')
            objectives = cycle_info.get('main_objectives', [])
            focus_characters = cycle_info.get('focus_characters', [])
            
            # ç»Ÿè®¡å„è§’è‰²å‡ºç°æ¬¡æ•°
            character_stats = {}
            location_stats = {}
            
            for day in daily_schedules:
                for slot in day.get('time_slots', []):
                    # ç»Ÿè®¡è§’è‰²
                    chars = slot.get('involved_characters', [])
                    for char in chars:
                        character_stats[char] = character_stats.get(char, 0) + 1
                    
                    # ç»Ÿè®¡åœ°ç‚¹
                    location = slot.get('location', '')
                    if location:
                        location_stats[location] = location_stats.get(location, 0) + 1
            
            # æ„å»ºæ€»ç»“æç¤ºè¯ï¼šsystemçº¯æŒ‡ä»¤ï¼ŒuseråŠ¨æ€æ•°æ®
            system_summary_instructions = """è¯·ç”Ÿæˆä¸€ä¸ªç¬¬ä¸‰äººç§°çš„å‘¨æœŸæ€»ç»“ï¼Œé‡ç‚¹å…³æ³¨ï¼š
1. å‘¨æœŸä¸»é¢˜çš„ä½“ç°å’Œç›®æ ‡è¾¾æˆæƒ…å†µ
2. é‡ç‚¹è§’è‰²å…³ç³»çš„å‘å±•å˜åŒ–
3. ä¸»è¦æ´»åŠ¨å’Œé‡è¦äº‹ä»¶
4. ç©†æ˜­çš„é¥®é£Ÿç»†èŠ‚
5. ä¸ºä¸‹ä¸ªå‘¨æœŸçš„é“ºå«

è¦æ±‚ï¼šç®€æ´æ˜äº†ï¼Œçªå‡ºé‡ç‚¹ï¼Œ400å­—ä»¥å†…ã€‚ä»…è¾“å‡ºæ€»ç»“æ–‡æœ¬ï¼Œä¸æ·»åŠ é¢å¤–è§£é‡Šæˆ–å¤šä½™å†…å®¹ã€‚"""

            user_summary_dynamic = f"""æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ï¼Œä¸ºè¿™ä¸ªå‘¨æœŸç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ€»ç»“ï¼ˆ300å­—ä»¥å†…ï¼‰ï¼š

## å‘¨æœŸä¿¡æ¯
- ä¸»é¢˜ï¼š{cycle_theme}
- ç›®æ ‡ï¼š{', '.join(objectives)}
- é‡ç‚¹è§’è‰²ï¼š{', '.join(focus_characters)}
- å®é™…å¤©æ•°ï¼š{len(daily_schedules)}å¤©

## è§’è‰²äº’åŠ¨ç»Ÿè®¡
{chr(10).join([f"- {char}: {count}æ¬¡äº’åŠ¨" for char, count in sorted(character_stats.items(), key=lambda x: x[1], reverse=True)[:5]])}

## åœ°ç‚¹æ´»åŠ¨ç»Ÿè®¡  
{chr(10).join([f"- {loc}: {count}æ¬¡" for loc, count in sorted(location_stats.items(), key=lambda x: x[1], reverse=True)[:5]])}"""
            
            # è°ƒç”¨LLMç”Ÿæˆæ€»ç»“ï¼ˆsystem çº¯æŒ‡ä»¤ + user åŠ¨æ€èµ„æ–™ï¼‰
            from core.types import Message, MessageRole
            messages = [
                Message(role=MessageRole.SYSTEM, content=system_summary_instructions),
                Message(role=MessageRole.USER, content=user_summary_dynamic)
            ]
            
            summary_content = ""
            async for chunk_data in llm.stream_generate(messages, mode="normal", return_dict=True):
                summary_content += chunk_data.get("content", "")
            
            # æ¸…ç†æ€»ç»“å†…å®¹
            summary_content = summary_content.strip()
            if len(summary_content) > 500:
                summary_content = summary_content[:500] + "..."
            
            logger.info(f"å‘¨æœŸæ€»ç»“ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(summary_content)} å­—ç¬¦")
            return summary_content
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå‘¨æœŸæ€»ç»“å¤±è´¥: {e}")
            return f"å‘¨æœŸ{cycle_info.get('cycle_number', '')}å®Œæˆï¼Œå…±{len(daily_schedules)}å¤©ï¼Œä¸»é¢˜ï¼š{cycle_info.get('cycle_theme', '')}ã€‚"
        
    def _extract_json_from_content(self, content: str) -> str:
        """ä»ç”Ÿæˆå†…å®¹ä¸­æå–JSONéƒ¨åˆ† - ä¿®å¤å®Œæ•´JSONæå–"""
        import re
        import json

        
        logger.info(f"ğŸ” å¼€å§‹æå–JSONï¼ŒåŸå§‹å†…å®¹é•¿åº¦: {len(content)}")
        
        # ä¼˜å…ˆæŸ¥æ‰¾```json...```ä»£ç å—
        json_pattern = r'```json\s*(.*?)\s*```'
        matches = re.findall(json_pattern, content, re.DOTALL | re.IGNORECASE)
        
        for match in matches:
            extracted_json = match.strip()
            if self._is_valid_json(extracted_json):
                logger.info(f"âœ… ä»```json```ä»£ç å—æå–æœ‰æ•ˆJSONï¼Œé•¿åº¦: {len(extracted_json)}")
                return extracted_json

        # æ–¹æ³•2: æŸ¥æ‰¾```...```ä»£ç å—ï¼ˆä¸ä¸€å®šæ ‡æ³¨jsonï¼‰
        code_pattern = r'```[a-zA-Z]*\s*(.*?)\s*```'
        code_matches = re.findall(code_pattern, content, re.DOTALL | re.IGNORECASE)
        
        for match in code_matches:
            extracted = match.strip()
            if extracted.startswith('{') and self._is_valid_json(extracted):
                logger.info(f"âœ… ä»ä»£ç å—æå–æœ‰æ•ˆJSONï¼Œé•¿åº¦: {len(extracted)}")
                return extracted
        
        # æ–¹æ³•3: ä½¿ç”¨æ‹¬å·åŒ¹é…è®¡æ•°æå–å®Œæ•´JSON
        def extract_complete_json(text):
            start_pos = text.find('{')
            if start_pos == -1:
                return None
            
            brace_count = 0
            in_string = False
            escape_next = False
            
            for i, char in enumerate(text[start_pos:], start_pos):
                if escape_next:
                    escape_next = False
                    continue
                    
                if char == '\\' and in_string:
                    escape_next = True
                    continue
                    
                if char == '"' and not escape_next:
                    in_string = not in_string
                    continue
                    
                if not in_string:
                    if char == '{':
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            return text[start_pos:i+1]
            
            return None
        
        complete_json = extract_complete_json(content)
        if complete_json and self._is_valid_json(complete_json):
            logger.info(f"âœ… ä½¿ç”¨æ‹¬å·åŒ¹é…æå–æœ‰æ•ˆJSONï¼Œé•¿åº¦: {len(complete_json)}")
            return complete_json.strip()       
        # æ–¹æ³•4: å¤šé‡æ­£åˆ™åŒ¹é…åéªŒè¯
        json_patterns = [
            r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}',  # ç®€å•åµŒå¥—
            r'\{.*?\}',  # è´ªå©ªåŒ¹é…
            r'\{.*\}'    # æœ€è´ªå©ªåŒ¹é…
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, content, re.DOTALL)
            if matches:
                # æŒ‰é•¿åº¦æ’åºï¼Œä¼˜å…ˆå°è¯•æœ€é•¿çš„åŒ¹é…
                sorted_matches = sorted(matches, key=len, reverse=True)
                for match in sorted_matches:
                    if self._is_valid_json(match):
                        logger.info(f"âœ… æ­£åˆ™æ¨¡å¼åŒ¹é…åˆ°æœ‰æ•ˆJSONï¼Œé•¿åº¦: {len(match)}")
                        return match.strip()
        logger.warning("âŒ æ‰€æœ‰æ–¹æ³•éƒ½æœªèƒ½æå–æœ‰æ•ˆJSONï¼Œè¿”å›åŸå†…å®¹")

    def _is_valid_json(self, json_str: str) -> bool:
        """éªŒè¯JSONå­—ç¬¦ä¸²æ˜¯å¦æœ‰æ•ˆ"""
        try:
            json.loads(json_str)
            return True
        except (json.JSONDecodeError, ValueError):
            return False
# æ•°æ®åº“ä¿å­˜èŠ‚ç‚¹å·²åˆ é™¤ï¼Œæ”¹ä¸ºåœ¨batch_schedule_generator.pyä¸­ç›´æ¥ä¿å­˜CSV


async def main():
    """æœ¬åœ°ä¸»å‡½æ•° - ç›´æ¥æ‰§è¡Œå·¥ä½œæµè¿›è¡Œå¤§æ‰¹æ¬¡æ—¥ç¨‹ç”Ÿæˆ"""
    import random
    import argparse
    from datetime import datetime, timedelta
    from pathlib import Path
    from dotenv import load_dotenv
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # å‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='æ—¥ç¨‹ç”Ÿæˆå·¥ä½œæµ - æœ¬åœ°æ‰¹é‡æ‰§è¡Œ')
    parser.add_argument('--start-date', default='2025-07-14', help='å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--mega-batches', type=int, default=2, help='å¤§æ‰¹æ¬¡æ•°é‡')
    parser.add_argument('--days-per-batch', type=int, default=9, help='æ¯å¤§æ‰¹æ¬¡å¤©æ•°')
    
    args = parser.parse_args()
    
    print(f"ğŸš€ æ—¥ç¨‹ç”Ÿæˆå·¥ä½œæµæœ¬åœ°æ‰§è¡Œå¯åŠ¨")
    print(f"ğŸ“… å¼€å§‹æ—¥æœŸ: {args.start_date}")
    print(f"ğŸ”¢ å¤§æ‰¹æ¬¡æ•°é‡: {args.mega_batches}")
    print(f"ğŸ“Š æ¯æ‰¹æ¬¡å¤©æ•°: {args.days_per_batch}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: workspace/batch_schedule_output/")
    
    try:
        # åˆå§‹åŒ–LLM
        from llm.base import LLMFactory
        from core.types import LLMConfig
        import os
        
        llm_config = LLMConfig(
            provider="doubao",
            api_key=os.getenv('DOUBAO_API_KEY', 'b633a622-b5d0-4f16-a8a9-616239cf15d1'),
            model_name=os.getenv('DOUBAO_MODEL_DEEPSEEKR1', 'ep-20250221154107-c4qc7'),
            temperature=0.7,
            max_tokens=16384
        )
        
        llm_factory = LLMFactory()
        llm = llm_factory.create(llm_config)
        
        # åˆ›å»ºå·¥ä½œæµå®ä¾‹
        workflow = ScheduleWorkflow(llm=llm)
        
        print(f"âœ… LLMå’Œå·¥ä½œæµåˆå§‹åŒ–æˆåŠŸ")
        
        # åˆå§‹åŒ–çŠ¶æ€
        current_date = datetime.strptime(args.start_date, '%Y-%m-%d')
        success_count = 0
        failed_count = 0
        
        # åˆ›å»ºç®€åŒ–çš„å·¥ä½œæµèŠå¤©æ¥å£
        class LocalWorkflowChat:
            def __init__(self):
                self.current_node = ""
            
            async def add_node_message(self, node_name: str, message: str, status: str):
                # ç®€åŒ–è¾“å‡ºï¼Œåªæ˜¾ç¤ºé‡è¦ä¿¡æ¯
                clean_message = message.replace('âœ…', '[å®Œæˆ]').replace('âŒ', '[å¤±è´¥]').replace('âš ï¸', '[è­¦å‘Š]').replace('ğŸ”„', '[è¿›è¡Œä¸­]')
                if status in ['success', 'error', 'warning']:
                    print(f"  [{node_name}] {clean_message}")
            
            def _create_workflow_progress(self):
                return ""
        
        # å¾ªç¯æ‰§è¡Œå¤§æ‰¹æ¬¡
        for mega_batch_num in range(1, args.mega_batches + 1):
            print(f"\n{'=' * 80}")
            print(f"ğŸ¯ æ­£åœ¨æ‰§è¡Œç¬¬ {mega_batch_num}/{args.mega_batches} ä¸ªå¤§æ‰¹æ¬¡")
            print(f"ğŸ“… å½“å‰å¼€å§‹æ—¥æœŸ: {current_date.strftime('%Y-%m-%d')}")
            print(f"{'='*80}")
            
            try:
                # è®¡ç®—å¤§æ‰¹æ¬¡çš„ç»“æŸæ—¥æœŸ
                end_date = current_date + timedelta(days=args.days_per_batch - 1)
                
                # è·å–å¯ç”¨è§’è‰²å’Œåœ°ç‚¹
                available_characters = list(workflow.characters_data.get("è§’è‰²åˆ—è¡¨", {}).keys())
                if 'ç©†æ˜­' in available_characters:
                    available_characters.remove('ç©†æ˜­')
                
                available_locations = []
                for district_info in workflow.locations_data.get("districts", {}).values():
                    for loc_info in district_info.get("locations", {}).values():
                        available_locations.append(loc_info.get('name', ''))
                
                # éšæœºé€‰æ‹©è§’è‰²å’Œåœ°ç‚¹
                selected_characters = random.sample(available_characters, min(random.randint(4, 8), len(available_characters)))
                selected_locations = random.sample(available_locations, min(random.randint(5, 10), len(available_locations)))
                
                # æ„å»ºé…ç½®
                config = {
                    'protagonist': 'ç©†æ˜­',
                    'schedule_type': 'mega_batch',
                    'start_date': current_date.strftime('%Y-%m-%d'),
                    'end_date': end_date.strftime('%Y-%m-%d'),
                    'total_days': args.days_per_batch,
                    'selected_characters': selected_characters,
                    'selected_locations': selected_locations,
                    'selected_stories': [],
                    'time_slots_config': {
                        'å¤œé—´': {'start': '23:00', 'end': '06:00'},
                        'ä¸Šåˆ': {'start': '06:00', 'end': '11:00'},
                        'ä¸­åˆ': {'start': '11:00', 'end': '14:00'},
                        'ä¸‹åˆ': {'start': '14:00', 'end': '18:00'},
                        'æ™šä¸Š': {'start': '18:00', 'end': '23:00'}
                    },
                    'character_distribution': 'balanced',
                    'story_integration': 'moderate',
                    'include_holidays': True,
                    'include_lunar': True,
                    'mood_variety': True,
                    'location_variety': True,
                    'enable_cycle_summary': True,
                    'previous_batch_summary': ""  # TODO: å¯ä»¥ä»å†å²ä¸­è·å–
                }
                
                print(f"  ğŸ“‹ é…ç½®ä¿¡æ¯:")
                print(f"    æ—¥æœŸèŒƒå›´: {config['start_date']} - {config['end_date']} ({config['total_days']}å¤©)")
                print(f"    è§’è‰²æ•°é‡: {len(selected_characters)} ({', '.join(selected_characters[:3])}...)")
                print(f"    åœ°ç‚¹æ•°é‡: {len(selected_locations)} ({', '.join(selected_locations[:3])}...)")
                
                # åˆ›å»ºå·¥ä½œæµèŠå¤©æ¥å£
                workflow_chat = LocalWorkflowChat()
                
                # æ‰§è¡Œå·¥ä½œæµ
                print(f"  ğŸš€ å¼€å§‹æ‰§è¡Œå·¥ä½œæµ...")
                
                progress_count = 0
                async for stream_event in workflow.execute_workflow_stream(config, workflow_chat):
                    progress_count += 1
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ç»ˆå®Œæˆäº‹ä»¶
                    if isinstance(stream_event, tuple) and len(stream_event) >= 4:
                        html, content, message, is_complete = stream_event
                        if "æ‰§è¡Œå®Œæˆ" in message or "ç”Ÿæˆå®Œæˆ" in message:
                            print(f"    âœ… æ£€æµ‹åˆ°å®Œæˆä¿¡å·: {message}")
                
                print(f"  ğŸ“Š å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼Œå…±æ”¶åˆ° {progress_count} æ¬¡äº‹ä»¶")
                
                # ç­‰å¾…æ•°æ®åº“å†™å…¥
                import time
                time.sleep(2)
                
                # æ›´æ–°çŠ¶æ€
                current_date = end_date + timedelta(days=1)
                success_count += 1
                
                print(f"  ğŸ‰ å¤§æ‰¹æ¬¡ {mega_batch_num} æ‰§è¡ŒæˆåŠŸ")
                print(f"    ğŸ“… ä¸‹æ‰¹æ¬¡å¼€å§‹æ—¥æœŸ: {current_date.strftime('%Y-%m-%d')}")
                
            except Exception as e:
                failed_count += 1
                print(f"  ğŸ’¥ å¤§æ‰¹æ¬¡ {mega_batch_num} æ‰§è¡Œå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                
                # å¤±è´¥æ—¶ä¹Ÿè¦æ¨è¿›æ—¥æœŸï¼Œé¿å…å¡ä½
                current_date += timedelta(days=args.days_per_batch)
                print(f"    â­ï¸ è·³è¿‡åˆ°ä¸‹æ‰¹æ¬¡å¼€å§‹æ—¥æœŸ: {current_date.strftime('%Y-%m-%d')}")
            
            # æ‰¹æ¬¡é—´ä¼‘æ¯
            print(f"  â¸ï¸ å¤§æ‰¹æ¬¡é—´ä¼‘æ¯ 3 ç§’...")
            import asyncio
            await asyncio.sleep(3)
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        print(f"\nğŸ æ‰€æœ‰å¤§æ‰¹æ¬¡æ‰§è¡Œå®Œæˆ!")
        print(f"âœ… æˆåŠŸ: {success_count}/{args.mega_batches}")
        print(f"âŒ å¤±è´¥: {failed_count}/{args.mega_batches}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {success_count/args.mega_batches*100:.1f}%")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: workspace/batch_schedule_output/batch_schedules.csv")
        print(f"ğŸ“… æœ€ç»ˆæ—¥æœŸ: {current_date.strftime('%Y-%m-%d')}")
        
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œç¨‹åºé€€å‡º")
    except Exception as e:
        print(f"ğŸ’¥ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print(f"\nğŸ‘‹ ç¨‹åºé€€å‡º")


if __name__ == "__main__":
    """æœ¬åœ°æ‰§è¡Œå…¥å£"""
    import asyncio
    import sys
    
    # è®¾ç½®Windowså¼‚æ­¥äº‹ä»¶å¾ªç¯ç­–ç•¥
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    
    # æ·»åŠ é¡¹ç›®è·¯å¾„
    import os
    from pathlib import Path
    
    current_dir = Path(__file__).parent
    project_root = current_dir.parent.parent  # å›åˆ°é¡¹ç›®æ ¹ç›®å½•
    sys.path.insert(0, str(project_root))
    sys.path.insert(0, str(current_dir.parent))  # srcç›®å½•
    
    asyncio.run(main())