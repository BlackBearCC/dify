"""æ•…äº‹ç”Ÿæˆå™¨ - åŸºäºå›¾ç‰‡æè¿°ç»“æœç”Ÿæˆæ•…äº‹
è¯»å–å›¾ç‰‡è¯†åˆ«ç»“æœï¼Œç»“åˆè§’è‰²äººè®¾ç”Ÿæˆæ•…äº‹ï¼Œå¹¶ä¿å­˜ä¸ºå¸¦æ•…äº‹åç¼€çš„CSVæ–‡ä»¶
"""

import os
import csv
import json
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional

import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from core.base import BaseNode
from core.types import Message, MessageRole

logger = logging.getLogger(__name__)

class StoryGenerationNode(BaseNode):
    """æ•…äº‹ç”ŸæˆèŠ‚ç‚¹ - è¯»å–å›¾ç‰‡æè¿°ç»“æœï¼Œç»“åˆè§’è‰²äººè®¾ç”Ÿæˆæ•…äº‹"""
    
    def __init__(self):
        super().__init__(name="story_generation", stream=True)
        self.protagonist_data = ""
        self.locations_data = {}
        self._load_protagonist_data()
        self._load_locations_data()
    
    def _load_protagonist_data(self):
        """åŠ è½½ä¸»è§’æ–¹çŸ¥è¡¡çš„è¯¦ç»†äººè®¾"""
        try:
            protagonist_path = os.path.join(os.path.dirname(__file__), '../../config/åŸºç¡€äººè®¾_æ–¹çŸ¥è¡¡100.txt')
            if os.path.exists(protagonist_path):
                with open(protagonist_path, 'r', encoding='utf-8') as f:
                    self.protagonist_data = f.read()
                    logger.info(f"æˆåŠŸåŠ è½½ä¸»è§’äººè®¾ï¼Œå†…å®¹é•¿åº¦: {len(self.protagonist_data)} å­—ç¬¦")
            else:
                logger.warning("ä¸»è§’äººè®¾æ–‡ä»¶ä¸å­˜åœ¨")
                
        except Exception as e:
            logger.error(f"åŠ è½½ä¸»è§’äººè®¾å¤±è´¥: {e}")
            
    def _load_locations_data(self):
        """åŠ è½½äº‘æ¢å¸‚åœ°ç‚¹æ•°æ®"""
        try:
            locations_path = os.path.join(os.path.dirname(__file__), '../../config/yunhub_locations.json')
            if os.path.exists(locations_path):
                with open(locations_path, 'r', encoding='utf-8') as f:
                    self.locations_data = json.load(f)
                    districts_count = len(self.locations_data.get("districts", {}))
                    logger.info(f"æˆåŠŸåŠ è½½äº‘æ¢å¸‚åœ°ç‚¹æ•°æ®ï¼ŒåŒ…å« {districts_count} ä¸ªåŒºåŸŸ")
            else:
                logger.warning("äº‘æ¢å¸‚åœ°ç‚¹æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
                
        except Exception as e:
            logger.error(f"åŠ è½½äº‘æ¢å¸‚åœ°ç‚¹æ•°æ®å¤±è´¥: {e}")
    
    async def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæ•…äº‹ç”ŸæˆèŠ‚ç‚¹"""
        final_result = None
        async for result in self.execute_stream(input_data):
            final_result = result
        return final_result or input_data
    
    async def execute_stream(self, input_data: Dict[str, Any]):
        """æµå¼æ‰§è¡Œæ•…äº‹ç”ŸæˆèŠ‚ç‚¹ - æ”¯æŒæ‰¹é‡å¤„ç†å’Œå¢é‡ä¿å­˜"""
        print("ğŸ“ å¼€å§‹ç”Ÿæˆæ•…äº‹...")
        
        workflow_chat = input_data.get('workflow_chat')
        llm = input_data.get('llm')
        recognition_results = input_data.get('recognition_results', [])
        csv_save_result = input_data.get('csv_save_result', {})
        csv_file_path = csv_save_result.get('file_path', '')
        batch_size = input_data.get('batch_size', 1)  # é»˜è®¤æ¯æ‰¹å¤„ç†5ä¸ªå›¾ç‰‡
        
        if not recognition_results:
            # æ²¡æœ‰è¯†åˆ«ç»“æœï¼Œæ— æ³•ç”Ÿæˆæ•…äº‹
            if workflow_chat:
                await workflow_chat.add_node_message(
                    "æ•…äº‹ç”Ÿæˆ",
                    "âš ï¸ æ²¡æœ‰å›¾ç‰‡è¯†åˆ«ç»“æœï¼Œæ— æ³•ç”Ÿæˆæ•…äº‹",
                    "warning"
                )
            
            output_data = input_data.copy()
            output_data['story_results'] = []
            output_data['story_save_result'] = {
                'success': False,
                'message': "æ²¡æœ‰å›¾ç‰‡è¯†åˆ«ç»“æœï¼Œæ— æ³•ç”Ÿæˆæ•…äº‹"
            }
            yield output_data
            return
        
        if not llm:
            # LLMæœªåˆå§‹åŒ–ï¼Œæ— æ³•ç”Ÿæˆæ•…äº‹
            if workflow_chat:
                await workflow_chat.add_node_message(
                    "æ•…äº‹ç”Ÿæˆ",
                    "âš ï¸ LLMæœªåˆå§‹åŒ–ï¼Œæ— æ³•ç”Ÿæˆæ•…äº‹",
                    "warning"
                )
            
            output_data = input_data.copy()
            output_data['story_results'] = []
            output_data['story_save_result'] = {
                'success': False,
                'message': "LLMæœªåˆå§‹åŒ–ï¼Œæ— æ³•ç”Ÿæˆæ•…äº‹"
            }
            yield output_data
            return
        
        if workflow_chat:
            await workflow_chat.add_node_message(
                "æ•…äº‹ç”Ÿæˆ",
                f"æ­£åœ¨ä¸º{len(recognition_results)}å¼ å›¾ç‰‡ç”Ÿæˆæ•…äº‹...",
                "progress"
            )
        
        # åˆ†æ‰¹å¤„ç†å›¾ç‰‡
        total_results = len(recognition_results)
        total_batches = (total_results + batch_size - 1) // batch_size
        all_story_results = []
        
        if workflow_chat:
            await workflow_chat.add_node_message(
                "æ•…äº‹ç”Ÿæˆ",
                f"å°†åˆ†{total_batches}æ‰¹å¤„ç†{total_results}å¼ å›¾ç‰‡ï¼Œæ¯æ‰¹{batch_size}å¼ ...",
                "progress"
            )
        
        # åˆ†æ‰¹å¼‚æ­¥å¤„ç†
        import asyncio
        batch_tasks = []
        
        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, total_results)
            batch_results = recognition_results[start_idx:end_idx]
            
            # åˆ›å»ºæ‰¹å¤„ç†ä»»åŠ¡
            task = asyncio.create_task(
                self._process_batch(
                    batch_idx, batch_results, llm, csv_file_path, workflow_chat
                )
            )
            batch_tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰æ‰¹æ¬¡å®Œæˆå¹¶æ”¶é›†ç»“æœ
        for completed_task in asyncio.as_completed(batch_tasks):
            batch_story_results = await completed_task
            all_story_results.extend(batch_story_results)
        
        # æ‰¹é‡æ›´æ–°åŸå§‹CSVæ–‡ä»¶ä¸­çš„å…³é”®è¯å’Œæ•…äº‹å†…å®¹
        update_result = await self._update_csv_with_stories(all_story_results, csv_file_path, workflow_chat)
        
        if workflow_chat:
            await workflow_chat.add_node_message(
                "æ•…äº‹ç”Ÿæˆ",
                f"âœ… å·²æˆåŠŸç”Ÿæˆå¹¶ä¿å­˜ {len(all_story_results)} ä¸ªæ•…äº‹",
                "success"
            )
        
        # è¾“å‡ºç»“æœ
        output_data = input_data.copy()
        output_data['story_results'] = all_story_results
        output_data['story_save_result'] = update_result
        
        logger.info(f"âœ… æ•…äº‹ç”Ÿæˆå®Œæˆï¼Œå…± {len(all_story_results)} ä¸ª")
        yield output_data
    
    async def _process_batch(self, batch_idx: int, batch_results: List[Dict], 
                           llm, csv_path: str, workflow_chat=None) -> List[Dict]:
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„å›¾ç‰‡"""
        logger.info(f"å¼€å§‹å¤„ç†ç¬¬{batch_idx+1}æ‰¹ï¼ŒåŒ…å«{len(batch_results)}å¼ å›¾ç‰‡")
        if workflow_chat:
            await workflow_chat.add_node_message(
                "æ•…äº‹ç”Ÿæˆ",
                f"æ­£åœ¨å¤„ç†ç¬¬{batch_idx+1}æ‰¹ï¼Œ{len(batch_results)}å¼ å›¾ç‰‡...",
                "progress"
            )
        
        # ç”Ÿæˆæ•…äº‹
        story_results = []
        for idx, result in enumerate(batch_results):
            try:
                # ä¸å†éœ€è¦å‡†å¤‡äº‘æ¢å¸‚åœ°ç‚¹ä¿¡æ¯ï¼Œå› ä¸ºæˆ‘ä»¬ä¸å¸Œæœ›æ•…äº‹ä¸­åŒ…å«å…·ä½“åœ°ç‚¹
                
                # æ„å»ºæ•…äº‹ç”Ÿæˆæç¤ºè¯
                system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•…äº‹åˆ›ä½œåŠ©æ‰‹ï¼Œæ“…é•¿æ ¹æ®å›¾ç‰‡æè¿°å’Œè§’è‰²äººè®¾ç”Ÿæˆæœ‰æ·±åº¦ã€æƒ…èŠ‚ä¸°å¯Œçš„æ•…äº‹ã€‚

è¯·æ ¹æ®æä¾›çš„å›¾ç‰‡æè¿°å’Œè§’è‰²äººè®¾ï¼Œåˆ›ä½œä¸€ä¸ªçŸ­ç¯‡æ•…äº‹ã€‚æ•…äº‹åº”è¯¥å›´ç»•ä¸»è§’æ–¹çŸ¥è¡¡å±•å¼€ï¼Œå¹¶ä¸å›¾ç‰‡æè¿°ä¸­çš„åœºæ™¯ã€å…ƒç´ è‡ªç„¶èåˆã€‚
æ³¨æ„è¿™ä¸ªæ•…äº‹æ˜¯åƒäº²å¯†çš„äººåˆ†äº«ä½¿ç”¨çš„ï¼Œä»–åº”è¯¥èƒ½è§¦å‘ç”œèœœç¾å¥½çš„è¯é¢˜

## è§’è‰²äººè®¾
""" + self.protagonist_data + f"""

## å›¾ç‰‡æè¿°
æ ‡é¢˜ï¼š{result.get('title', '')}
è¯¦ç»†æè¿°ï¼š{result.get('description', '')}
ï¼ˆè¿™æ˜¯è§’è‰²æ‹ä¸‹çš„ç…§ç‰‡çš„æè¿°ï¼Œä½†ä¸éœ€è¦äº¤ä»£ç›¸æœºæˆ–æ‰‹æœºæ‹æ‘„çš„è¿‡ç¨‹ï¼‰

è¯·åˆ›ä½œä¸€ä¸ª100-150å­—çš„çŸ­ç¯‡æ•…äº‹ï¼Œè¦æ±‚ï¼š
1. æ•…äº‹å¿…é¡»ä»¥æ–¹çŸ¥è¡¡ä¸ºä¸»è§’ï¼Œå¹¶ä¸å›¾ç‰‡æè¿°ä¸­çš„åœºæ™¯å’Œå…ƒç´ è‡ªç„¶èåˆ
2. æ•…äº‹å¿…é¡»åŒ…å«å…·ä½“çš„äº‹ä»¶å’Œå†²çªï¼Œé¿å…ç©ºæ´çš„æè¿°
6. ä¸è¦æåŠä»»ä½•å…·ä½“çš„åœ°ç‚¹åç§°ï¼Œä¿æŒåœºæ™¯æè¿°é€šç”¨åŒ–
7. æœ‰ç‚¹åƒæ¸¸æˆæ”¯çº¿å‰§æƒ…ï¼Œæƒ…èŠ‚è¦æœ‰æ„å¤–æ€§å’Œè½¬æŠ˜
9. è§’è‰²è¡Œä¸ºè¦ç¬¦åˆå…¶äººè®¾ï¼Œä¿æŒä¸€è‡´æ€§
10. æ•…äº‹æœ€åä¸è¦æ”¹å˜ä»»ä½•äººç‰©çŠ¶æ€ï¼Œæ¯”å¦‚å…»åŠ¨ç‰©
11. ä¸è¦ç”¨xxxç»“æŸäº†ä¸€å¤©çš„å·¥ä½œä½œä¸ºå¼•å­

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼šJSONæ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- story: æ•…äº‹å†…å®¹ï¼ˆåŒ…å«å…·ä½“å¯¹è¯ã€å†…å¿ƒæ´»åŠ¨å’Œç¯å¢ƒæå†™ï¼‰
- elements: æ•…äº‹ä¸­å‡ºç°çš„å…·ä½“ç‰©å“ã€äººç‰©ã€åŠ¨ç‰©å®ä½“åè¯ï¼Œ5-20ä¸ªï¼ˆæ•°ç»„ï¼Œå¦‚ï¼šçŒ«ã€æ ‘ã€è½¦ã€äººã€èŠ±ã€å»ºç­‘ç­‰ï¼Œä¸è¦æ¦‚å¿µã€å½¢å®¹è¯ã€é¢œè‰²ï¼‰

è¯·ç¡®ä¿è¾“å‡ºä¸ºä¸¥æ ¼çš„JSONæ ¼å¼ï¼Œç¦æ­¢è¾“å‡ºä»»ä½•å…¶ä»–å†…å®¹ã€‚
ç¤ºä¾‹ï¼š
{{
  "story": "æ–¹çŸ¥è¡¡èµ°åœ¨è¡—é“ä¸Š...",
  "elements": [ "è¡—é“", "çŒ«", "æ ‘", "è½¦"]
}}"""
                
                # æ„å»ºç”¨æˆ·æ¶ˆæ¯
                user_message = Message(
                    role=MessageRole.USER,
                    content=f"è¯·æ ¹æ®ä»¥ä¸‹å›¾ç‰‡æè¿°å’Œè§’è‰²äººè®¾ï¼Œåˆ›ä½œä¸€ä¸ªçŸ­ç¯‡æ•…äº‹ï¼š\n\nå›¾ç‰‡æ ‡é¢˜ï¼š{result.get('title', '')}\nå›¾ç‰‡æè¿°ï¼š{result.get('description', '')}\nä¸»è¦å…ƒç´ ï¼š{', '.join(result.get('elements', []))}\n\nè¯·ç¡®ä¿æ•…äº‹ä»¥æ–¹çŸ¥è¡¡ä¸ºä¸»è§’ï¼Œä¸è¦æåŠå…·ä½“åœ°ç‚¹åç§°ï¼Œå¹¶ä¸å›¾ç‰‡æè¿°ä¸­çš„åœºæ™¯ã€å…ƒç´ è‡ªç„¶èåˆã€‚"
                )
                
                # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
                messages = [
                    Message(role=MessageRole.SYSTEM, content=system_prompt),
                    user_message
                ]
                
                # è°ƒç”¨LLMç”Ÿæˆæ•…äº‹
                logger.info(f"å¼€å§‹ä¸ºå›¾ç‰‡ç”Ÿæˆæ•…äº‹: {result.get('image_name', '')}")
                
                # ä½¿ç”¨åŸå§‹æ¨¡å‹ï¼ˆæ–‡æœ¬æ¨¡å‹ï¼‰
                original_model = llm.config.model_name
                text_model = os.getenv('DOUBAO_MODEL_PRO', 'ep-20250312153153-npj4s')
                llm.config.model_name = text_model
                
                logger.info(f"ä½¿ç”¨æ–‡æœ¬æ¨¡å‹: {text_model}")
                
                # è°ƒç”¨LLM - ä½¿ç”¨æµå¼è¾“å‡º
                content = ""
                logger.info("å¼€å§‹æµå¼ç”Ÿæˆæ•…äº‹...")
                async for chunk in llm.stream_generate(
                    messages,
                    temperature=0.8,  # ç¨å¾®æé«˜åˆ›æ„æ€§
                    max_tokens=4096,
                    mode="normal"
                ):
                    # æ‰“å°æ¯ä¸ªchunk
                    chunk_text = chunk.content if hasattr(chunk, 'content') else str(chunk)
                    print(chunk_text,end='',flush=True)
                    content += chunk_text
                    
                # æ¢å¤åŸå§‹æ¨¡å‹åç§°
                llm.config.model_name = original_model
                
                # è§£æç»“æœ
                # contentå·²ç»åœ¨æµå¼ç”Ÿæˆè¿‡ç¨‹ä¸­ç´¯ç§¯
                
                # ä»å›å¤ä¸­æå–JSON
                import re
                json_match = re.search(r'```json\s*(.*?)\s*```', content, re.DOTALL)
                
                if json_match:
                    json_str = json_match.group(1)
                else:
                    # å°è¯•æ‰¾åˆ°å¤§æ‹¬å·åŒ…å›´çš„JSON
                    json_match = re.search(r'\{.*\}', content, re.DOTALL)
                    if json_match:
                        json_str = json_match.group(0)
                    else:
                        json_str = content
                
                # è§£æJSON
                try:
                    story_data = json.loads(json_str.strip())
                    # ç¡®ä¿åŒ…å«å¿…è¦å­—æ®µ
                    if 'story' not in story_data:
                        story_data = {"story": content}
                    if 'elements' not in story_data:
                        story_data['elements'] = []
                except json.JSONDecodeError:
                    logger.warning(f"JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å›å¤")
                    story_data = {"story": content, "elements": []}
                
                # æ·»åŠ å›¾ç‰‡ä¿¡æ¯å’ŒåŸå§‹æè¿°
                story_data["image_name"] = result.get("image_name", "")
                story_data["image_path"] = result.get("image_path", "")
                story_data["original_title"] = result.get("title", "")
                story_data["original_description"] = result.get("description", "")
                story_data["original_elements"] = result.get("elements", [])
                
                story_results.append(story_data)
                logger.info(f"æ•…äº‹ç”ŸæˆæˆåŠŸ: {result.get('image_name', '')}")
                
            except Exception as e:
                logger.error(f"æ•…äº‹ç”Ÿæˆå¤±è´¥: {e}")
                story_results.append({
                    "image_name": result.get("image_name", "æœªçŸ¥å›¾ç‰‡"),
                    "image_path": result.get("image_path", "æœªçŸ¥è·¯å¾„"),
                    "original_title": result.get("title", ""),
                    "original_description": result.get("description", ""),
                    "original_elements": result.get("elements", []),
                    "title": f"æ–¹çŸ¥è¡¡ä¸{result.get('title', 'æœªçŸ¥åœºæ™¯')}",
                    "story": f"æ•…äº‹ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}",
                    "theme": "é”™è¯¯",
                    "elements": [],
                    "error": str(e)
                })
        
        # æ³¨é‡Šæ‰æ‰¹æ¬¡ä¿å­˜ï¼Œæ”¹ä¸ºåœ¨ä¸»æ–¹æ³•ä¸­ç»Ÿä¸€æ›´æ–°
        # await self._append_to_csv(story_results, csv_path, workflow_chat)
        
        if workflow_chat:
            await workflow_chat.add_node_message(
                "æ•…äº‹ç”Ÿæˆ",
                f"âœ… ç¬¬{batch_idx+1}æ‰¹ï¼šå·²ç”Ÿæˆå¹¶ä¿å­˜ {len(story_results)} ä¸ªæ•…äº‹",
                "streaming"
            )
        
        logger.info(f"âœ… ç¬¬{batch_idx+1}æ‰¹å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(story_results)} ä¸ªæ•…äº‹")
        return story_results
    
    async def _prepare_csv_file(self, original_csv_path: str) -> str:
        """å‡†å¤‡CSVæ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´"""
        # ç”Ÿæˆå¸¦æ•…äº‹åç¼€çš„CSVæ–‡ä»¶å
        if not original_csv_path or not os.path.exists(original_csv_path):
            # å¦‚æœåŸå§‹CSVä¸å­˜åœ¨ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„
            output_dir = 'workspace/image_recognition_output'
            os.makedirs(output_dir, exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            story_csv_path = os.path.join(output_dir, f'image_recognition_with_story_{timestamp}.csv')
        else:
            # åŸºäºåŸå§‹CSVåˆ›å»ºå¸¦æ•…äº‹åç¼€çš„æ–°æ–‡ä»¶
            base_name, ext = os.path.splitext(original_csv_path)
            story_csv_path = f"{base_name}_with_story{ext}"
        
        # åˆ›å»ºæ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´
        with open(story_csv_path, 'w', newline='', encoding='utf-8-sig') as f:
            fieldnames = ['å›¾ç‰‡åç§°', 'å›¾ç‰‡è·¯å¾„', 'åŸå§‹æ ‡é¢˜', 'åŸå§‹æè¿°', 'åŸå§‹å…ƒç´ ', 'æ•…äº‹å†…å®¹']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
        
        logger.info(f"å·²åˆ›å»ºCSVæ–‡ä»¶: {story_csv_path}")
        return story_csv_path
    
    async def _update_csv_with_stories(self, story_results: List[Dict], csv_path: str, workflow_chat=None) -> Dict:
        """æ›´æ–°åŸå§‹CSVæ–‡ä»¶ï¼Œå¡«å……å…³é”®è¯å’Œæ•…äº‹å†…å®¹"""
        try:
            if not story_results:
                return {
                    'success': False,
                    'message': "æ²¡æœ‰æ•…äº‹ç»“æœéœ€è¦ä¿å­˜"
                }
            
            # è¯»å–åŸå§‹CSVæ–‡ä»¶
            rows = []
            fieldnames = []
            
            if os.path.exists(csv_path):
                with open(csv_path, 'r', encoding='utf-8-sig', newline='') as f:
                    reader = csv.DictReader(f)
                    fieldnames = reader.fieldnames
                    rows = list(reader)
            else:
                # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºé»˜è®¤ç»“æ„
                fieldnames = ['åºå·ID', 'å›¾ç‰‡åç§°', 'å›¾ç‰‡è·¯å¾„', 'å›¾ç‰‡æ ‡é¢˜', 'å›¾ç‰‡æè¿°', 'å…³é”®è¯', 'æ•…äº‹å†…å®¹']
            
            # åˆ›å»ºæ•…äº‹ç»“æœçš„ç´¢å¼•ï¼Œä»¥å›¾ç‰‡è·¯å¾„ä¸ºé”®
            story_dict = {}
            for story in story_results:
                image_path = story.get('image_path', '')
                if image_path:
                    story_dict[image_path] = story
            
            # æ›´æ–°è¡Œæ•°æ®
            for row in rows:
                image_path = row.get('å›¾ç‰‡è·¯å¾„', '')
                if image_path in story_dict:
                    story_data = story_dict[image_path]
                    # æ›´æ–°å…³é”®è¯å­—æ®µï¼ˆä½¿ç”¨æ•…äº‹ä¸­çš„å®ä½“åè¯ï¼‰
                    elements = story_data.get('elements', [])
                    if isinstance(elements, list):
                        row['å…³é”®è¯'] = ' '.join(elements)
                    else:
                        row['å…³é”®è¯'] = elements
                    
                    # æ›´æ–°æ•…äº‹å†…å®¹
                    row['æ•…äº‹å†…å®¹'] = story_data.get('story', '')
            
            # å†™å›CSVæ–‡ä»¶
            with open(csv_path, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(rows)
            
            logger.info(f"âœ… å·²æ›´æ–°CSVæ–‡ä»¶ï¼š{len(story_results)}ä¸ªæ•…äº‹çš„å…³é”®è¯å’Œå†…å®¹å·²æ·»åŠ åˆ° {os.path.basename(csv_path)}")
            
            return {
                'success': True,
                'message': f"æˆåŠŸæ›´æ–°{len(story_results)}æ¡è®°å½•çš„æ•…äº‹å’Œå…³é”®è¯",
                'count': len(story_results),
                'file_path': csv_path
            }
            
        except Exception as e:
            logger.error(f"CSVæ–‡ä»¶æ›´æ–°å¤±è´¥: {e}")
            if workflow_chat:
                await workflow_chat.add_node_message(
                    "æ•…äº‹ç”Ÿæˆ",
                    f"âŒ CSVæ–‡ä»¶æ›´æ–°å¤±è´¥: {str(e)}",
                    "error"
                )
            
            return {
                'success': False,
                'message': f"æ›´æ–°å¤±è´¥: {str(e)}",
                'error': str(e)
            }
    
    async def _append_to_csv(self, story_results: List[Dict], csv_path: str, workflow_chat=None) -> Dict:
        """å¢é‡è¿½åŠ æ•…äº‹ç»“æœåˆ°CSVæ–‡ä»¶"""
        try:
            if not story_results:
                return {
                    'success': False,
                    'message': "æ²¡æœ‰æ•…äº‹ç»“æœéœ€è¦ä¿å­˜"
                }
            
            # è¿½åŠ æ¨¡å¼å†™å…¥CSVæ–‡ä»¶
            with open(csv_path, 'a', newline='', encoding='utf-8-sig') as f:
                fieldnames = ['å›¾ç‰‡åç§°', 'å›¾ç‰‡è·¯å¾„', 'å›¾ç‰‡æ ‡é¢˜', 'å›¾ç‰‡æè¿°', 'å…³é”®è¯', 'æ•…äº‹å†…å®¹']
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                
                # å†™å…¥æ•…äº‹ç»“æœ
                for result in story_results:
                    writer.writerow({
                        'å›¾ç‰‡åç§°': result.get('image_name', ''),
                        'å›¾ç‰‡è·¯å¾„': result.get('image_path', ''),
                        'å›¾ç‰‡æ ‡é¢˜': result.get('original_title', ''),
                        'å›¾ç‰‡æè¿°': result.get('original_description', ''),
                        'å…³é”®è¯': ','.join(result.get('original_elements', [])) if isinstance(result.get('original_elements', []), list) else result.get('original_elements', ''),
                        'æ•…äº‹å†…å®¹': result.get('story', '')
                    })
            
            logger.info(f"âœ… å¢é‡ä¿å­˜ï¼š{len(story_results)}ä¸ªæ•…äº‹å·²è¿½åŠ åˆ° {os.path.basename(csv_path)}")
            
            return {
                'success': True,
                'message': f"æˆåŠŸè¿½åŠ {len(story_results)}ä¸ªæ•…äº‹",
                'count': len(story_results),
                'file_path': csv_path
            }
            
        except Exception as e:
            logger.error(f"æ•…äº‹CSVå¢é‡ä¿å­˜å¤±è´¥: {e}")
            if workflow_chat:
                await workflow_chat.add_node_message(
                    "æ•…äº‹ç”Ÿæˆ",
                    f"âŒ æ•…äº‹CSVå¢é‡ä¿å­˜å¤±è´¥: {str(e)}",
                    "error"
                )
            
            return {
                'success': False,
                'message': f"å¢é‡ä¿å­˜å¤±è´¥: {str(e)}",
                'error': str(e)
            }
    
    async def _save_to_csv(self, story_results: List[Dict], original_csv_path: str, workflow_chat=None) -> Dict:
        """ä¿å­˜æ•…äº‹ç»“æœåˆ°CSVæ–‡ä»¶ï¼ˆæ—§æ–¹æ³•ï¼Œä¿ç•™å‘åå…¼å®¹ï¼‰"""
        """ä¿å­˜æ•…äº‹ç»“æœåˆ°CSVæ–‡ä»¶"""
        try:
            if not story_results:
                return {
                    'success': False,
                    'message': "æ²¡æœ‰æ•…äº‹ç»“æœéœ€è¦ä¿å­˜"
                }
            
            # ç”Ÿæˆå¸¦æ•…äº‹åç¼€çš„CSVæ–‡ä»¶å
            if not original_csv_path or not os.path.exists(original_csv_path):
                # å¦‚æœåŸå§‹CSVä¸å­˜åœ¨ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„
                output_dir = 'workspace/image_recognition_output'
                os.makedirs(output_dir, exist_ok=True)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                story_csv_path = os.path.join(output_dir, f'image_recognition_with_story_{timestamp}.csv')
            else:
                # åŸºäºåŸå§‹CSVåˆ›å»ºå¸¦æ•…äº‹åç¼€çš„æ–°æ–‡ä»¶
                base_name, ext = os.path.splitext(original_csv_path)
                story_csv_path = f"{base_name}_with_story{ext}"
            
            # å†™å…¥CSVæ–‡ä»¶
            with open(story_csv_path, 'w', newline='', encoding='utf-8-sig') as f:
                fieldnames = ['å›¾ç‰‡åç§°', 'å›¾ç‰‡è·¯å¾„', 'æ ‡é¢˜', 'å›¾ç‰‡æè¿°', 'å›¾ç‰‡å…ƒç´ ', 'æ•…äº‹å†…å®¹']
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                
                # å†™å…¥æ•…äº‹ç»“æœ
                for result in story_results:
                    writer.writerow({
                        'å›¾ç‰‡åç§°': result.get('image_name', ''),
                        'å›¾ç‰‡è·¯å¾„': result.get('image_path', ''),
                        'åŸå§‹æ ‡é¢˜': result.get('original_title', ''),
                        'åŸå§‹æè¿°': result.get('original_description', ''),
                        'åŸå§‹å…ƒç´ ': ','.join(result.get('original_elements', [])) if isinstance(result.get('original_elements'), list) else result.get('original_elements', ''),
                        'æ•…äº‹å†…å®¹': result.get('story', '')
                    })
            
            if workflow_chat:
                await workflow_chat.add_node_message(
                    "æ•…äº‹ç”Ÿæˆ",
                    f"âœ… {len(story_results)}ä¸ªæ•…äº‹å·²ä¿å­˜åˆ°CSVæ–‡ä»¶: {os.path.basename(story_csv_path)}",
                    "success"
                )
            
            logger.info(f"âœ… æ•…äº‹CSVä¿å­˜å®Œæˆï¼š{len(story_results)}ä¸ªæ•…äº‹ä¿å­˜åˆ° {story_csv_path}")
            
            return {
                'success': True,
                'message': f"æˆåŠŸä¿å­˜{len(story_results)}ä¸ªæ•…äº‹",
                'count': len(story_results),
                'file_path': story_csv_path
            }
            
        except Exception as e:
            logger.error(f"æ•…äº‹CSVä¿å­˜å¤±è´¥: {e}")
            if workflow_chat:
                await workflow_chat.add_node_message(
                    "æ•…äº‹ç”Ÿæˆ",
                    f"âŒ æ•…äº‹CSVä¿å­˜å¤±è´¥: {str(e)}",
                    "error"
                )
            
            return {
                'success': False,
                'message': f"ä¿å­˜å¤±è´¥: {str(e)}",
                'error': str(e)
            }

    @staticmethod
    def read_csv_file(csv_file_path: str) -> List[Dict]:
        """è¯»å–CSVæ–‡ä»¶ä¸­çš„å›¾ç‰‡è¯†åˆ«ç»“æœ"""
        recognition_results = []
        try:
            if not os.path.exists(csv_file_path):
                print(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file_path}")
                return []
            
            with open(csv_file_path, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    # å°†CSVè¡Œè½¬æ¢ä¸ºè¯†åˆ«ç»“æœæ ¼å¼
                    elements = row.get('ä¸»è¦å…ƒç´ ', '').split(',') if row.get('ä¸»è¦å…ƒç´ ') else []
                    recognition_result = {
                        'image_name': row.get('å›¾ç‰‡åç§°', ''),
                        'image_path': row.get('å›¾ç‰‡è·¯å¾„', ''),
                        'title': row.get('æ ‡é¢˜', ''),
                        'description': row.get('è¯¦ç»†æè¿°', ''),
                        'elements': elements
                    }
                    recognition_results.append(recognition_result)
            
            print(f"âœ… æˆåŠŸè¯»å–CSVæ–‡ä»¶ï¼Œå…±{len(recognition_results)}æ¡å›¾ç‰‡è¯†åˆ«ç»“æœ")
            return recognition_results
        except Exception as e:
            print(f"âŒ è¯»å–CSVæ–‡ä»¶å¤±è´¥: {e}")
            return []


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("æ•…äº‹ç”Ÿæˆå™¨æ¨¡å— - ç›´æ¥è¿è¡Œæµ‹è¯•")
    
    # åˆ›å»ºèŠ‚ç‚¹å®ä¾‹
    node = StoryGenerationNode()
    print(f"æˆåŠŸåŠ è½½ä¸»è§’äººè®¾ï¼Œé•¿åº¦: {len(node.protagonist_data)} å­—ç¬¦")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šCSVæ–‡ä»¶
    if len(sys.argv) > 1:
        csv_file_path = sys.argv[1]
    else:
        # é»˜è®¤å¤„ç†æœ€æ–°çš„å›¾ç‰‡è¯†åˆ«ç»“æœCSVæ–‡ä»¶
        csv_file_path = "workspace/image_recognition_output/image_recognition_20250704_112047.csv"
    
    # æ£€æŸ¥æ˜¯å¦æŒ‡å®šäº†æ‰¹å¤„ç†å¤§å°
    batch_size = 5  # é»˜è®¤æ¯æ‰¹5ä¸ª
    if len(sys.argv) > 2:
        try:
            batch_size = int(sys.argv[2])
        except ValueError:
            print(f"æ‰¹å¤„ç†å¤§å°å‚æ•°æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼: {batch_size}")
    
    # è¯»å–CSVæ–‡ä»¶
    recognition_results = StoryGenerationNode.read_csv_file(csv_file_path)
    
    if recognition_results:
        print(f"å¼€å§‹å¤„ç†{len(recognition_results)}æ¡å›¾ç‰‡è¯†åˆ«ç»“æœ...")
        
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        import asyncio
        from llm.doubao import DoubaoLLM
        
        # åˆ›å»ºå¼‚æ­¥è¿è¡Œå‡½æ•°
        async def run_story_generation():
            # åˆå§‹åŒ–LLM
            from core.types import LLMConfig
            # åˆ›å»ºLLMé…ç½®
            llm_config = LLMConfig(
                provider="doubao",
                model_name=os.getenv('DOUBAO_MODEL_PRO', 'ep-20250312153153-npj4s'),
                api_key=os.getenv('DOUBAO_API_KEY', 'b633a622-b5d0-4f16-a8a9-616239cf15d1'),
                api_base=os.getenv('DOUBAO_BASE_URL', 'https://ark.cn-beijing.volces.com/api/v3'),
                temperature=0.7,
                max_tokens=4096,
                streaming=True
            )
            # ä½¿ç”¨é…ç½®åˆå§‹åŒ–LLM
            llm = DoubaoLLM(config=llm_config)
            
            # å‡†å¤‡è¾“å…¥æ•°æ®
            input_data = {
                'llm': llm,
                'recognition_results': recognition_results,
                'csv_save_result': {'file_path': csv_file_path},
                'batch_size': batch_size  # è®¾ç½®æ‰¹å¤„ç†å¤§å°
            }
            
            # æ‰§è¡Œæ•…äº‹ç”Ÿæˆ
            async for result in node.execute_stream(input_data):
                # å¤„ç†å®Œæˆ
                story_results = result.get('story_results', [])
                story_save_result = result.get('story_save_result', {})
                
                if story_save_result.get('success'):
                    print(f"âœ… æ•…äº‹ç”Ÿæˆå®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°: {story_save_result.get('file_path')}")
                else:
                    print(f"âŒ æ•…äº‹ç”Ÿæˆå¤±è´¥: {story_save_result.get('message')}")
        
        # è¿è¡Œå¼‚æ­¥å‡½æ•°
        asyncio.run(run_story_generation())
    else:
        print("æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡è¯†åˆ«ç»“æœï¼Œæ— æ³•ç”Ÿæˆæ•…äº‹")
    
    print("æµ‹è¯•å®Œæˆ")