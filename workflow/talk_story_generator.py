"""å¯¹è¯æ•…äº‹ç”Ÿæˆå™¨ - ç›´æ¥ç”Ÿæˆé€‚åˆåˆ†äº«çš„å¯¹è¯æ•…äº‹
æ— éœ€å‰ç½®å›¾ç‰‡æè¿°ï¼Œç›´æ¥ç”Ÿæˆæ¸©é¦¨æœ‰è¶£çš„å¯¹è¯æ•…äº‹ï¼Œå¹¶ä¿å­˜ä¸ºCSVæ–‡ä»¶
"""

import os
import csv
import json
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional

import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from core.base import BaseNode
from core.types import Message, MessageRole

logger = logging.getLogger(__name__)

class TalkStoryGenerationNode(BaseNode):
    """å¯¹è¯æ•…äº‹ç”ŸæˆèŠ‚ç‚¹ - ç›´æ¥ç”Ÿæˆé€‚åˆä¸äº²å¯†çš„äººåˆ†äº«çš„å¯¹è¯æ•…äº‹"""
    
    def __init__(self):
        super().__init__(name="talk_story_generation", stream=True)
        self.protagonist_data = ""
        self._load_protagonist_data()
    
    def _load_protagonist_data(self):
        """åŠ è½½ä¸»è§’æ–¹çŸ¥è¡¡çš„è¯¦ç»†äººè®¾"""
        try:
            protagonist_path = os.path.join(os.path.dirname(__file__), '../../config/åŸºç¡€äººè®¾.txt')
            if os.path.exists(protagonist_path):
                with open(protagonist_path, 'r', encoding='utf-8') as f:
                    self.protagonist_data = f.read()
                    logger.info(f"æˆåŠŸåŠ è½½ä¸»è§’äººè®¾ï¼Œå†…å®¹é•¿åº¦: {len(self.protagonist_data)} å­—ç¬¦")
            else:
                logger.warning("ä¸»è§’äººè®¾æ–‡ä»¶ä¸å­˜åœ¨")
                
        except Exception as e:
            logger.error(f"åŠ è½½ä¸»è§’äººè®¾å¤±è´¥: {e}")
    
    async def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå¯¹è¯æ•…äº‹ç”ŸæˆèŠ‚ç‚¹"""
        final_result = None
        async for result in self.execute_stream(input_data):
            final_result = result
        return final_result or input_data
    
    async def execute_stream(self, input_data: Dict[str, Any]):
        """æµå¼æ‰§è¡Œå¯¹è¯æ•…äº‹ç”ŸæˆèŠ‚ç‚¹ - æ”¯æŒæ‰¹é‡å¤„ç†å’Œå¢é‡ä¿å­˜"""
        print("ğŸ“ å¼€å§‹ç”Ÿæˆå¯¹è¯æ•…äº‹...")
        
        workflow_chat = input_data.get('workflow_chat')
        llm = input_data.get('llm')
        story_count = input_data.get('story_count', 5)  # é»˜è®¤ç”Ÿæˆ5ä¸ªæ•…äº‹
        topics = input_data.get('topics', [])  # å¯é€‰çš„è¯é¢˜åˆ—è¡¨
        output_dir = input_data.get('output_dir', 'workspace/talk_story_output')
        batch_size = input_data.get('batch_size', 5)  # é»˜è®¤æ¯æ‰¹å¤„ç†5ä¸ªæ•…äº‹
        
        if not llm:
            # LLMæœªåˆå§‹åŒ–ï¼Œæ— æ³•ç”Ÿæˆæ•…äº‹
            if workflow_chat:
                await workflow_chat.add_node_message(
                    "å¯¹è¯æ•…äº‹ç”Ÿæˆ",
                    "âš ï¸ LLMæœªåˆå§‹åŒ–ï¼Œæ— æ³•ç”Ÿæˆå¯¹è¯æ•…äº‹",
                    "warning"
                )
            
            output_data = input_data.copy()
            output_data['story_results'] = []
            output_data['story_save_result'] = {
                'success': False,
                'message': "LLMæœªåˆå§‹åŒ–ï¼Œæ— æ³•ç”Ÿæˆå¯¹è¯æ•…äº‹"
            }
            yield output_data
            return
        
        if workflow_chat:
            await workflow_chat.add_node_message(
                "å¯¹è¯æ•…äº‹ç”Ÿæˆ",
                f"æ­£åœ¨ç”Ÿæˆ{story_count}ä¸ªå¯¹è¯æ•…äº‹...",
                "progress"
            )
        
        # åˆ†æ‰¹å¤„ç†
        total_batches = (story_count + batch_size - 1) // batch_size
        all_story_results = []
        
        if workflow_chat:
            await workflow_chat.add_node_message(
                "å¯¹è¯æ•…äº‹ç”Ÿæˆ",
                f"å°†åˆ†{total_batches}æ‰¹ç”Ÿæˆ{story_count}ä¸ªæ•…äº‹ï¼Œæ¯æ‰¹{batch_size}ä¸ª...",
                "progress"
            )
        
        # åˆ›å»ºCSVæ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        csv_path = os.path.join(output_dir, f'talk_stories_{timestamp}.csv')
        
        with open(csv_path, 'w', newline='', encoding='utf-8-sig') as f:
            fieldnames = ['æ•…äº‹ID', 'æ•…äº‹æ ‡é¢˜', 'æ•…äº‹å†…å®¹', 'æ•…äº‹ä¸»é¢˜', 'æ•…äº‹å¯“æ„', 'ç”Ÿæˆæ—¶é—´']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
        
        logger.info(f"å·²åˆ›å»ºCSVæ–‡ä»¶: {csv_path}")
        
        # åˆ†æ‰¹å¼‚æ­¥å¤„ç†
        import asyncio
        batch_tasks = []
        
        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, story_count)
            batch_count = end_idx - start_idx
            
            # ä¸ºå½“å‰æ‰¹æ¬¡é€‰æ‹©è¯é¢˜ï¼ˆå¦‚æœæœ‰ï¼‰
            batch_topics = []
            if topics:
                for i in range(start_idx, end_idx):
                    topic_idx = i % len(topics)
                    batch_topics.append(topics[topic_idx])
            
            # åˆ›å»ºæ‰¹å¤„ç†ä»»åŠ¡
            task = asyncio.create_task(
                self._process_batch(
                    batch_idx, batch_count, batch_topics, llm, csv_path, workflow_chat
                )
            )
            batch_tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰æ‰¹æ¬¡å®Œæˆå¹¶æ”¶é›†ç»“æœ
        for completed_task in asyncio.as_completed(batch_tasks):
            batch_story_results = await completed_task
            all_story_results.extend(batch_story_results)
        
        if workflow_chat:
            await workflow_chat.add_node_message(
                "å¯¹è¯æ•…äº‹ç”Ÿæˆ",
                f"âœ… å·²æˆåŠŸç”Ÿæˆå¹¶ä¿å­˜ {len(all_story_results)} ä¸ªå¯¹è¯æ•…äº‹",
                "success"
            )
        
        # è¾“å‡ºç»“æœ
        output_data = input_data.copy()
        output_data['story_results'] = all_story_results
        output_data['story_save_result'] = {
            'success': True,
            'message': f"æˆåŠŸç”Ÿæˆå¹¶ä¿å­˜{len(all_story_results)}ä¸ªå¯¹è¯æ•…äº‹",
            'count': len(all_story_results),
            'file_path': csv_path
        }
        
        logger.info(f"âœ… å¯¹è¯æ•…äº‹ç”Ÿæˆå®Œæˆï¼Œå…± {len(all_story_results)} ä¸ª")
        yield output_data
    
    async def _process_batch(self, batch_idx: int, batch_count: int, batch_topics: List[str], 
                           llm, csv_path: str, workflow_chat=None) -> List[Dict]:
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„æ•…äº‹ç”Ÿæˆ"""
        logger.info(f"å¼€å§‹å¤„ç†ç¬¬{batch_idx+1}æ‰¹ï¼Œç”Ÿæˆ{batch_count}ä¸ªå¯¹è¯æ•…äº‹")
        if workflow_chat:
            await workflow_chat.add_node_message(
                "å¯¹è¯æ•…äº‹ç”Ÿæˆ",
                f"æ­£åœ¨å¤„ç†ç¬¬{batch_idx+1}æ‰¹ï¼Œç”Ÿæˆ{batch_count}ä¸ªå¯¹è¯æ•…äº‹...",
                "progress"
            )
        
        # ç”Ÿæˆæ•…äº‹
        story_results = []
        for idx in range(batch_count):
            try:
                # è·å–å½“å‰æ•…äº‹çš„è¯é¢˜ï¼ˆå¦‚æœæœ‰ï¼‰
                current_topic = ""
                if idx < len(batch_topics):
                    current_topic = batch_topics[idx]
                
                # æ„å»ºæ•…äº‹ç”Ÿæˆæç¤ºè¯
                system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¯¹è¯æ•…äº‹åˆ›ä½œåŠ©æ‰‹ï¼Œæ“…é•¿ç”Ÿæˆé«˜è´¨é‡ã€æƒ…æ„Ÿä¸°å¯Œã€é€‚åˆåˆ†äº«çš„å¯¹è¯æ•…äº‹ã€‚

è¯·æ ¹æ®æä¾›çš„è§’è‰²äººè®¾ï¼Œåˆ›ä½œä¸€ä¸ªç²¾å½©çš„çŸ­ç¯‡å¯¹è¯æ•…äº‹ã€‚
æ•…äº‹åº”ä¸“æ³¨äºæè¿°ä¸€ä¸ªå…·ä½“çš„æœ‰è¶£äº‹ä»¶æˆ–æƒ…å¢ƒï¼Œèƒ½å¤Ÿå¼•å‘æƒ…æ„Ÿå…±é¸£ï¼Œä½†ä¸åŒ…å«ä»»ä½•ç§å¯†æˆ–æ•æ„Ÿçš„ä¸ªäººä¿¡æ¯ã€‚

## è§’è‰²äººè®¾
{self.protagonist_data}  

{f'## æ•…äº‹è¯é¢˜\n{current_topic}' if current_topic else ''}

è¯·åˆ›ä½œä¸€ä¸ª200-400å­—çš„é«˜è´¨é‡å¯¹è¯æ•…äº‹ï¼Œè¦æ±‚ï¼š
1. æ•…äº‹å¿…é¡»æœ‰æ¸…æ™°çš„å¼€ç«¯ã€å‘å±•ã€é«˜æ½®å’Œç»“å°¾ç»“æ„
2. æ•…äº‹å¿…é¡»åŒ…å«å…·ä½“çš„äº‹ä»¶å’Œå†²çªï¼Œé¿å…å¹³æ·¡å™è¿°
3. è§’è‰²å¯¹è¯è¦è‡ªç„¶ç”ŸåŠ¨ï¼Œå±•ç°äººç‰©æ€§æ ¼ç‰¹ç‚¹
4. ä¸è¦æåŠä»»ä½•å…·ä½“çš„åœ°ç‚¹åç§°ï¼Œä¿æŒåœºæ™¯æè¿°é€šç”¨åŒ–
7. æ•…äº‹å¯ä»¥åŒ…å«å¹½é»˜ã€æ¸©é¦¨æˆ–æ„Ÿäººçš„ç¬é—´ï¼Œå¢å¼ºæƒ…æ„Ÿè¿æ¥
8. è§’è‰²è¡Œä¸ºè¦å®Œå…¨ç¬¦åˆå…¶äººè®¾ï¼Œä¿æŒä¸€è‡´æ€§
9. æ•…äº‹æœ€ååº”è¯¥æœ‰ä¸€ä¸ªæ¸©æš–ã€æœ‰æ„ä¹‰æˆ–å¼•äººæ€è€ƒçš„ç»“å°¾
10. ä¸è¦ç”¨"ç»“æŸäº†ä¸€å¤©çš„å·¥ä½œ"ä½œä¸ºå¼•å­
11. æ•…äº‹ä¸­åº”åŒ…å«ä¸€ä¸ªå°å°çš„æ„å¤–æˆ–è½¬æŠ˜ï¼Œå¢åŠ è¶£å‘³æ€§

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼šJSONæ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- title: æ•…äº‹æ ‡é¢˜ï¼ˆç®€æ´æœ‰å¸å¼•åŠ›ï¼‰
- story: æ•…äº‹å†…å®¹
- theme: æ•…äº‹ä¸»é¢˜æˆ–å…³é”®è¯ï¼ˆ3-5ä¸ªè¯è¯­ï¼‰

{
  "title": "å£è¢‹é‡Œçš„å°æƒŠå–œ",
  "story": "æ–¹çŸ¥è¡¡æ•´ç†æ—§å¤–å¥—æ—¶ï¼Œä»å£è¢‹é‡Œæ‘¸å‡ºä¸€å¼ æ—©å·²è¤ªè‰²çš„åœ°é“ç¥¨ï¼ŒèƒŒé¢æ­ªæ­ªæ‰­æ‰­åœ°ç”»ç€ä¸€åªå¾®ç¬‘çš„çŒ«ã€‚ä»–å®Œå…¨æƒ³ä¸èµ·è¿™å¼ ç¥¨çš„æ¥å†ï¼Œä¾¿éšæ‰‹è´´åœ¨ä¹¦æ¡Œå‰å½“è£…é¥°ã€‚å‡ å¤©åï¼ŒåŒäº‹è·¯è¿‡æ—¶çªç„¶æƒŠå‘¼ï¼š\"è¿™ä¸å°±æ˜¯æˆ‘å¤§å­¦ç¤¾å›¢å°çš„å°çŒ«å—ï¼Ÿ\" åŸæ¥é‚£å¤©ç¤¾å›¢ä¹‰å–ï¼Œä»–æå‡ºäº†æœ€åä¸€å¼ å°æœ‰å¹¸è¿çŒ«çš„çºªå¿µç¥¨ï¼Œæ°å·§è¢«æ–¹çŸ¥è¡¡éšæ‰‹ä¹°ä¸‹ã€‚ä¸¤äººå¯¹è§†ä¸€ç¬‘ï¼Œä»¿ä½›æ—¶å…‰åœ¨æ­¤åˆ»å®Œæˆäº†ä¸€æ¬¡æ¸©æŸ”çš„è¿ç»“â€”â€”é‚£åªå¾®ç¬‘çš„çŒ«ï¼Œç«Ÿè®©å¤šå¹´åçš„äººç”Ÿè·¯å¾„åœ¨ä¸ç»æ„é—´é‡å ã€‚",
  "theme": "å¶é‡, å›å¿†, å°å¹¸è¿",
}

è¯·ç¡®ä¿è¾“å‡ºä¸ºä¸¥æ ¼çš„JSONæ ¼å¼ï¼Œç¦æ­¢è¾“å‡ºä»»ä½•å…¶ä»–å†…å®¹ã€‚"""
                
                # æ„å»ºç”¨æˆ·æ¶ˆæ¯
                user_message = Message(
                    role=MessageRole.USER,
                    content=f"è¯·åˆ›ä½œä¸€ä¸ªç²¾å½©çš„çŸ­ç¯‡å¯¹è¯æ•…äº‹"
                )
                
                # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
                messages = [
                    Message(role=MessageRole.SYSTEM, content=system_prompt),
                    user_message
                ]
                
                # è°ƒç”¨LLMç”Ÿæˆæ•…äº‹
                story_id = batch_idx * batch_count + idx + 1
                logger.info(f"å¼€å§‹ç”Ÿæˆç¬¬{story_id}ä¸ªå¯¹è¯æ•…äº‹")
                
                # ä½¿ç”¨æ–‡æœ¬æ¨¡å‹
                original_model = llm.config.model_name
                text_model = os.getenv('DOUBAO_MODEL_PRO', 'ep-20250312153153-npj4s')
                llm.config.model_name = text_model
                
                logger.info(f"ä½¿ç”¨æ–‡æœ¬æ¨¡å‹: {text_model}")
                
                # è°ƒç”¨LLM - ä½¿ç”¨æµå¼è¾“å‡º
                content = ""
                logger.info("å¼€å§‹æµå¼ç”Ÿæˆå¯¹è¯æ•…äº‹...")
                async for chunk in llm.stream_generate(
                    messages,
                    temperature=0.85,  # æé«˜åˆ›æ„æ€§
                    max_tokens=4096,
                    mode="normal"
                ):
                    # æ‰“å°æ¯ä¸ªchunk
                    chunk_text = chunk.content if hasattr(chunk, 'content') else str(chunk)
                    print(chunk_text,end='',flush=True)
                    content += chunk_text
                    
                # æ¢å¤åŸå§‹æ¨¡å‹åç§°
                llm.config.model_name = original_model
                
                # ä»å›å¤ä¸­æå–JSON
                import re
                json_match = re.search(r'```json\s*(.*?)\s*```', content, re.DOTALL)
                
                if json_match:
                    json_str = json_match.group(1)
                else:
                    # å°è¯•æ‰¾åˆ°å¤§æ‹¬å·åŒ…å›´çš„JSON
                    json_match = re.search(r'\{.*\}', content, re.DOTALL)
                    if json_match:
                        json_str = json_match.group(0)
                    else:
                        json_str = content
                
                # è§£æJSON
                try:
                    story_data = json.loads(json_str.strip())
                    # ç¡®ä¿åŒ…å«å¿…è¦å­—æ®µ
                    if 'story' not in story_data:
                        story_data["story"] = content
                    if 'title' not in story_data:
                        story_data["title"] = f"å¯¹è¯æ•…äº‹ #{story_id}"
                    if 'theme' not in story_data:
                        story_data["theme"] = "æ—¥å¸¸,å¯¹è¯,æƒ…æ„Ÿ"
                except json.JSONDecodeError:
                    logger.warning(f"JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å›å¤")
                    story_data = {
                        "story": content,
                        "title": f"å¯¹è¯æ•…äº‹ #{story_id}",
                        "theme": "æ—¥å¸¸,å¯¹è¯,æƒ…æ„Ÿ"
                    }
                
                # æ·»åŠ æ•…äº‹IDå’Œç”Ÿæˆæ—¶é—´
                story_data["story_id"] = story_id
                story_data["generated_at"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                
                story_results.append(story_data)
                logger.info(f"å¯¹è¯æ•…äº‹ç”ŸæˆæˆåŠŸ: {story_data.get('title', '')}")
                
            except Exception as e:
                logger.error(f"å¯¹è¯æ•…äº‹ç”Ÿæˆå¤±è´¥: {e}")
                story_id = batch_idx * batch_count + idx + 1
                story_results.append({
                    "story_id": story_id,
                    "title": f"å¯¹è¯æ•…äº‹ #{story_id} (ç”Ÿæˆå¤±è´¥)",
                    "story": f"å¯¹è¯æ•…äº‹ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}",
                    "theme": "é”™è¯¯",
                    "generated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "error": str(e)
                })
        
        # å¢é‡ä¿å­˜å½“å‰æ‰¹æ¬¡ç»“æœåˆ°CSV
        await self._append_to_csv(story_results, csv_path, workflow_chat)
        
        if workflow_chat:
            await workflow_chat.add_node_message(
                "å¯¹è¯æ•…äº‹ç”Ÿæˆ",
                f"âœ… ç¬¬{batch_idx+1}æ‰¹ï¼šå·²ç”Ÿæˆå¹¶ä¿å­˜ {len(story_results)} ä¸ªå¯¹è¯æ•…äº‹",
                "streaming"
            )
        
        logger.info(f"âœ… ç¬¬{batch_idx+1}æ‰¹å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(story_results)} ä¸ªå¯¹è¯æ•…äº‹")
        return story_results
    
    async def _append_to_csv(self, story_results: List[Dict], csv_path: str, workflow_chat=None) -> Dict:
        """å¢é‡è¿½åŠ æ•…äº‹ç»“æœåˆ°CSVæ–‡ä»¶"""
        try:
            if not story_results:
                return {
                    'success': False,
                    'message': "æ²¡æœ‰æ•…äº‹ç»“æœéœ€è¦ä¿å­˜"
                }
            
            # è¿½åŠ æ¨¡å¼å†™å…¥CSVæ–‡ä»¶
            with open(csv_path, 'a', newline='', encoding='utf-8-sig') as f:
                fieldnames = ['æ•…äº‹ID', 'æ•…äº‹æ ‡é¢˜', 'æ•…äº‹å†…å®¹', 'æ•…äº‹ä¸»é¢˜', 'æ•…äº‹å¯“æ„', 'ç”Ÿæˆæ—¶é—´']
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                
                # å†™å…¥æ•…äº‹ç»“æœ
                for result in story_results:
                    writer.writerow({
                        'æ•…äº‹ID': result.get('story_id', ''),
                        'æ•…äº‹æ ‡é¢˜': result.get('title', ''),
                        'æ•…äº‹å†…å®¹': result.get('story', ''),
                        'æ•…äº‹ä¸»é¢˜': result.get('theme', ''),
                        'æ•…äº‹å¯“æ„': result.get('moral', ''),
                        'ç”Ÿæˆæ—¶é—´': result.get('generated_at', datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
                    })
            
            logger.info(f"âœ… å¢é‡ä¿å­˜ï¼š{len(story_results)}ä¸ªå¯¹è¯æ•…äº‹å·²è¿½åŠ åˆ° {os.path.basename(csv_path)}")
            
            return {
                'success': True,
                'message': f"æˆåŠŸè¿½åŠ {len(story_results)}ä¸ªå¯¹è¯æ•…äº‹",
                'count': len(story_results),
                'file_path': csv_path
            }
            
        except Exception as e:
            logger.error(f"å¯¹è¯æ•…äº‹CSVå¢é‡ä¿å­˜å¤±è´¥: {e}")
            if workflow_chat:
                await workflow_chat.add_node_message(
                    "å¯¹è¯æ•…äº‹ç”Ÿæˆ",
                    f"âŒ å¯¹è¯æ•…äº‹CSVå¢é‡ä¿å­˜å¤±è´¥: {str(e)}",
                    "error"
                )
            
            return {
                'success': False,
                'message': f"å¢é‡ä¿å­˜å¤±è´¥: {str(e)}",
                'error': str(e)
            }


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    import asyncio
    from llm.doubao import DoubaoLLM
    from core.types import LLMConfig
    
    async def test_talk_story_generation():
        # åˆå§‹åŒ–LLM
        # åˆ›å»ºLLMé…ç½®
        llm_config = LLMConfig(
            provider="doubao",
            model_name=os.getenv('DOUBAO_MODEL_PRO', 'ep-20250312153153-npj4s'),
            api_key=os.getenv('DOUBAO_API_KEY', 'b633a622-b5d0-4f16-a8a9-616239cf15d1'),
            api_base=os.getenv('DOUBAO_BASE_URL', 'https://ark.cn-beijing.volces.com/api/v3'),
            temperature=0.7,
            max_tokens=4096,
            streaming=True
        )
        # ä½¿ç”¨é…ç½®åˆå§‹åŒ–LLM
        llm = DoubaoLLM(config=llm_config)
        
        # åˆå§‹åŒ–å¯¹è¯æ•…äº‹ç”ŸæˆèŠ‚ç‚¹
        node = TalkStoryGenerationNode()
        
        # å‡†å¤‡è¾“å…¥æ•°æ®
        input_data = {
            'llm': llm,
            'story_count': 2,  # ç”Ÿæˆ2ä¸ªæ•…äº‹ç”¨äºæµ‹è¯•
            'topics': ['ä¸€æ¬¡æœ‰è¶£çš„è¯¯ä¼š', 'ä¸€æ¬¡æ¸©é¦¨çš„æ—¥å¸¸å¯¹è¯'],
            'batch_size': 2
        }
        
        # æ‰§è¡ŒèŠ‚ç‚¹
        result = await node.execute(input_data)
        
        # æ‰“å°ç»“æœ
        print(f"\nç”Ÿæˆå®Œæˆï¼Œç»“æœä¿å­˜åœ¨: {result.get('story_save_result', {}).get('file_path', '')}")
        
        # æ‰“å°ç¬¬ä¸€ä¸ªæ•…äº‹ç¤ºä¾‹
        if result.get('story_results'):
            first_story = result['story_results'][0]
            print(f"\nç¤ºä¾‹æ•…äº‹: {first_story.get('title', '')}")
            print(f"ä¸»é¢˜: {first_story.get('theme', '')}")
            print(f"å†…å®¹:\n{first_story.get('story', '')}")
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_talk_story_generation())