"""å›¾ç‰‡è¯†åˆ«æ€§èƒ½åˆ†æå·¥ä½œæµ - åŸºäºè±†åŒ…/DoubaoLLMçš„å›¾ç‰‡è¯†åˆ«æ€§èƒ½æµ‹è¯•ç³»ç»Ÿ
æä¾›å¯¹å¤šç§æ¨¡å‹çš„å›¾ç‰‡è¯†åˆ«æ€§èƒ½å¯¹æ¯”åˆ†æï¼ŒåŒ…æ‹¬è€—æ—¶ç»Ÿè®¡å’ŒTokenæ¶ˆè€—ç»Ÿè®¡
"""

import json
import asyncio
import base64
import logging
import os
import time
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import csv

import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from core.graph import StateGraph
from core.base import BaseNode
from llm.base import LLMFactory
from core.types import LLMConfig, TaskResult, Message, MessageRole

logger = logging.getLogger(__name__)

class VisionPerformanceWorkflow:
    """å›¾ç‰‡è¯†åˆ«æ€§èƒ½åˆ†æå·¥ä½œæµç®¡ç†å™¨"""
    
    def __init__(self):
        self.graph = None
        self.models = {}

        # ä»ç¯å¢ƒå˜é‡è·å–3ç§æ¨¡å‹é…ç½®
        self.model_configs = {
            # 'vision_pro': {
            #     'name': 'è±†åŒ…Vision Pro',
            #     'env_key': 'DOUBAO_MODEL_VISION_PRO',
            #     'default': 'ep-20250704095927-j6t2g'
            # },
            # 'deepseek_r1': {
            #     'name': 'è±†åŒ…1.6',
            #     'env_key': 'DOUBAO_MODEL_1.6', 
            #     'default': 'ep-20250704095927-j6t2g'
            # },
            'deepseek_v3': {
                'name': 'è±†åŒ…1.6 Flash',
                'env_key': 'DOUBAO_MODEL_1.6_FLASH',
                'default': 'ep-20250612122042-t6g56'
            }
        }
        
        self.current_config = {
            'batch_size': 10,  # æ¯æ‰¹å¤„ç†çš„å›¾ç‰‡æ•°é‡
            'test_all_models': True,  # æ˜¯å¦æµ‹è¯•æ‰€æœ‰æ¨¡å‹
            'csv_output': {
                'enabled': True,
                'output_dir': 'workspace/vision_performance_output',
                'recognition_filename': 'vision_recognition_results.csv',
                'performance_filename': 'vision_performance_stats.csv',
                'encoding': 'utf-8-sig'
            }
        }
    
    async def initialize_models(self):
        """åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹"""
        api_key = os.getenv('ARK_API_KEY', "b633a622-b5d0-4f16-a8a9-616239cf15d1")
        api_base = "https://ark.cn-beijing.volces.com/api/v3"
        
        for model_key, config in self.model_configs.items():
            try:
                from llm.doubao import DoubaoLLM
                
                model_name = os.getenv(config['env_key'], config['default'])
                
                llm_config = LLMConfig(
                    provider="doubao",
                    model_name=model_name,
                    api_key=api_key.strip(),
                    api_base=api_base
                )
                
                self.models[model_key] = {
                    'llm': DoubaoLLM(config=llm_config),
                    'name': config['name'],
                    'model_name': model_name
                }
                
                logger.info(f"âœ… åˆå§‹åŒ–æ¨¡å‹æˆåŠŸ: {config['name']} ({model_name})")
                
            except Exception as e:
                logger.error(f"âŒ åˆå§‹åŒ–æ¨¡å‹å¤±è´¥ {config['name']}: {e}")
    
    def update_config(self, config_updates: Dict[str, Any]):
        """æ›´æ–°å·¥ä½œæµé…ç½®"""
        self.current_config.update(config_updates)
    
    async def create_vision_performance_graph(self) -> StateGraph:
        """åˆ›å»ºå›¾ç‰‡è¯†åˆ«æ€§èƒ½åˆ†æå·¥ä½œæµå›¾"""
        self.graph = StateGraph(name="vision_performance_workflow")
        
        # åˆ›å»ºèŠ‚ç‚¹
        image_loading_node = ImageLoadingNode()
        performance_test_node = VisionPerformanceTestNode()
        statistics_node = PerformanceStatisticsNode()
        result_save_node = PerformanceResultSaveNode()
        
        # æ·»åŠ èŠ‚ç‚¹åˆ°å›¾
        self.graph.add_node("image_loading", image_loading_node)
        self.graph.add_node("performance_test", performance_test_node)
        self.graph.add_node("statistics", statistics_node)
        self.graph.add_node("result_save", result_save_node)
        
        # å®šä¹‰èŠ‚ç‚¹è¿æ¥å…³ç³»
        self.graph.add_edge("image_loading", "performance_test")
        self.graph.add_edge("performance_test", "statistics")
        self.graph.add_edge("statistics", "result_save")
        
        # è®¾ç½®å…¥å£ç‚¹
        self.graph.set_entry_point("image_loading")
        
        return self.graph
    
    async def execute_workflow_stream(self, config: Dict[str, Any], workflow_chat, images=None):
        """æµå¼æ‰§è¡Œå›¾ç‰‡è¯†åˆ«æ€§èƒ½åˆ†æå·¥ä½œæµ"""
        try:
            # åˆå§‹åŒ–æ¨¡å‹
            await self.initialize_models()
            
            # å‡†å¤‡åˆå§‹è¾“å…¥
            initial_input = {
                'config': config,
                'workflow_chat': workflow_chat,
                'models': self.models,
                'images': images or [],
                'performance_stats': []
            }
            
            # åˆ›å»ºå¹¶ç¼–è¯‘å›¾å·¥ä½œæµ
            if not self.graph:
                await self.create_vision_performance_graph()
            
            compiled_graph = self.graph.compile()
            
            # ä½¿ç”¨å›¾çš„æµå¼æ‰§è¡Œ
            async for stream_event in compiled_graph.stream(initial_input):
                event_type = stream_event.get('type')
                node_name = stream_event.get('node')
                
                if event_type == 'start':
                    yield (
                        workflow_chat._create_workflow_progress(),
                        "",
                        "å›¾ç‰‡è¯†åˆ«æ€§èƒ½åˆ†æå·¥ä½œæµå¼€å§‹æ‰§è¡Œ...",
                        False
                    )
                
                elif event_type == 'node_start':
                    node_display_name = self._get_node_display_name(node_name)
                    workflow_chat.current_node = self._get_node_id(node_name)
                    
                    await workflow_chat.add_node_message(
                        node_display_name,
                        "å¼€å§‹æ‰§è¡Œ...",
                        "progress"
                    )
                    
                    yield (
                        workflow_chat._create_workflow_progress(),
                        "",
                        f"{node_display_name}å¼€å§‹æ‰§è¡Œ...",
                        False
                    )
                
                elif event_type == 'node_streaming':
                    intermediate_result = stream_event.get('intermediate_result')
                    if intermediate_result and intermediate_result.state_update:
                        # å¤„ç†æµå¼æ›´æ–°
                        node_display_name = self._get_node_display_name(node_name)
                        
                        if 'current_model' in intermediate_result.state_update:
                            current_model = intermediate_result.state_update['current_model']
                            await workflow_chat.add_node_message(
                                node_display_name,
                                f"æ­£åœ¨æµ‹è¯•æ¨¡å‹: {current_model}",
                                "streaming"
                            )
                            
                            yield (
                                workflow_chat._create_workflow_progress(),
                                "",
                                f"æ­£åœ¨æµ‹è¯•æ¨¡å‹: {current_model}",
                                False
                            )
                
                elif event_type == 'node_complete':
                    node_display_name = self._get_node_display_name(node_name)
                    result_content = "âœ… æ‰§è¡Œå®Œæˆ"
                    
                    await workflow_chat.add_node_message(
                        node_display_name,
                        result_content,
                        "completed"
                    )
                    
                    yield (
                        workflow_chat._create_workflow_progress(),
                        "",
                        f"{node_display_name}æ‰§è¡Œå®Œæˆ",
                        False
                    )
                
                elif event_type == 'node_error':
                    error_msg = stream_event.get('error', 'æœªçŸ¥é”™è¯¯')
                    node_display_name = self._get_node_display_name(node_name)
                    
                    await workflow_chat.add_node_message(
                        node_display_name,
                        f"æ‰§è¡Œå¤±è´¥: {error_msg}",
                        "error"
                    )
                    
                    yield (
                        workflow_chat._create_workflow_progress(),
                        "",
                        "",
                        False
                    )
                
                elif event_type == 'final':
                    yield (
                        workflow_chat._create_workflow_progress(),
                        "",
                        "å›¾ç‰‡è¯†åˆ«æ€§èƒ½åˆ†æå·¥ä½œæµæ‰§è¡Œå®Œæˆ",
                        False
                    )
                
                else:
                    yield (
                        workflow_chat._create_workflow_progress(),
                        "",
                        "å›¾ç‰‡è¯†åˆ«æ€§èƒ½åˆ†æå·¥ä½œæµæ‰§è¡Œä¸­...",
                        False
                    )
                
        except Exception as e:
            logger.error(f"å›¾ç‰‡è¯†åˆ«æ€§èƒ½åˆ†æå·¥ä½œæµæµå¼æ‰§è¡Œå¤±è´¥: {e}")
            await workflow_chat.add_node_message(
                "ç³»ç»Ÿ",
                f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}",
                "error"
            )
            yield (
                workflow_chat._create_workflow_progress(),
                "",
                "",
                False
            )
    
    def _get_node_display_name(self, node_name: str) -> str:
        """è·å–èŠ‚ç‚¹æ˜¾ç¤ºåç§°"""
        name_mapping = {
            'image_loading': 'å›¾ç‰‡åŠ è½½',
            'performance_test': 'æ€§èƒ½æµ‹è¯•',
            'statistics': 'ç»Ÿè®¡åˆ†æ',
            'result_save': 'ç»“æœä¿å­˜'
        }
        return name_mapping.get(node_name, node_name)
    
    def _get_node_id(self, node_name: str) -> str:
        """è·å–èŠ‚ç‚¹ID"""
        id_mapping = {
            'image_loading': 'loading',
            'performance_test': 'testing',
            'statistics': 'stats',
            'result_save': 'save'
        }
        return id_mapping.get(node_name, node_name)


class ImageLoadingNode(BaseNode):
    """å›¾ç‰‡åŠ è½½èŠ‚ç‚¹"""
    
    def __init__(self):
        super().__init__(name="image_loading", stream=True)
    
    async def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå›¾ç‰‡åŠ è½½èŠ‚ç‚¹"""
        final_result = None
        async for result in self.execute_stream(input_data):
            final_result = result
        return final_result or input_data
    
    async def execute_stream(self, input_data: Dict[str, Any]):
        """æµå¼æ‰§è¡Œå›¾ç‰‡åŠ è½½èŠ‚ç‚¹"""
        print("ğŸ“· å¼€å§‹åŠ è½½å›¾ç‰‡...")
        
        workflow_chat = input_data.get('workflow_chat')
        images = input_data.get('images', [])
        
        if not images:
            if workflow_chat:
                await workflow_chat.add_node_message(
                    "å›¾ç‰‡åŠ è½½",
                    "âš ï¸ æ²¡æœ‰å›¾ç‰‡éœ€è¦å¤„ç†",
                    "warning"
                )
            yield input_data
            return
        
        if workflow_chat:
            await workflow_chat.add_node_message(
                "å›¾ç‰‡åŠ è½½",
                f"æ­£åœ¨åŠ è½½ {len(images)} å¼ å›¾ç‰‡...",
                "progress"
            )
        
        # åŠ è½½å’Œé¢„å¤„ç†å›¾ç‰‡
        loaded_images = []
        for img_idx, img_path in enumerate(images):
            try:
                # å¤„ç†ç‰¹æ®Šæ–‡ä»¶åï¼ˆä»¥@å¼€å¤´çš„æ–‡ä»¶åï¼‰
                actual_path = img_path
                if img_path.startswith('@'):
                    # å°è¯•åœ¨å„ä¸ªå¯èƒ½çš„ç›®å½•ä¸‹æŸ¥æ‰¾
                    possible_paths = [
                        img_path,
                        img_path[1:],
                        os.path.join('workspace', 'input', img_path),
                        os.path.join('workspace', 'input', img_path[1:]),
                        os.path.join('workspace', 'input', 'æ–¹çŸ¥è¡¡100', 'å® ç‰©', img_path[1:]),
                        os.path.join('workspace', 'input', 'æ–¹çŸ¥è¡¡100', 'é£æ™¯', img_path[1:]),
                        os.path.join('workspace', 'input', 'æ–¹çŸ¥è¡¡100', 'å·¥ä½œ', img_path[1:]),
                        os.path.join('workspace', 'input', 'æ–¹çŸ¥è¡¡100', 'é£æ™¯ä¿®', img_path[1:])
                    ]
                    
                    for path in possible_paths:
                        if os.path.exists(path):
                            actual_path = path
                            break
                
                if not os.path.exists(actual_path):
                    logger.warning(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {img_path}ï¼Œè·³è¿‡å¤„ç†")
                    continue
                
                # è¯»å–å›¾ç‰‡æ–‡ä»¶å¹¶è¿›è¡ŒBase64ç¼–ç 
                with open(actual_path, "rb") as img_file:
                    img_data = img_file.read()
                    base64_img = base64.b64encode(img_data).decode("utf-8")
                
                # è·å–æ–‡ä»¶ä¿¡æ¯
                img_name = os.path.basename(img_path)
                img_size = len(img_data)
                img_ext = os.path.splitext(img_path)[1].lower()
                
                if not img_ext:
                    if img_data.startswith(b'\x89PNG'):
                        img_ext = '.png'
                    elif img_data.startswith(b'\xff\xd8'):
                        img_ext = '.jpg'
                    else:
                        img_ext = '.png'
                
                # ç¡®å®šMIMEç±»å‹
                mime_type = "image/jpeg"
                if img_ext == ".png":
                    mime_type = "image/png"
                elif img_ext == ".gif":
                    mime_type = "image/gif"
                elif img_ext in [".jpg", ".jpeg"]:
                    mime_type = "image/jpeg"
                elif img_ext in [".webp"]:
                    mime_type = "image/webp"
                
                loaded_images.append({
                    "image_path": img_path,
                    "actual_path": actual_path,
                    "image_name": img_name,
                    "base64_data": base64_img,
                    "mime_type": mime_type,
                    "file_size": img_size,
                    "image_index": img_idx
                })
                
                logger.info(f"æˆåŠŸåŠ è½½å›¾ç‰‡: {img_name} ({img_size} å­—èŠ‚)")
                
            except Exception as e:
                logger.error(f"åŠ è½½å›¾ç‰‡å¤±è´¥ ({img_path}): {e}")
        
        if workflow_chat:
            await workflow_chat.add_node_message(
                "å›¾ç‰‡åŠ è½½",
                f"âœ… å·²æˆåŠŸåŠ è½½ {len(loaded_images)} å¼ å›¾ç‰‡",
                "success"
            )
        
        # æ„å»ºè¾“å‡ºæ•°æ®
        output_data = input_data.copy()
        output_data['loaded_images'] = loaded_images
        
        logger.info(f"âœ… å›¾ç‰‡åŠ è½½å®Œæˆï¼Œå…± {len(loaded_images)} å¼ ")
        yield output_data


class VisionPerformanceTestNode(BaseNode):
    """å›¾ç‰‡è¯†åˆ«æ€§èƒ½æµ‹è¯•èŠ‚ç‚¹"""
    
    def __init__(self):
        super().__init__(name="performance_test", stream=True)
    
    async def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæ€§èƒ½æµ‹è¯•èŠ‚ç‚¹"""
        final_result = None
        async for result in self.execute_stream(input_data):
            final_result = result
        return final_result or input_data
    
    async def execute_stream(self, input_data: Dict[str, Any]):
        """æµå¼æ‰§è¡Œæ€§èƒ½æµ‹è¯•èŠ‚ç‚¹"""
        print("ğŸ” å¼€å§‹å›¾ç‰‡è¯†åˆ«æ€§èƒ½æµ‹è¯•...")
        
        workflow_chat = input_data.get('workflow_chat')
        models = input_data.get('models', {})
        loaded_images = input_data.get('loaded_images', [])
        config = input_data.get('config', {})
        
        if not loaded_images:
            if workflow_chat:
                await workflow_chat.add_node_message(
                    "æ€§èƒ½æµ‹è¯•",
                    "âš ï¸ æ²¡æœ‰å›¾ç‰‡éœ€è¦æµ‹è¯•",
                    "warning"
                )
            yield input_data
            return
        
        if not models:
            if workflow_chat:
                await workflow_chat.add_node_message(
                    "æ€§èƒ½æµ‹è¯•",
                    "âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹",
                    "error"
                )
            yield input_data
            return
        
        # æ€§èƒ½æµ‹è¯•ç»“æœ
        all_test_results = []
        
        # æµ‹è¯•æ¯ä¸ªæ¨¡å‹
        for model_key, model_info in models.items():
            model_name = model_info['name']
            llm = model_info['llm']
            
            if workflow_chat:
                await workflow_chat.add_node_message(
                    "æ€§èƒ½æµ‹è¯•",
                    f"å¼€å§‹æµ‹è¯•æ¨¡å‹: {model_name}",
                    "progress"
                )
            
            # ä¸ºå½“å‰æ¨¡å‹æµ‹è¯•æ‰€æœ‰å›¾ç‰‡
            model_results = []
            for img_data in loaded_images:
                try:
                    # è®°å½•å¼€å§‹æ—¶é—´
                    start_time = time.time()
                    
                    # æ‰§è¡Œå›¾ç‰‡è¯†åˆ«
                    result = await self._recognize_image_with_stats(llm, img_data, model_info)
                    
                    # è®°å½•ç»“æŸæ—¶é—´
                    end_time = time.time()
                    duration = end_time - start_time
                    
                    # æ·»åŠ æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯
                    result.update({
                        'model_key': model_key,
                        'model_name': model_name,
                        'model_id': model_info['model_name'],
                        'duration_seconds': round(duration, 3),
                        'timestamp': datetime.now().isoformat()
                    })
                    
                    model_results.append(result)
                    
                    logger.info(f"æ¨¡å‹ {model_name} è¯†åˆ«å›¾ç‰‡ {img_data['image_name']} å®Œæˆï¼Œè€—æ—¶: {duration:.3f}ç§’")
                    
                except Exception as e:
                    logger.error(f"æ¨¡å‹ {model_name} è¯†åˆ«å›¾ç‰‡ {img_data['image_name']} å¤±è´¥: {e}")
                    model_results.append({
                        'model_key': model_key,
                        'model_name': model_name,
                        'model_id': model_info['model_name'],
                        'image_name': img_data['image_name'],
                        'image_path': img_data['image_path'],
                        'title': 'è¯†åˆ«å¤±è´¥',
                        'description': f'é”™è¯¯: {str(e)}',
                        'duration_seconds': 0,
                        'input_tokens': 0,
                        'output_tokens': 0,
                        'total_tokens': 0,
                        'error': str(e),
                        'timestamp': datetime.now().isoformat()
                    })
            
            all_test_results.extend(model_results)
            
            if workflow_chat:
                await workflow_chat.add_node_message(
                    "æ€§èƒ½æµ‹è¯•",
                    f"âœ… æ¨¡å‹ {model_name} æµ‹è¯•å®Œæˆï¼Œå¤„ç†äº† {len(model_results)} å¼ å›¾ç‰‡",
                    "success"
                )
        
        # æ„å»ºè¾“å‡ºæ•°æ®
        output_data = input_data.copy()
        output_data['test_results'] = all_test_results
        
        logger.info(f"âœ… æ€§èƒ½æµ‹è¯•å®Œæˆï¼Œå…±æµ‹è¯•äº† {len(models)} ä¸ªæ¨¡å‹ï¼Œ{len(loaded_images)} å¼ å›¾ç‰‡")
        yield output_data
    
    async def _recognize_image_with_stats(self, llm, img_data: Dict, model_info: Dict) -> Dict:
        """æ‰§è¡Œå›¾ç‰‡è¯†åˆ«å¹¶æ”¶é›†ç»Ÿè®¡ä¿¡æ¯"""
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å›¾ç‰‡è¯†åˆ«åŠ©æ‰‹ï¼Œæ“…é•¿åˆ†æå›¾ç‰‡å†…å®¹å¹¶ç”Ÿæˆå‡†ç¡®çš„è¯¦ç»†æè¿°ã€‚
è¯·æ ¹æ®æä¾›çš„å›¾ç‰‡å†…å®¹ï¼Œæä¾›ç²¾ç¡®çš„å›¾ç‰‡å†…å®¹æè¿°ï¼ˆ100-150å­—ï¼‰

è¾“å‡ºç›´æ¥ä¸ºæ–‡æœ¬ï¼Œä¸è¾“å‡ºä»»ä½•å…¶ä»–å†…å®¹ã€‚
ç¤ºä¾‹ï¼š
åœ¨ç§‹æ—¥å…¬å›­çš„ç…§ç‰‡ï¼Œç”»é¢ä¸­ä¸€åªé“¶ç°è‰²çŸ­æ¯›çŒ«æ­£è¹²ååœ¨äººè¡Œé“ä¸Šï¼Œå¥½å¥‡åœ°ç”¨çˆªå­è§¦ç¢°ä¸€ç‰‡æ¯é»„çš„è½å¶ã€‚èƒŒæ™¯æ˜¯å…¬å›­å…¥å£å¤„çš„ç»¿è‰²æ‹±é—¨å’Œæ ‡è¯†ç‰Œï¼Œå‘¨å›´ç¯ç»•ç€å¤šæ£µè½å¶æ ‘æœ¨ï¼Œæ ‘å¶å‘ˆç°é‡‘é»„è‰²è°ƒã€‚é˜³å…‰é€è¿‡æ ‘å¶å½¢æˆæŸ”å’Œçš„å…‰å½±æ•ˆæœï¼Œæ•´ä¸ªåœºæ™¯å……æ»¡å®é™ç¥¥å’Œçš„ç§‹æ—¥æ°›å›´ã€‚
"""
        
        user_message = Message(
            role=MessageRole.USER,
            content="è¯·åˆ†æè¿™å¼ å›¾ç‰‡ï¼Œæä¾›æ ‡é¢˜å’Œè¯¦ç»†æè¿°ã€‚",
            metadata={
                "has_image": True,
                "image_data": img_data["base64_data"],
                "image_mime": img_data["mime_type"]
            }
        )
        
        messages = [
            Message(role=MessageRole.SYSTEM, content=system_prompt),
            user_message
        ]
        
        # Monkey patchæ”¯æŒå›¾ç‰‡
        original_convert_messages = llm._convert_messages
        
        def patched_convert_messages(messages_list):
            """æ·»åŠ å¯¹å›¾ç‰‡çš„æ”¯æŒ"""
            converted = []
            for msg in messages_list:
                role = "user" if msg.role == MessageRole.USER else "assistant"
                if msg.role == MessageRole.SYSTEM:
                    role = "system"
                
                if msg.metadata and msg.metadata.get("has_image"):
                    converted.append({
                        "role": role,
                        "content": [
                            {"type": "text", "text": msg.content},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:{msg.metadata.get('image_mime', 'image/jpeg')};base64,{msg.metadata.get('image_data')}",
                                }
                            }
                        ]
                    })
                else:
                    converted.append({
                        "role": role,
                        "content": msg.content
                    })
            
            return converted
        
        # åº”ç”¨monkey patch
        llm._convert_messages = patched_convert_messages
        
        try:
            # è°ƒç”¨LLM
            response = await llm.generate(
                messages,
                temperature=0.7,
                max_tokens=4096,
                mode="normal"
            )
            
            # æ¢å¤åŸå§‹æ–¹æ³•
            llm._convert_messages = original_convert_messages
            
            # è§£æç»“æœ
            content = response.content
            
            # æå–JSON
            import re
            json_match = re.search(r'```json\s*(.*?)\s*```', content, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    json_str = json_match.group(0)
                else:
                    json_str = content
            
            try:
                result_data = json.loads(json_str.strip())
            except json.JSONDecodeError:
                result_data = {
                    "title": "è§£æå¤±è´¥",
                    "description": content
                }
            
            # ä»metadataä¸­æå–tokenç»Ÿè®¡ä¿¡æ¯
            usage_info = response.metadata.get('usage', {})
            input_tokens = usage_info.get('prompt_tokens', 0)
            output_tokens = usage_info.get('completion_tokens', 0)
            total_tokens = usage_info.get('total_tokens', input_tokens + output_tokens)
            
            # æ·»åŠ åŸºæœ¬ä¿¡æ¯å’ŒTokenç»Ÿè®¡
            result_data.update({
                'image_name': img_data['image_name'],
                'image_path': img_data['image_path'],
                'input_tokens': input_tokens,
                'output_tokens': output_tokens,
                'total_tokens': total_tokens,
                'usage_raw': usage_info  # ä¿ç•™åŸå§‹usageä¿¡æ¯ç”¨äºè°ƒè¯•
            })
            
            return result_data
            
        except Exception as e:
            # æ¢å¤åŸå§‹æ–¹æ³•
            llm._convert_messages = original_convert_messages
            raise e


class PerformanceStatisticsNode(BaseNode):
    """æ€§èƒ½ç»Ÿè®¡åˆ†æèŠ‚ç‚¹"""
    
    def __init__(self):
        super().__init__(name="statistics", stream=True)
    
    async def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œç»Ÿè®¡åˆ†æèŠ‚ç‚¹"""
        final_result = None
        async for result in self.execute_stream(input_data):
            final_result = result
        return final_result or input_data
    
    async def execute_stream(self, input_data: Dict[str, Any]):
        """æµå¼æ‰§è¡Œç»Ÿè®¡åˆ†æèŠ‚ç‚¹"""
        print("ğŸ“Š å¼€å§‹ç»Ÿè®¡åˆ†æ...")
        
        workflow_chat = input_data.get('workflow_chat')
        test_results = input_data.get('test_results', [])
        
        if not test_results:
            if workflow_chat:
                await workflow_chat.add_node_message(
                    "ç»Ÿè®¡åˆ†æ",
                    "âš ï¸ æ²¡æœ‰æµ‹è¯•ç»“æœéœ€è¦åˆ†æ",
                    "warning"
                )
            yield input_data
            return
        
        if workflow_chat:
            await workflow_chat.add_node_message(
                "ç»Ÿè®¡åˆ†æ",
                "æ­£åœ¨ç”Ÿæˆæ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š...",
                "progress"
            )
        
        # æŒ‰æ¨¡å‹åˆ†ç»„ç»Ÿè®¡
        model_stats = {}
        for result in test_results:
            model_key = result.get('model_key', 'unknown')
            model_name = result.get('model_name', 'Unknown Model')
            
            if model_key not in model_stats:
                model_stats[model_key] = {
                    'model_name': model_name,
                    'model_id': result.get('model_id', ''),
                    'total_images': 0,
                    'success_count': 0,
                    'error_count': 0,
                    'total_duration': 0,
                    'total_input_tokens': 0,
                    'total_output_tokens': 0,
                    'total_tokens': 0,
                    'durations': [],
                    'token_counts': []
                }
            
            stats = model_stats[model_key]
            stats['total_images'] += 1
            
            if 'error' not in result:
                stats['success_count'] += 1
                stats['total_duration'] += result.get('duration_seconds', 0)
                stats['total_input_tokens'] += result.get('input_tokens', 0)
                stats['total_output_tokens'] += result.get('output_tokens', 0)
                stats['total_tokens'] += result.get('total_tokens', 0)
                stats['durations'].append(result.get('duration_seconds', 0))
                stats['token_counts'].append(result.get('total_tokens', 0))
            else:
                stats['error_count'] += 1
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        performance_summary = []
        for model_key, stats in model_stats.items():
            if stats['success_count'] > 0:
                avg_duration = stats['total_duration'] / stats['success_count']
                avg_tokens = stats['total_tokens'] / stats['success_count']
                avg_input_tokens = stats['total_input_tokens'] / stats['success_count']
                avg_output_tokens = stats['total_output_tokens'] / stats['success_count']
                
                # è®¡ç®—ä¸­ä½æ•°å’Œæ ‡å‡†å·®
                import statistics
                durations = stats['durations']
                tokens = stats['token_counts']
                
                median_duration = statistics.median(durations) if durations else 0
                duration_stdev = statistics.stdev(durations) if len(durations) > 1 else 0
                median_tokens = statistics.median(tokens) if tokens else 0
                token_stdev = statistics.stdev(tokens) if len(tokens) > 1 else 0
            else:
                avg_duration = 0
                avg_tokens = 0
                avg_input_tokens = 0
                avg_output_tokens = 0
                median_duration = 0
                duration_stdev = 0
                median_tokens = 0
                token_stdev = 0
            
            summary = {
                'model_key': model_key,
                'model_name': stats['model_name'],
                'model_id': stats['model_id'],
                'total_images': stats['total_images'],
                'success_count': stats['success_count'],
                'error_count': stats['error_count'],
                'success_rate': round(stats['success_count'] / stats['total_images'] * 100, 2) if stats['total_images'] > 0 else 0,
                'avg_duration_seconds': round(avg_duration, 3),
                'median_duration_seconds': round(median_duration, 3),
                'duration_stdev': round(duration_stdev, 3),
                'total_duration_seconds': round(stats['total_duration'], 3),
                'avg_input_tokens': round(avg_input_tokens, 1),
                'avg_output_tokens': round(avg_output_tokens, 1),
                'avg_total_tokens': round(avg_tokens, 1),
                'median_tokens': round(median_tokens, 1),
                'token_stdev': round(token_stdev, 1),
                'total_input_tokens': stats['total_input_tokens'],
                'total_output_tokens': stats['total_output_tokens'],
                'total_tokens': stats['total_tokens']
            }
            
            performance_summary.append(summary)
        
        if workflow_chat:
            await workflow_chat.add_node_message(
                "ç»Ÿè®¡åˆ†æ",
                f"âœ… å·²ç”Ÿæˆ {len(performance_summary)} ä¸ªæ¨¡å‹çš„æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š",
                "success"
            )
        
        # æ„å»ºè¾“å‡ºæ•°æ®
        output_data = input_data.copy()
        output_data['performance_summary'] = performance_summary
        
        logger.info(f"âœ… ç»Ÿè®¡åˆ†æå®Œæˆï¼Œç”Ÿæˆäº† {len(performance_summary)} ä¸ªæ¨¡å‹çš„æ€§èƒ½æŠ¥å‘Š")
        yield output_data


class PerformanceResultSaveNode(BaseNode):
    """æ€§èƒ½ç»“æœä¿å­˜èŠ‚ç‚¹"""
    
    def __init__(self):
        super().__init__(name="result_save", stream=True)
    
    async def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œç»“æœä¿å­˜èŠ‚ç‚¹"""
        final_result = None
        async for result in self.execute_stream(input_data):
            final_result = result
        return final_result or input_data
    
    async def execute_stream(self, input_data: Dict[str, Any]):
        """æµå¼æ‰§è¡Œç»“æœä¿å­˜èŠ‚ç‚¹"""
        print("ğŸ’¾ å¼€å§‹ä¿å­˜ç»“æœ...")
        
        workflow_chat = input_data.get('workflow_chat')
        test_results = input_data.get('test_results', [])
        performance_summary = input_data.get('performance_summary', [])
        config = input_data.get('config', {})
        
        if workflow_chat:
            await workflow_chat.add_node_message(
                "ç»“æœä¿å­˜",
                "æ­£åœ¨ä¿å­˜è¯†åˆ«ç»“æœå’Œæ€§èƒ½ç»Ÿè®¡...",
                "progress"
            )
        
        # ä¿å­˜è¯¦ç»†è¯†åˆ«ç»“æœ
        recognition_save_result = await self._save_recognition_results(test_results, config)
        
        # ä¿å­˜æ€§èƒ½ç»Ÿè®¡
        performance_save_result = await self._save_performance_stats(performance_summary, config)
        
        if workflow_chat:
            await workflow_chat.add_node_message(
                "ç»“æœä¿å­˜",
                "âœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜å®Œæˆ",
                "success"
            )
        
        # æ„å»ºè¾“å‡ºæ•°æ®
        output_data = input_data.copy()
        output_data.update({
            'recognition_save_result': recognition_save_result,
            'performance_save_result': performance_save_result,
            'save_success': recognition_save_result.get('success', False) and performance_save_result.get('success', False)
        })
        
        yield output_data
    
    async def _save_recognition_results(self, test_results: List[Dict], config: Dict) -> Dict:
        """ä¿å­˜è¯¦ç»†è¯†åˆ«ç»“æœ"""
        try:
            csv_config = config.get('csv_output', {})
            output_dir = csv_config.get('output_dir', 'workspace/vision_performance_output')
            filename = csv_config.get('recognition_filename', 'vision_recognition_results.csv')
            encoding = csv_config.get('encoding', 'utf-8-sig')
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(output_dir, exist_ok=True)
            
            # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            base_name = os.path.splitext(filename)[0]
            ext = os.path.splitext(filename)[1]
            timestamped_filename = f"{base_name}_{timestamp}{ext}"
            
            csv_file = os.path.join(output_dir, timestamped_filename)
            
            # å†™å…¥CSVæ–‡ä»¶
            with open(csv_file, 'w', newline='', encoding=encoding) as f:
                fieldnames = [
                    'æ¨¡å‹åç§°', 'æ¨¡å‹ID', 'å›¾ç‰‡åç§°', 'å›¾ç‰‡è·¯å¾„', 'å›¾ç‰‡æ ‡é¢˜', 'å›¾ç‰‡æè¿°',
                    'è€—æ—¶(ç§’)', 'è¾“å…¥Token', 'è¾“å‡ºToken', 'æ€»Token', 'æ—¶é—´æˆ³', 'é”™è¯¯ä¿¡æ¯'
                ]
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                
                for result in test_results:
                    writer.writerow({
                        'æ¨¡å‹åç§°': result.get('model_name', ''),
                        'æ¨¡å‹ID': result.get('model_id', ''),
                        'å›¾ç‰‡åç§°': result.get('image_name', ''),
                        'å›¾ç‰‡è·¯å¾„': result.get('image_path', ''),
                        'å›¾ç‰‡æ ‡é¢˜': result.get('title', ''),
                        'å›¾ç‰‡æè¿°': result.get('description', ''),
                        'è€—æ—¶(ç§’)': result.get('duration_seconds', 0),
                        'è¾“å…¥Token': result.get('input_tokens', 0),
                        'è¾“å‡ºToken': result.get('output_tokens', 0),
                        'æ€»Token': result.get('total_tokens', 0),
                        'æ—¶é—´æˆ³': result.get('timestamp', ''),
                        'é”™è¯¯ä¿¡æ¯': result.get('error', '')
                    })
            
            logger.info(f"âœ… è¯†åˆ«ç»“æœä¿å­˜å®Œæˆï¼š{len(test_results)}æ¡è®°å½•ä¿å­˜åˆ° {csv_file}")
            
            return {
                'success': True,
                'message': f"æˆåŠŸä¿å­˜{len(test_results)}æ¡è¯†åˆ«ç»“æœ",
                'count': len(test_results),
                'file_path': csv_file
            }
            
        except Exception as e:
            logger.error(f"è¯†åˆ«ç»“æœä¿å­˜å¤±è´¥: {e}")
            return {
                'success': False,
                'message': f"ä¿å­˜å¤±è´¥: {str(e)}",
                'error': str(e)
            }
    
    async def _save_performance_stats(self, performance_summary: List[Dict], config: Dict) -> Dict:
        """ä¿å­˜æ€§èƒ½ç»Ÿè®¡"""
        try:
            csv_config = config.get('csv_output', {})
            output_dir = csv_config.get('output_dir', 'workspace/vision_performance_output')
            filename = csv_config.get('performance_filename', 'vision_performance_stats.csv')
            encoding = csv_config.get('encoding', 'utf-8-sig')
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(output_dir, exist_ok=True)
            
            # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            base_name = os.path.splitext(filename)[0]
            ext = os.path.splitext(filename)[1]
            timestamped_filename = f"{base_name}_{timestamp}{ext}"
            
            csv_file = os.path.join(output_dir, timestamped_filename)
            
            # å†™å…¥CSVæ–‡ä»¶
            with open(csv_file, 'w', newline='', encoding=encoding) as f:
                fieldnames = [
                    'æ¨¡å‹åç§°', 'æ¨¡å‹ID', 'æµ‹è¯•å›¾ç‰‡æ•°', 'æˆåŠŸæ•°é‡', 'å¤±è´¥æ•°é‡', 'æˆåŠŸç‡(%)',
                    'å¹³å‡è€—æ—¶(ç§’)', 'ä¸­ä½è€—æ—¶(ç§’)', 'è€—æ—¶æ ‡å‡†å·®', 'æ€»è€—æ—¶(ç§’)',
                    'å¹³å‡è¾“å…¥Token', 'å¹³å‡è¾“å‡ºToken', 'å¹³å‡æ€»Token', 'Tokenä¸­ä½æ•°', 'Tokenæ ‡å‡†å·®',
                    'æ€»è¾“å…¥Token', 'æ€»è¾“å‡ºToken', 'æ€»Tokenæ•°'
                ]
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                
                for summary in performance_summary:
                    writer.writerow({
                        'æ¨¡å‹åç§°': summary.get('model_name', ''),
                        'æ¨¡å‹ID': summary.get('model_id', ''),
                        'æµ‹è¯•å›¾ç‰‡æ•°': summary.get('total_images', 0),
                        'æˆåŠŸæ•°é‡': summary.get('success_count', 0),
                        'å¤±è´¥æ•°é‡': summary.get('error_count', 0),
                        'æˆåŠŸç‡(%)': summary.get('success_rate', 0),
                        'å¹³å‡è€—æ—¶(ç§’)': summary.get('avg_duration_seconds', 0),
                        'ä¸­ä½è€—æ—¶(ç§’)': summary.get('median_duration_seconds', 0),
                        'è€—æ—¶æ ‡å‡†å·®': summary.get('duration_stdev', 0),
                        'æ€»è€—æ—¶(ç§’)': summary.get('total_duration_seconds', 0),
                        'å¹³å‡è¾“å…¥Token': summary.get('avg_input_tokens', 0),
                        'å¹³å‡è¾“å‡ºToken': summary.get('avg_output_tokens', 0),
                        'å¹³å‡æ€»Token': summary.get('avg_total_tokens', 0),
                        'Tokenä¸­ä½æ•°': summary.get('median_tokens', 0),
                        'Tokenæ ‡å‡†å·®': summary.get('token_stdev', 0),
                        'æ€»è¾“å…¥Token': summary.get('total_input_tokens', 0),
                        'æ€»è¾“å‡ºToken': summary.get('total_output_tokens', 0),
                        'æ€»Tokenæ•°': summary.get('total_tokens', 0)
                    })
            
            logger.info(f"âœ… æ€§èƒ½ç»Ÿè®¡ä¿å­˜å®Œæˆï¼š{len(performance_summary)}ä¸ªæ¨¡å‹çš„ç»Ÿè®¡ä¿å­˜åˆ° {csv_file}")
            
            return {
                'success': True,
                'message': f"æˆåŠŸä¿å­˜{len(performance_summary)}ä¸ªæ¨¡å‹çš„æ€§èƒ½ç»Ÿè®¡",
                'count': len(performance_summary),
                'file_path': csv_file
            }
            
        except Exception as e:
            logger.error(f"æ€§èƒ½ç»Ÿè®¡ä¿å­˜å¤±è´¥: {e}")
            return {
                'success': False,
                'message': f"ä¿å­˜å¤±è´¥: {str(e)}",
                'error': str(e)
            }


if __name__ == "__main__":
    """ç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶è¿›è¡Œå›¾ç‰‡è¯†åˆ«æ€§èƒ½æµ‹è¯•"""
    import asyncio
    import glob
    
    print("ğŸ–¼ï¸ å›¾ç‰‡è¯†åˆ«æ€§èƒ½åˆ†æå·¥ä½œæµ")
    print("=" * 60)
    
    class MockWorkflowChat:
        def __init__(self):
            self.current_node = ""
        
        async def add_node_message(self, node_name: str, message: str, status: str):
            print(f"[{node_name}] {status}: {message}")
        
        def _create_workflow_progress(self):
            return "<div>å·¥ä½œæµè¿›åº¦</div>"
    
    async def main():
        try:
            # åˆå§‹åŒ–å·¥ä½œæµ
            workflow = VisionPerformanceWorkflow()
            print("âœ… å›¾ç‰‡è¯†åˆ«æ€§èƒ½åˆ†æå·¥ä½œæµåˆå§‹åŒ–å®Œæˆ")
            
            # è‡ªåŠ¨æ‰«æå›¾ç‰‡
            image_paths = []
            image_dirs = [
                "workspace/input/å¯¹è¯æ—¥å¸¸å›¾ç‰‡/é€šç”¨/*.png",
                "workspace/input/å¯¹è¯æ—¥å¸¸å›¾ç‰‡/åŠ¨ç‰©ä¿®/*.png",
                "workspace/input/å¯¹è¯æ—¥å¸¸å›¾ç‰‡/ç¾é£Ÿä¿®/*.png", 
                "workspace/input/å¯¹è¯æ—¥å¸¸å›¾ç‰‡/é£æ™¯ä¿®/*.png"
            ]
            
            for pattern in image_dirs:
                image_paths.extend(glob.glob(pattern))
            
            # é™åˆ¶æµ‹è¯•å›¾ç‰‡æ•°é‡ï¼ˆæ¼”ç¤ºç”¨ï¼‰
            image_paths = image_paths[:3] if len(image_paths) > 3 else image_paths
            
            print(f"ğŸ–¼ï¸ å‘ç°å›¾ç‰‡æ•°é‡: {len(image_paths)}")
            
            # é…ç½®
            config = {
                'batch_size': 10,
                'test_all_models': True,
                'csv_output': {
                    'enabled': True,
                    'output_dir': 'workspace/vision_performance_output',
                    'recognition_filename': 'vision_recognition_results.csv',
                    'performance_filename': 'vision_performance_stats.csv',
                    'encoding': 'utf-8-sig'
                }
            }
            
            # åˆ›å»ºæ¨¡æ‹ŸèŠå¤©ç•Œé¢
            mock_chat = MockWorkflowChat()
            
            # åˆ›å»ºå·¥ä½œæµå›¾
            graph = await workflow.create_vision_performance_graph()
            compiled_graph = graph.compile()
            print("âœ… å·¥ä½œæµå›¾åˆ›å»ºå®Œæˆ")
            
            # å‡†å¤‡è¾“å…¥æ•°æ®
            input_data = {
                'config': config,
                'workflow_chat': mock_chat,
                'images': image_paths
            }
            
            print(f"\nğŸš€ å¼€å§‹æ€§èƒ½æµ‹è¯•ï¼Œå¤„ç†{len(image_paths)}å¼ å›¾ç‰‡...")
            
            # æ‰§è¡Œå·¥ä½œæµ
            final_result = None
            async for result in compiled_graph.stream(input_data):
                if result:
                    final_result = result
            
            print("\nâœ… å›¾ç‰‡è¯†åˆ«æ€§èƒ½åˆ†æå·¥ä½œæµæ‰§è¡Œå®Œæˆ!")
            
            # æ‰“å°ç®€è¦ç»Ÿè®¡
            if final_result and 'performance_summary' in final_result:
                print("\nğŸ“Š æ€§èƒ½ç»Ÿè®¡æ‘˜è¦:")
                print("-" * 60)
                for summary in final_result['performance_summary']:
                    print(f"æ¨¡å‹: {summary['model_name']}")
                    print(f"  æˆåŠŸç‡: {summary['success_rate']}%")
                    print(f"  å¹³å‡è€—æ—¶: {summary['avg_duration_seconds']}ç§’")
                    print(f"  å¹³å‡Token: {summary['avg_total_tokens']}")
                    print(f"  æ€»Token: {summary['total_tokens']}")
                    print()
            
        except Exception as e:
            print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    # è¿è¡Œä¸»å‡½æ•°
    asyncio.run(main())